{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "seed = 0\n",
    "os.environ['PYTHONHASSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add modules path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_reader import DataReader\n",
    "from src.feature_extraction import get_principal_components, dimensionality_reduction\n",
    "from src.metrics import compute_evaluation_metrics\n",
    "from src.model import create_mlp_model, get_callbacks\n",
    "from src.model_evaluation import evaluate_mlp, evaluate_mlp_multiple_splits\n",
    "from src.plotting import plot_loss_curves\n",
    "from src.save_object import load_object, save_object\n",
    "from src.training import train_mlp, MLPConfigParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input and output paths\n",
    "filename = '../data/N-CMAPSS_DS02-006.h5'\n",
    "output_path = '../results/experiment_set_18'\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_mlp_pca(x_train, y_train,\n",
    "                           x_test, y_test,\n",
    "                           num_trials,\n",
    "                           mlp_config_params,\n",
    "                           results_path,\n",
    "                           epochs, batch_size, results_file=None):\n",
    "    mse_vals = []\n",
    "    rmse_vals = []\n",
    "    cmapss_vals = []\n",
    "    \n",
    "    for trial_num in range(num_trials):\n",
    "        # Train-validation split for early stopping\n",
    "        x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train,\n",
    "                                                                                  y_train,\n",
    "                                                                                  test_size=0.1,\n",
    "                                                                                  random_state=trial_num)\n",
    "        # Create output path\n",
    "        results_path_crr_split = os.path.join(results_path, f\"split_{trial_num}\")\n",
    "        if not os.path.exists(results_path_crr_split):\n",
    "            os.makedirs(results_path_crr_split)\n",
    "    \n",
    "        # Standardization\n",
    "        scaler_file = os.path.join(results_path_crr_split, 'scaler.pkl')\n",
    "        scaler = StandardScaler()\n",
    "        x_train_scaled = scaler.fit_transform(x_train_split)\n",
    "        x_val_scaled = scaler.transform(x_val_split)\n",
    "        x_test_scaled = scaler.transform(x_test)\n",
    "        save_object(scaler, scaler_file)\n",
    "    \n",
    "        # PCA\n",
    "        pca = get_principal_components(x_train_scaled, debug=True)\n",
    "        pca_file = os.path.join(results_path_crr_split, 'pca.pkl')\n",
    "        save_object(pca, pca_file)\n",
    "    \n",
    "        x_train_final = dimensionality_reduction(x_train_scaled, pca)\n",
    "        x_val_final = dimensionality_reduction(x_val_scaled, pca)\n",
    "        x_test_final = dimensionality_reduction(x_test_scaled, pca)\n",
    "        input_dim = x_train_final.shape[1]\n",
    "    \n",
    "        # Create model\n",
    "        weights_file = os.path.join(results_path, 'mlp_initial_weights.h5')\n",
    "        model_path = os.path.join(results_path_crr_split, 'mlp_model_trained.h5')\n",
    "    \n",
    "        # Save initial weights\n",
    "        if trial_num == 0:\n",
    "            model = create_mlp_model(input_dim,\n",
    "                                     hidden_layer_sizes=mlp_config_params.layer_sizes,\n",
    "                                     activation=mlp_config_params.activation,\n",
    "                                     dropout=mlp_config_params.dropout,\n",
    "                                     output_weights_file=weights_file)\n",
    "        else:\n",
    "            model = create_mlp_model(input_dim,\n",
    "                                     hidden_layer_sizes=mlp_config_params.layer_sizes,\n",
    "                                     activation=mlp_config_params.activation,\n",
    "                                     dropout=mlp_config_params.dropout)\n",
    "        model.summary()\n",
    "    \n",
    "        # Train model\n",
    "        history = train_mlp(model,\n",
    "                            x_train_final, y_train_split,\n",
    "                            x_val_final, y_val_split,\n",
    "                            weights_file=weights_file,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=get_callbacks(model_path))\n",
    "    \n",
    "        history_file = os.path.join(results_path_crr_split, f\"history_{trial_num}.pkl\")\n",
    "        plot_loss_curves(history.history)\n",
    "        save_object(history.history, history_file)\n",
    "    \n",
    "        # Performance evaluation\n",
    "        loaded_model = load_model(model_path)\n",
    "        predictions_test = loaded_model.predict(x_test_final).flatten()\n",
    "        mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)\n",
    "    \n",
    "        mse_vals.append(mse)\n",
    "        rmse_vals.append(rmse)\n",
    "        cmapss_vals.append(cmapss_score)\n",
    "    \n",
    "    mse_mean = np.mean(mse_vals)\n",
    "    mse_std = np.std(mse_vals)\n",
    "    rmse_mean = np.mean(rmse_vals)\n",
    "    rmse_std = np.std(rmse_vals)\n",
    "    cmapss_mean = np.mean(cmapss_vals)\n",
    "    cmapss_std = np.std(cmapss_vals)\n",
    "\n",
    "    if results_file is not None:\n",
    "        with open(results_file, \"a\") as file:\n",
    "            line_to_write = f\"{numbers_list_to_string(mse_vals)}, {numbers_list_to_string(rmse_vals)},\"\n",
    "            line_to_write += f\"{numbers_list_to_string(cmapss_vals)}, {mse_mean}, {mse_std}, {rmse_mean},\"\n",
    "            line_to_write += f\"{rmse_std}, {cmapss_mean}, {cmapss_std}\\n\"\n",
    "            file.write(line_to_write)\n",
    "    \n",
    "    print(\"MSE: mean = {:.2f}   stddev = {:.2f}\".format(mse_mean, mse_std))\n",
    "    print(\"RMSE: mean = {:.2f}   stddev = {:.2f}\".format(rmse_mean, rmse_std))\n",
    "    print(\"CMAPSS: mean = {:.2f}   stddev = {:.2f}\".format(cmapss_mean, cmapss_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation time (sec):  4.171875\n",
      "Train set shape: (5263447, 47)\n",
      "Test set shape: (1253743, 47)\n"
     ]
    }
   ],
   "source": [
    "data_reader = DataReader()\n",
    "\n",
    "start_time = time.process_time()  \n",
    "data_reader.load_dataset(filename, load_train=True, load_test=True)\n",
    "print(\"Operation time (sec): \" , (time.process_time() - start_time))\n",
    "\n",
    "if data_reader.train_set is not None:\n",
    "    print(\"Train set shape: \" + str(data_reader.train_set.shape))\n",
    "    \n",
    "if data_reader.test_set is not None:   \n",
    "    print(\"Test set shape: \" + str(data_reader.test_set.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data_reader.train_set\n",
    "test_set = data_reader.test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set['RUL']\n",
    "x_train = train_set.drop(['RUL'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_columns = data_reader.column_names.w_cols + data_reader.column_names.x_s_cols + data_reader.column_names.x_v_cols\n",
    "#x_train = x_train[selected_columns]\n",
    "\n",
    "y_test = test_set['RUL']\n",
    "x_test = test_set[x_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved object to file: ../results/experiment_set_18\\results_pca_a1\\split_0\\scaler.pkl\n",
      "Can reduce from 46 to 8 dimensions while retaining 0.99% of variance.\n",
      "Saved object to file: ../results/experiment_set_18\\results_pca_a1\\split_0\\pca.pkl\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               2304      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 232,577\n",
      "Trainable params: 232,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "9253/9253 [==============================] - ETA: 0s - loss: 73.7332\n",
      "Epoch 00001: val_loss improved from inf to 0.18970, saving model to ../results/experiment_set_18\\results_pca_a1\\split_0\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 39s 4ms/step - loss: 73.7332 - val_loss: 0.1897\n",
      "Epoch 2/60\n",
      "9249/9253 [============================>.] - ETA: 0s - loss: 0.0851\n",
      "Epoch 00002: val_loss improved from 0.18970 to 0.04112, saving model to ../results/experiment_set_18\\results_pca_a1\\split_0\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 0.0851 - val_loss: 0.0411\n",
      "Epoch 3/60\n",
      " 926/9253 [==>...........................] - ETA: 32s - loss: 0.0466"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# Test effect of PCA for feature extraction\n",
    "###########################################\n",
    "NUM_TRIALS = 3\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "mlp_params = MLPConfigParams(layer_sizes=[256, 256, 512, 64], activation='tanh', dropout=0.0)\n",
    "\n",
    "results_path = os.path.join(output_path, \"results_pca_a1\")\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "\n",
    "results_file = os.path.join(output_path, \"results_pca_a1.csv\")\n",
    "with open(results_file, \"w\") as file:\n",
    "    file.write(\"mse,rmse,cmapss,mse(mean),mse(std),rmse(mean),rmse(std),cmapss(mean),cmapss(std)\\n\")\n",
    "\n",
    "train_evaluate_mlp_pca(\n",
    "    x_train, y_train, \n",
    "    x_test, y_test,\n",
    "    NUM_TRIALS, \n",
    "    mlp_params, \n",
    "    results_path, \n",
    "    EPOCHS, BATCH_SIZE,\n",
    "    results_file=results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 3\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "mlp_params = MLPConfigParams(layer_sizes=[128, 256, 64], activation='tanh', dropout=0.05)\n",
    "\n",
    "results_path = os.path.join(output_path, \"results_pca_a2\")\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "\n",
    "results_file = os.path.join(output_path, \"results_pca_a2.csv\")\n",
    "with open(results_file, \"w\") as file:\n",
    "    file.write(\"mse,rmse,cmapss,mse(mean),mse(std),rmse(mean),rmse(std),cmapss(mean),cmapss(std)\\n\")\n",
    "\n",
    "train_evaluate_pca(\n",
    "    x_train, y_train, \n",
    "    x_test, y_test,\n",
    "    NUM_TRIALS, \n",
    "    mlp_params, \n",
    "    results_path, \n",
    "    EPOCHS, BATCH_SIZE,\n",
    "    results_file=results_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-keras-gpu",
   "language": "python",
   "name": "tf-keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
