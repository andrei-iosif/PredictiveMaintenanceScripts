{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "seed = 0\n",
    "os.environ['PYTHONHASSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../../data/turbofan_dataset/N-CMAPSS_DS02-006.h5'\n",
    "output_path = './experiment_set_12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename, load_test_set=True):\n",
    "    \"\"\" Reads a dataset from a given .h5 file and compose (in memory) the train and test data. \n",
    "    Args:\n",
    "        filename(str): path to the .h5 file\n",
    "    Returns:\n",
    "        train_set(pd.DataFrame), test_set(pd.DataFrame)\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'r') as hdf:\n",
    "        # Development set\n",
    "        W_dev = np.array(hdf.get('W_dev'))             # W\n",
    "        X_s_dev = np.array(hdf.get('X_s_dev'))         # X_s\n",
    "        X_v_dev = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "        T_dev = np.array(hdf.get('T_dev'))             # T\n",
    "        Y_dev = np.array(hdf.get('Y_dev'))             # RUL  \n",
    "        A_dev = np.array(hdf.get('A_dev'))             # Auxiliary\n",
    "\n",
    "        # Test set\n",
    "        if load_test_set:\n",
    "            W_test = np.array(hdf.get('W_test'))           # W\n",
    "            X_s_test = np.array(hdf.get('X_s_test'))       # X_s\n",
    "            X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "            T_test = np.array(hdf.get('T_test'))           # T\n",
    "            Y_test = np.array(hdf.get('Y_test'))           # RUL  \n",
    "            A_test = np.array(hdf.get('A_test'))           # Auxiliary\n",
    "        \n",
    "        # Column names\n",
    "        W_var = np.array(hdf.get('W_var'))\n",
    "        X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "        X_v_var = np.array(hdf.get('X_v_var')) \n",
    "        T_var = np.array(hdf.get('T_var'))\n",
    "        A_var = np.array(hdf.get('A_var'))\n",
    "        \n",
    "        columns = []\n",
    "        columns.append(list(np.array(A_var, dtype='U20')))\n",
    "        columns.append(list(np.array(T_var, dtype='U20')))\n",
    "        columns.append(list(np.array(X_s_var, dtype='U20')))\n",
    "        columns.append(list(np.array(X_v_var, dtype='U20')))\n",
    "        columns.append(list(np.array(W_var, dtype='U20')))\n",
    "        columns.append(['RUL'])\n",
    "        \n",
    "        columns_list = []\n",
    "        for columns_per_category in columns:\n",
    "            columns_list += columns_per_category\n",
    "        \n",
    "    train_set = np.concatenate((A_dev, T_dev, X_s_dev, X_v_dev, W_dev, Y_dev), axis=1)\n",
    "    if load_test_set:\n",
    "        test_set = np.concatenate((A_test, T_test, X_s_test, X_v_test, W_test, Y_test), axis=1)\n",
    "        return pd.DataFrame(data=train_set, columns=columns_list), pd.DataFrame(data=test_set, columns=columns_list), columns\n",
    "    else:\n",
    "        return pd.DataFrame(data=train_set, columns=columns_list), None, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_cycle_info(df, compute_cycle_len=False):\n",
    "    unit_ids = np.unique(df['unit'])\n",
    "    print('Engine units in df: ', unit_ids)\n",
    "    for i in unit_ids:\n",
    "        num_cycles = len(np.unique(df.loc[df['unit'] == i, 'cycle']))\n",
    "        print('Unit: ', i, ' - Number of flight cycles: ', num_cycles)\n",
    "        \n",
    "    if compute_cycle_len:\n",
    "        cycle_ids = np.unique(df['cycle'])\n",
    "        print('Total number of cycles: ', len(cycle_ids))\n",
    "        min_len = np.inf\n",
    "        max_len = 0\n",
    "        for i in cycle_ids:\n",
    "            cycle_len = len(df.loc[df['cycle'] == i, 'cycle'])\n",
    "            if cycle_len < min_len:\n",
    "                min_len = cycle_len\n",
    "            elif cycle_len > max_len:\n",
    "                max_len = cycle_len\n",
    "        print('Min cycle length: ', min_len)\n",
    "        print('Max cycle length: ', max_len)\n",
    "    \n",
    "    return unit_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter constant and quasi-constant features\n",
    "def get_quasi_constant_features(dataset, variance_th=0.01, debug=True):\n",
    "    constant_filter = VarianceThreshold(threshold=variance_th)\n",
    "    constant_filter.fit(dataset)\n",
    "    constant_features = [col for col in dataset.columns \n",
    "                         if col not in dataset.columns[constant_filter.get_support()]]\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Number of non-constant features: \", len(dataset.columns[constant_filter.get_support()]))\n",
    "        \n",
    "        print(\"Number of quasi-constant features: \", len(constant_features))\n",
    "        print(\"Quasi-constant features: \")\n",
    "        for col in constant_features:\n",
    "            print(col)\n",
    "    return constant_features\n",
    "\n",
    "def get_non_correlated_features(dataset, corr_th=0.9, debug=True):\n",
    "    corr_mat = dataset.corr()\n",
    "    corr_mat = np.abs(corr_mat)\n",
    "    \n",
    "    num_cols = corr_mat.shape[0]\n",
    "    columns = np.full((num_cols,), True, dtype=bool)\n",
    "    for i in range(num_cols):\n",
    "        for j in range(i+1, num_cols):\n",
    "            val = corr_mat.iloc[i, j]\n",
    "            if val >= corr_th:\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "                    if debug:\n",
    "                        print(dataset.columns[i], \"|\", dataset.columns[j], \"|\", round(val, 2))\n",
    "    if debug:        \n",
    "        correlated_features = dataset.columns[~columns]\n",
    "        print(\"Number of correlated features: \", len(correlated_features))\n",
    "        print(\"Correlated features: \", list(correlated_features))\n",
    "    \n",
    "    selected_columns = dataset.columns[columns]\n",
    "    if debug:\n",
    "        print(\"Number of selected features: \", len(selected_columns))\n",
    "        print(\"Selected features: \", list(selected_columns))\n",
    "    return selected_columns\n",
    "\n",
    "def cmapss_score_function(actual, predictions, normalize=True):\n",
    "    # diff < 0 -> over-estimation\n",
    "    # diff > 0 -> under-estimation\n",
    "    diff = actual - predictions\n",
    "    alpha = np.full_like(diff, 1/13)\n",
    "    negative_diff_mask = diff < 0\n",
    "    alpha[negative_diff_mask] = 1/10\n",
    "    score = np.sum(np.exp(alpha * np.abs(diff)))\n",
    "    \n",
    "    if normalize:\n",
    "        N = len(predictions)\n",
    "        score /= N\n",
    "    return score\n",
    "\n",
    "def compute_evaluation_metrics(actual, predictions, label='Test'):\n",
    "    mse = mean_squared_error(actual, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    cmapss_score = cmapss_score_function(actual, predictions)\n",
    "    print('{} set:\\nMSE: {:.2f}\\nRMSE: {:.2f}\\nCMAPSS score: {:.2f}\\n'.format(label, mse, rmse, \n",
    "                                                                     cmapss_score))\n",
    "    return mse, rmse, cmapss_score\n",
    "    \n",
    "def plot_loss_curves(history, output_path=None, y_lim=[0, 150]):\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim(y_lim)\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    \n",
    "    if output_path is not None:\n",
    "        plt.savefig(os.path.join(output_path, 'loss_curves.png'), format='png', dpi=300) \n",
    "    plt.show()\n",
    "    \n",
    "def plot_rul(expected, predicted):\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(expected)), expected, label='Expected')\n",
    "    plt.plot(range(len(predicted)), predicted, label='Predicted')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def create_mlp_model(input_dim, hidden_layer_sizes, activation='relu', output_weights_file=None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_sizes[0], \n",
    "                    input_dim=input_dim, \n",
    "                    kernel_initializer='random_normal', \n",
    "                    activation=activation))\n",
    "\n",
    "    for layer_size in hidden_layer_sizes[1:]:\n",
    "        model.add(Dense(layer_size, \n",
    "                        kernel_initializer='random_normal', \n",
    "                        activation=activation))\n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer='random_normal'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    if output_weights_file is not None:\n",
    "        model.save_weights(output_weights_file)\n",
    "    return model\n",
    "\n",
    "def train_model_existing_weights(model, weights_file, x_train, y_train, x_val, y_val, epochs=200, batch_size=512, callbacks=[]):\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.load_weights(weights_file)\n",
    "    return model.fit(x_train, y_train,\n",
    "                     validation_data=(x_val, y_val),\n",
    "                     epochs=epochs,\n",
    "                     batch_size=batch_size,\n",
    "                     verbose=1,\n",
    "                     callbacks=callbacks)\n",
    "\n",
    "def save_history(history, output_file=os.path.join(output_path, \"history.pkl\")):\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(history.history, file)\n",
    "    print(\"Saved training history to file: {}\".format(output_file))\n",
    "\n",
    "def load_history(file):\n",
    "    return pickle.load(open(file, \"rb\"))\n",
    "\n",
    "def save_object(obj, output_file):\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "    print(\"Saved object to file: {}\".format(output_file))\n",
    "    \n",
    "def load_object(file):\n",
    "    return pickle.load(open(file, \"rb\"))\n",
    "\n",
    "def model_evaluation(model, x_test, y_test, x_train=None, y_train=None, plot_range=[0, 10**3]):\n",
    "    if x_train is not None and y_train is not None:\n",
    "        predictions_train = model.predict(x_train).flatten()\n",
    "        compute_evaluation_metrics(predictions_train, y_train, 'Train')\n",
    "        \n",
    "        expected = y_train[plot_range[0]:plot_range[1]]\n",
    "        predicted = predictions_train[plot_range[0]:plot_range[1]]\n",
    "        plot_rul(expected, predicted)\n",
    "        \n",
    "    predictions_test = model.predict(x_test).flatten()\n",
    "    compute_evaluation_metrics(predictions_test, y_test)\n",
    "    \n",
    "    expected = y_test[plot_range[0]:plot_range[1]]\n",
    "    predicted = predictions_test[plot_range[0]:plot_range[1]]\n",
    "    plot_rul(expected, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list(string_list, output_file):\n",
    "    output_file.write(\"[\")\n",
    "    n = len(string_list)\n",
    "    for i in range(n - 1):\n",
    "        output_file.write(\"{}, \".format(string_list[i]))\n",
    "    output_file.write(\"{}]\\n\".format(string_list[-1]))\n",
    "    \n",
    "def feature_list_to_string(feature_list):\n",
    "    return \"__\".join(feature_list)\n",
    "\n",
    "def numbers_list_to_string(num_list):\n",
    "    return \" \".join([str(x) for x in num_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mi_ranked_features(mi, n):\n",
    "    mi_sorted = mi.sort_values(by=\"MI\", ascending=False)\n",
    "    return mi[:n][\"Col\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mutual_info(X, y, percentage=0.3):\n",
    "    _, x_mi, _, y_mi = train_test_split(X, y, test_size=percentage, random_state=42)\n",
    "    \n",
    "    start_time = time.process_time()  \n",
    "    mutual_info = mutual_info_regression(x_mi, y_mi, random_state=seed, discrete_features=False)\n",
    "    print(\"Operation time (sec): \" , (time.process_time() - start_time))\n",
    "\n",
    "    mi_series = pd.Series(mutual_info, index=x_train.columns)\n",
    "    mi_series = mi_series.sort_values(ascending=False)\n",
    "    return mi_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation time (sec):  4.625\n",
      "Train set shape: (5263447, 47)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.process_time()  \n",
    "train_set, test_set, columns = load_dataset(filename)\n",
    "print(\"Operation time (sec): \" , (time.process_time() - start_time))\n",
    "print(\"Train set shape: \" + str(train_set.shape))\n",
    "\n",
    "columns_aux = columns[0] \n",
    "columns_health_params = columns[1] \n",
    "columns_sensor_measurements = columns[2] \n",
    "columns_virtual_sensors = columns[3]\n",
    "columns_operating_conditions = columns[4] \n",
    "target_col = columns[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set['RUL']\n",
    "x_train = train_set.drop(['RUL'], axis=1)\n",
    "\n",
    "y_test = test_set['RUL']\n",
    "x_test = test_set.drop(['RUL'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_columns = columns_operating_conditions + columns_sensor_measurements + columns_virtual_sensors\n",
    "# x_train = x_train[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutual_info = compute_mutual_info(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutual_info_norm = (mutual_info - mutual_info.min()) / (mutual_info.max() - mutual_info.min())\n",
    "\n",
    "# mutual_info_norm[:15].plot.bar(color='steelblue', figsize=(12, 6))\n",
    "# plt.savefig(os.path.join(output_path, \"mutual_info.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutual_info_norm.to_csv(os.path.join(output_path, \"mutual_info.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info_series = pd.read_csv(os.path.join(output_path, \"mutual_info.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info_norm[:15].plot.bar(color='steelblue', figsize=(12, 6))\n",
    "plt.xlabel()\n",
    "plt.savefig(os.path.join(output_path, \"mutual_info.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_holdout, y_train, y_holdout = train_test_split(x_train, \n",
    "                                                          y_train, \n",
    "                                                          test_size=0.3, \n",
    "                                                          random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved object to file: ./experiment_set_12\\results_10\\split_0\\scaler.pkl\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               2816      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 233,089\n",
      "Trainable params: 233,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 538.2395\n",
      "Epoch 00001: val_loss improved from inf to 499.78467, saving model to ./experiment_set_12\\results_10\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 538.2144 - val_loss: 499.7847\n",
      "Epoch 2/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 500.6182\n",
      "Epoch 00002: val_loss did not improve from 499.78467\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6118 - val_loss: 499.7884\n",
      "Epoch 3/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.6153\n",
      "Epoch 00003: val_loss improved from 499.78467 to 499.77985, saving model to ./experiment_set_12\\results_10\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6154 - val_loss: 499.7798\n",
      "Epoch 4/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 500.6071\n",
      "Epoch 00004: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6071 - val_loss: 499.7822\n",
      "Epoch 5/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 500.6187\n",
      "Epoch 00005: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6134 - val_loss: 499.8065\n",
      "Epoch 6/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.6129\n",
      "Epoch 00006: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6162 - val_loss: 499.8934\n",
      "Epoch 7/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 500.6165\n",
      "Epoch 00007: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6158 - val_loss: 499.8108\n",
      "Epoch 8/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 500.6219\n",
      "Epoch 00008: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 500.6131 - val_loss: 499.7953\n",
      "Epoch 9/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.6067\n",
      "Epoch 00009: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6174 - val_loss: 499.7916\n",
      "Epoch 10/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 500.5969\n",
      "Epoch 00010: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6086 - val_loss: 499.9529\n",
      "Epoch 11/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6079\n",
      "Epoch 00011: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6147 - val_loss: 499.7838\n",
      "Epoch 12/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 500.6168\n",
      "Epoch 00012: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6122 - val_loss: 499.8415\n",
      "Epoch 13/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.6128\n",
      "Epoch 00013: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6143 - val_loss: 499.8130\n",
      "Epoch 00013: early stopping\n",
      "Saved training history to file: ./experiment_set_12\\results_10\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 500.62\n",
      "RMSE: 22.37\n",
      "CMAPSS score: 10.57\n",
      "\n",
      "Saved object to file: ./experiment_set_12\\results_10\\split_1\\scaler.pkl\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 256)               2816      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 233,089\n",
      "Trainable params: 233,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 538.0760- E\n",
      "Epoch 00001: val_loss improved from inf to 499.62741, saving model to ./experiment_set_12\\results_10\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 537.9747 - val_loss: 499.6274\n",
      "Epoch 2/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.9297\n",
      "Epoch 00002: val_loss improved from 499.62741 to 499.62195, saving model to ./experiment_set_12\\results_10\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.9166 - val_loss: 499.6219\n",
      "Epoch 3/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 500.6217\n",
      "Epoch 00003: val_loss improved from 499.62195 to 499.56287, saving model to ./experiment_set_12\\results_10\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 500.6402 - val_loss: 499.5629\n",
      "Epoch 4/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 500.6502\n",
      "Epoch 00004: val_loss did not improve from 499.56287\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6392 - val_loss: 499.6204\n",
      "Epoch 5/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 500.6406\n",
      "Epoch 00005: val_loss did not improve from 499.56287\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6406 - val_loss: 499.5656\n",
      "Epoch 6/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 500.6424\n",
      "Epoch 00006: val_loss improved from 499.56287 to 499.56268, saving model to ./experiment_set_12\\results_10\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6441 - val_loss: 499.5627\n",
      "Epoch 7/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6348\n",
      "Epoch 00007: val_loss improved from 499.56268 to 499.54861, saving model to ./experiment_set_12\\results_10\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6383 - val_loss: 499.5486\n",
      "Epoch 8/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 500.6354- ETA: 0s - loss\n",
      "Epoch 00008: val_loss did not improve from 499.54861\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6365 - val_loss: 499.7227\n",
      "Epoch 9/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 500.6140\n",
      "Epoch 00009: val_loss did not improve from 499.54861\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6367 - val_loss: 499.5886\n",
      "Epoch 10/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 500.6313\n",
      "Epoch 00010: val_loss did not improve from 499.54861\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6364 - val_loss: 499.6105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 500.6428\n",
      "Epoch 00011: val_loss did not improve from 499.54861\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6397 - val_loss: 499.5543\n",
      "Epoch 12/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.6357\n",
      "Epoch 00012: val_loss did not improve from 499.54861\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6372 - val_loss: 499.5635\n",
      "Epoch 13/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 500.6503\n",
      "Epoch 00013: val_loss did not improve from 499.54861\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6404 - val_loss: 499.6306\n",
      "Epoch 14/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 500.6498\n",
      "Epoch 00014: val_loss improved from 499.54861 to 499.54855, saving model to ./experiment_set_12\\results_10\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6448 - val_loss: 499.5486\n",
      "Epoch 15/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 500.6482\n",
      "Epoch 00015: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6399 - val_loss: 499.5558\n",
      "Epoch 16/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 500.6178\n",
      "Epoch 00016: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6382 - val_loss: 499.6561\n",
      "Epoch 17/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 500.6360\n",
      "Epoch 00017: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6412 - val_loss: 499.5572\n",
      "Epoch 18/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 500.6327\n",
      "Epoch 00018: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6403 - val_loss: 499.6749\n",
      "Epoch 19/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.6469\n",
      "Epoch 00019: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6389 - val_loss: 499.5602\n",
      "Epoch 20/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 500.6377\n",
      "Epoch 00020: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6421 - val_loss: 499.5586\n",
      "Epoch 21/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.6377\n",
      "Epoch 00021: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6369 - val_loss: 499.5527\n",
      "Epoch 22/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 500.6426\n",
      "Epoch 00022: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6412 - val_loss: 499.5556\n",
      "Epoch 23/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 500.6400- ETA: 0s - \n",
      "Epoch 00023: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6419 - val_loss: 499.5740\n",
      "Epoch 24/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 500.6469\n",
      "Epoch 00024: val_loss improved from 499.54855 to 499.54813, saving model to ./experiment_set_12\\results_10\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6398 - val_loss: 499.5481\n",
      "Epoch 25/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 500.6396\n",
      "Epoch 00025: val_loss did not improve from 499.54813\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6396 - val_loss: 499.5527\n",
      "Epoch 26/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 500.6245\n",
      "Epoch 00026: val_loss improved from 499.54813 to 499.54800, saving model to ./experiment_set_12\\results_10\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6399 - val_loss: 499.5480\n",
      "Epoch 27/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 500.6322\n",
      "Epoch 00027: val_loss improved from 499.54800 to 499.54794, saving model to ./experiment_set_12\\results_10\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6416 - val_loss: 499.5479\n",
      "Epoch 28/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6313\n",
      "Epoch 00028: val_loss did not improve from 499.54794\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6383 - val_loss: 499.6637\n",
      "Epoch 29/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.6513- ETA: 0s - loss: 500.6\n",
      "Epoch 00029: val_loss did not improve from 499.54794\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6392 - val_loss: 499.5517\n",
      "Epoch 30/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 500.6347- ETA: 0s - loss: 500\n",
      "Epoch 00030: val_loss did not improve from 499.54794\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6378 - val_loss: 499.5585\n",
      "Epoch 31/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 500.6438\n",
      "Epoch 00031: val_loss did not improve from 499.54794\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6395 - val_loss: 499.5614\n",
      "Epoch 32/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.6307\n",
      "Epoch 00032: val_loss did not improve from 499.54794\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6393 - val_loss: 499.5896\n",
      "Epoch 33/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 500.6108\n",
      "Epoch 00033: val_loss improved from 499.54794 to 499.54764, saving model to ./experiment_set_12\\results_10\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6379 - val_loss: 499.5476\n",
      "Epoch 34/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 500.6354\n",
      "Epoch 00034: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6457 - val_loss: 499.6257\n",
      "Epoch 35/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 500.6502\n",
      "Epoch 00035: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6416 - val_loss: 499.5542\n",
      "Epoch 36/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 500.6359\n",
      "Epoch 00036: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6359 - val_loss: 499.5823\n",
      "Epoch 37/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 500.6442\n",
      "Epoch 00037: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6370 - val_loss: 499.5711\n",
      "Epoch 38/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6450\n",
      "Epoch 00038: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6410 - val_loss: 499.5476\n",
      "Epoch 39/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6395\n",
      "Epoch 00039: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6391 - val_loss: 499.6147\n",
      "Epoch 40/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 500.6467\n",
      "Epoch 00040: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6471 - val_loss: 499.6336\n",
      "Epoch 41/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 500.6488\n",
      "Epoch 00041: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6393 - val_loss: 499.5514\n",
      "Epoch 42/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 500.6303\n",
      "Epoch 00042: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6428 - val_loss: 499.5847\n",
      "Epoch 43/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 500.6528\n",
      "Epoch 00043: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6396 - val_loss: 499.5493\n",
      "Epoch 00043: early stopping\n",
      "Saved training history to file: ./experiment_set_12\\results_10\\split_1\\history.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "MSE: 500.62\n",
      "RMSE: 22.37\n",
      "CMAPSS score: 10.57\n",
      "\n",
      "Saved object to file: ./experiment_set_12\\results_10\\split_2\\scaler.pkl\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 256)               2816      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 233,089\n",
      "Trainable params: 233,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 537.5331\n",
      "Epoch 00001: val_loss improved from inf to 500.05591, saving model to ./experiment_set_12\\results_10\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 537.5004 - val_loss: 500.0559\n",
      "Epoch 2/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 500.5838\n",
      "Epoch 00002: val_loss improved from 500.05591 to 500.04437, saving model to ./experiment_set_12\\results_10\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5883 - val_loss: 500.0444\n",
      "Epoch 3/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.5846\n",
      "Epoch 00003: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5835 - val_loss: 500.0604\n",
      "Epoch 4/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 500.5764\n",
      "Epoch 00004: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5832 - val_loss: 500.1666\n",
      "Epoch 5/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 500.5708\n",
      "Epoch 00005: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5791 - val_loss: 500.0476\n",
      "Epoch 6/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.5829\n",
      "Epoch 00006: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5894 - val_loss: 500.0475\n",
      "Epoch 7/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.6066\n",
      "Epoch 00007: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6017 - val_loss: 500.0561\n",
      "Epoch 8/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 500.5815\n",
      "Epoch 00008: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5815 - val_loss: 500.0839\n",
      "Epoch 9/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.5811\n",
      "Epoch 00009: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5857 - val_loss: 500.0773\n",
      "Epoch 10/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 500.5776\n",
      "Epoch 00010: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5827 - val_loss: 500.0529\n",
      "Epoch 11/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 500.5871\n",
      "Epoch 00011: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5810 - val_loss: 500.0457\n",
      "Epoch 12/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.5977\n",
      "Epoch 00012: val_loss improved from 500.04437 to 500.04413, saving model to ./experiment_set_12\\results_10\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5865 - val_loss: 500.0441\n",
      "Epoch 13/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.5774\n",
      "Epoch 00013: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5804 - val_loss: 500.0654\n",
      "Epoch 14/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.5903\n",
      "Epoch 00014: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5886 - val_loss: 500.0655\n",
      "Epoch 15/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 500.5798- ETA: 0s \n",
      "Epoch 00015: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.5820 - val_loss: 500.0684\n",
      "Epoch 16/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.5846\n",
      "Epoch 00016: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.5824 - val_loss: 500.1144\n",
      "Epoch 17/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 500.5705\n",
      "Epoch 00017: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5871 - val_loss: 500.0939\n",
      "Epoch 18/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.5813\n",
      "Epoch 00018: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5747 - val_loss: 500.1266\n",
      "Epoch 19/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 500.5945\n",
      "Epoch 00019: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5842 - val_loss: 500.1061\n",
      "Epoch 20/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 500.5807\n",
      "Epoch 00020: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.5833 - val_loss: 500.0774\n",
      "Epoch 21/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 500.5915\n",
      "Epoch 00021: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.5915 - val_loss: 500.0479\n",
      "Epoch 22/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 500.5771\n",
      "Epoch 00022: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5820 - val_loss: 500.0930\n",
      "Epoch 00022: early stopping\n",
      "Saved training history to file: ./experiment_set_12\\results_10\\split_2\\history.pkl\n",
      "Test set:\n",
      "MSE: 500.62\n",
      "RMSE: 22.37\n",
      "CMAPSS score: 10.54\n",
      "\n",
      "Saved object to file: ./experiment_set_12\\results_15\\split_0\\scaler.pkl\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 256)               4096      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 234,369\n",
      "Trainable params: 234,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 538.3765- ETA: 0s - loss:\n",
      "Epoch 00001: val_loss improved from inf to 499.78464, saving model to ./experiment_set_12\\results_15\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 538.3331 - val_loss: 499.7846\n",
      "Epoch 2/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 501.1164\n",
      "Epoch 00002: val_loss did not improve from 499.78464\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 501.1139 - val_loss: 499.7883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 500.6111\n",
      "Epoch 00003: val_loss improved from 499.78464 to 499.77985, saving model to ./experiment_set_12\\results_15\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6154 - val_loss: 499.7798\n",
      "Epoch 4/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 500.6194\n",
      "Epoch 00004: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6071 - val_loss: 499.7822\n",
      "Epoch 5/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 500.6173\n",
      "Epoch 00005: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6134 - val_loss: 499.8065\n",
      "Epoch 6/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6201\n",
      "Epoch 00006: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6162 - val_loss: 499.8934\n",
      "Epoch 7/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 500.6187\n",
      "Epoch 00007: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6158 - val_loss: 499.8108\n",
      "Epoch 8/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 500.6224\n",
      "Epoch 00008: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6131 - val_loss: 499.7953\n",
      "Epoch 9/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6130\n",
      "Epoch 00009: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6174 - val_loss: 499.7916\n",
      "Epoch 10/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 500.5937\n",
      "Epoch 00010: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6086 - val_loss: 499.9529\n",
      "Epoch 11/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6079\n",
      "Epoch 00011: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6146 - val_loss: 499.7838\n",
      "Epoch 12/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 500.6199\n",
      "Epoch 00012: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6122 - val_loss: 499.8415\n",
      "Epoch 13/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 500.6114\n",
      "Epoch 00013: val_loss did not improve from 499.77985\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6143 - val_loss: 499.8129\n",
      "Epoch 00013: early stopping\n",
      "Saved training history to file: ./experiment_set_12\\results_15\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 500.62\n",
      "RMSE: 22.37\n",
      "CMAPSS score: 10.57\n",
      "\n",
      "Saved object to file: ./experiment_set_12\\results_15\\split_1\\scaler.pkl\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 256)               4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 234,369\n",
      "Trainable params: 234,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 539.0242\n",
      "Epoch 00001: val_loss improved from inf to 499.62756, saving model to ./experiment_set_12\\results_15\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 539.0242 - val_loss: 499.6276\n",
      "Epoch 2/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.6430- ETA:\n",
      "Epoch 00002: val_loss did not improve from 499.62756\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6401 - val_loss: 499.6586\n",
      "Epoch 3/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 500.6352\n",
      "Epoch 00003: val_loss improved from 499.62756 to 499.56281, saving model to ./experiment_set_12\\results_15\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6407 - val_loss: 499.5628\n",
      "Epoch 4/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.6475\n",
      "Epoch 00004: val_loss did not improve from 499.56281\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6359 - val_loss: 499.6204\n",
      "Epoch 5/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 500.6454\n",
      "Epoch 00005: val_loss did not improve from 499.56281\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6406 - val_loss: 499.5656\n",
      "Epoch 6/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 500.6424\n",
      "Epoch 00006: val_loss improved from 499.56281 to 499.56277, saving model to ./experiment_set_12\\results_15\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6441 - val_loss: 499.5628\n",
      "Epoch 7/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 500.6365\n",
      "Epoch 00007: val_loss improved from 499.56277 to 499.54861, saving model to ./experiment_set_12\\results_15\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6383 - val_loss: 499.5486\n",
      "Epoch 8/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 500.6418\n",
      "Epoch 00008: val_loss did not improve from 499.54861\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6365 - val_loss: 499.7227\n",
      "Epoch 9/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.6227\n",
      "Epoch 00009: val_loss did not improve from 499.54861\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6367 - val_loss: 499.5886\n",
      "Epoch 10/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 500.6422\n",
      "Epoch 00010: val_loss did not improve from 499.54861\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6364 - val_loss: 499.6106\n",
      "Epoch 11/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6485\n",
      "Epoch 00011: val_loss did not improve from 499.54861\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6397 - val_loss: 499.5543\n",
      "Epoch 12/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.6347\n",
      "Epoch 00012: val_loss did not improve from 499.54861\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6373 - val_loss: 499.5635\n",
      "Epoch 13/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 500.6454\n",
      "Epoch 00013: val_loss did not improve from 499.54861\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6404 - val_loss: 499.6307\n",
      "Epoch 14/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 500.6454\n",
      "Epoch 00014: val_loss improved from 499.54861 to 499.54855, saving model to ./experiment_set_12\\results_15\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6447 - val_loss: 499.5486\n",
      "Epoch 15/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6415\n",
      "Epoch 00015: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6399 - val_loss: 499.5558\n",
      "Epoch 16/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 500.6166\n",
      "Epoch 00016: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6382 - val_loss: 499.6561\n",
      "Epoch 17/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 500.6373\n",
      "Epoch 00017: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6412 - val_loss: 499.5572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 500.6280\n",
      "Epoch 00018: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6403 - val_loss: 499.6749\n",
      "Epoch 19/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 500.6389- E\n",
      "Epoch 00019: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6389 - val_loss: 499.5602\n",
      "Epoch 20/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 500.6490\n",
      "Epoch 00020: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6421 - val_loss: 499.5587\n",
      "Epoch 21/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 500.6369- ETA: 0s - loss: 500.6\n",
      "Epoch 00021: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6369 - val_loss: 499.5527\n",
      "Epoch 22/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 500.6412\n",
      "Epoch 00022: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6412 - val_loss: 499.5556\n",
      "Epoch 23/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 500.6350\n",
      "Epoch 00023: val_loss did not improve from 499.54855\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6419 - val_loss: 499.5740\n",
      "Epoch 24/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.6465\n",
      "Epoch 00024: val_loss improved from 499.54855 to 499.54822, saving model to ./experiment_set_12\\results_15\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6398 - val_loss: 499.5482\n",
      "Epoch 25/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6395\n",
      "Epoch 00025: val_loss did not improve from 499.54822\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6396 - val_loss: 499.5527\n",
      "Epoch 26/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.6358\n",
      "Epoch 00026: val_loss improved from 499.54822 to 499.54800, saving model to ./experiment_set_12\\results_15\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6399 - val_loss: 499.5480\n",
      "Epoch 27/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6442\n",
      "Epoch 00027: val_loss improved from 499.54800 to 499.54794, saving model to ./experiment_set_12\\results_15\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6416 - val_loss: 499.5479\n",
      "Epoch 28/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6313\n",
      "Epoch 00028: val_loss did not improve from 499.54794\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6383 - val_loss: 499.6637\n",
      "Epoch 29/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 500.6417\n",
      "Epoch 00029: val_loss did not improve from 499.54794\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6392 - val_loss: 499.5517\n",
      "Epoch 30/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.6409\n",
      "Epoch 00030: val_loss did not improve from 499.54794\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6378 - val_loss: 499.5585\n",
      "Epoch 31/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6422\n",
      "Epoch 00031: val_loss did not improve from 499.54794\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6395 - val_loss: 499.5614\n",
      "Epoch 32/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.6241\n",
      "Epoch 00032: val_loss did not improve from 499.54794\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6393 - val_loss: 499.5896\n",
      "Epoch 33/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.6270\n",
      "Epoch 00033: val_loss improved from 499.54794 to 499.54764, saving model to ./experiment_set_12\\results_15\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6379 - val_loss: 499.5476\n",
      "Epoch 34/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.6366\n",
      "Epoch 00034: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6457 - val_loss: 499.6257\n",
      "Epoch 35/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 500.6421\n",
      "Epoch 00035: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.6416 - val_loss: 499.5542\n",
      "Epoch 36/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.6357\n",
      "Epoch 00036: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6359 - val_loss: 499.5823\n",
      "Epoch 37/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6370\n",
      "Epoch 00037: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6370 - val_loss: 499.5710\n",
      "Epoch 38/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 500.6534\n",
      "Epoch 00038: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6410 - val_loss: 499.5476\n",
      "Epoch 39/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.6395\n",
      "Epoch 00039: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6391 - val_loss: 499.6147\n",
      "Epoch 40/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 500.6461\n",
      "Epoch 00040: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6471 - val_loss: 499.6336\n",
      "Epoch 41/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.6451\n",
      "Epoch 00041: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 29s 4ms/step - loss: 500.6393 - val_loss: 499.5514\n",
      "Epoch 42/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 500.6255\n",
      "Epoch 00042: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6428 - val_loss: 499.5847\n",
      "Epoch 43/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.6430\n",
      "Epoch 00043: val_loss did not improve from 499.54764\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 500.6396 - val_loss: 499.5492\n",
      "Epoch 00043: early stopping\n",
      "Saved training history to file: ./experiment_set_12\\results_15\\split_1\\history.pkl\n",
      "Test set:\n",
      "MSE: 500.62\n",
      "RMSE: 22.37\n",
      "CMAPSS score: 10.57\n",
      "\n",
      "Saved object to file: ./experiment_set_12\\results_15\\split_2\\scaler.pkl\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               4096      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 234,369\n",
      "Trainable params: 234,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 537.4386\n",
      "Epoch 00001: val_loss improved from inf to 500.05600, saving model to ./experiment_set_12\\results_15\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 537.4386 - val_loss: 500.0560\n",
      "Epoch 2/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 500.5924\n",
      "Epoch 00002: val_loss improved from 500.05600 to 500.04437, saving model to ./experiment_set_12\\results_15\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5882 - val_loss: 500.0444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 500.5779\n",
      "Epoch 00003: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5835 - val_loss: 500.0604\n",
      "Epoch 4/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 501.2915\n",
      "Epoch 00004: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 501.3034 - val_loss: 500.1663\n",
      "Epoch 5/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.5758\n",
      "Epoch 00005: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.5867 - val_loss: 500.0476\n",
      "Epoch 6/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 500.6578\n",
      "Epoch 00006: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.6739 - val_loss: 500.0475\n",
      "Epoch 7/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 500.5889\n",
      "Epoch 00007: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5872 - val_loss: 500.0561\n",
      "Epoch 8/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 500.5785\n",
      "Epoch 00008: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5815 - val_loss: 500.0839\n",
      "Epoch 9/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.5811\n",
      "Epoch 00009: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5857 - val_loss: 500.0774\n",
      "Epoch 10/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 500.5692\n",
      "Epoch 00010: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.5828 - val_loss: 500.0529\n",
      "Epoch 11/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.5879\n",
      "Epoch 00011: val_loss did not improve from 500.04437\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.5810 - val_loss: 500.0457\n",
      "Epoch 12/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 500.6078\n",
      "Epoch 00012: val_loss improved from 500.04437 to 500.04413, saving model to ./experiment_set_12\\results_15\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5865 - val_loss: 500.0441\n",
      "Epoch 13/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 500.5725\n",
      "Epoch 00013: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5804 - val_loss: 500.0654\n",
      "Epoch 14/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 500.5931\n",
      "Epoch 00014: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.5886 - val_loss: 500.0655\n",
      "Epoch 15/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 500.5775\n",
      "Epoch 00015: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5820 - val_loss: 500.0684\n",
      "Epoch 16/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 500.5679\n",
      "Epoch 00016: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.5824 - val_loss: 500.1144\n",
      "Epoch 17/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.5749\n",
      "Epoch 00017: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.5871 - val_loss: 500.0939\n",
      "Epoch 18/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 500.5746\n",
      "Epoch 00018: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5746 - val_loss: 500.1266\n",
      "Epoch 19/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 500.6025\n",
      "Epoch 00019: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 500.5842 - val_loss: 500.1061\n",
      "Epoch 20/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 500.5793\n",
      "Epoch 00020: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.5833 - val_loss: 500.0774\n",
      "Epoch 21/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 500.5921\n",
      "Epoch 00021: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.5915 - val_loss: 500.0480\n",
      "Epoch 22/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 500.5771\n",
      "Epoch 00022: val_loss did not improve from 500.04413\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 500.5820 - val_loss: 500.0929\n",
      "Epoch 00022: early stopping\n",
      "Saved training history to file: ./experiment_set_12\\results_15\\split_2\\history.pkl\n",
      "Test set:\n",
      "MSE: 500.62\n",
      "RMSE: 22.37\n",
      "CMAPSS score: 10.54\n",
      "\n",
      "Saved object to file: ./experiment_set_12\\results_20\\split_0\\scaler.pkl\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 256)               5376      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 235,649\n",
      "Trainable params: 235,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 145.5997\n",
      "Epoch 00001: val_loss improved from inf to 52.60096, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 145.4347 - val_loss: 52.6010\n",
      "Epoch 2/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 46.9589\n",
      "Epoch 00002: val_loss improved from 52.60096 to 42.49833, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 46.9493 - val_loss: 42.4983\n",
      "Epoch 3/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 41.0891\n",
      "Epoch 00003: val_loss improved from 42.49833 to 38.44769, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 41.0889 - val_loss: 38.4477\n",
      "Epoch 4/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 37.9343\n",
      "Epoch 00004: val_loss improved from 38.44769 to 34.84269, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 37.9343 - val_loss: 34.8427\n",
      "Epoch 5/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 34.9802\n",
      "Epoch 00005: val_loss improved from 34.84269 to 32.98435, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 34.9825 - val_loss: 32.9843\n",
      "Epoch 6/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 33.1928\n",
      "Epoch 00006: val_loss improved from 32.98435 to 32.71799, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 33.1930 - val_loss: 32.7180\n",
      "Epoch 7/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 31.5971\n",
      "Epoch 00007: val_loss improved from 32.71799 to 29.60247, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 31.5950 - val_loss: 29.6025\n",
      "Epoch 8/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 30.4523\n",
      "Epoch 00008: val_loss did not improve from 29.60247\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 30.4538 - val_loss: 31.1921\n",
      "Epoch 9/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 29.2869\n",
      "Epoch 00009: val_loss did not improve from 29.60247\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 29.2861 - val_loss: 29.7295\n",
      "Epoch 10/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 28.6050\n",
      "Epoch 00010: val_loss improved from 29.60247 to 26.64030, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 28.6059 - val_loss: 26.6403\n",
      "Epoch 11/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 27.8301\n",
      "Epoch 00011: val_loss did not improve from 26.64030\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 27.8297 - val_loss: 27.1109\n",
      "Epoch 12/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 27.2834\n",
      "Epoch 00012: val_loss did not improve from 26.64030\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 27.2819 - val_loss: 27.4536\n",
      "Epoch 13/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 26.7726\n",
      "Epoch 00013: val_loss improved from 26.64030 to 24.85922, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 26.7721 - val_loss: 24.8592\n",
      "Epoch 14/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 26.2728\n",
      "Epoch 00014: val_loss improved from 24.85922 to 24.76484, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 26.2710 - val_loss: 24.7648\n",
      "Epoch 15/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 25.7787\n",
      "Epoch 00015: val_loss improved from 24.76484 to 23.96647, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 25.7787 - val_loss: 23.9665\n",
      "Epoch 16/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 25.3674\n",
      "Epoch 00016: val_loss did not improve from 23.96647\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 25.3674 - val_loss: 26.0215\n",
      "Epoch 17/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 25.1177\n",
      "Epoch 00017: val_loss improved from 23.96647 to 23.39840, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 25.1172 - val_loss: 23.3984\n",
      "Epoch 18/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 24.7443 ETA: \n",
      "Epoch 00018: val_loss did not improve from 23.39840\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 24.7446 - val_loss: 28.2407\n",
      "Epoch 19/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 24.5194\n",
      "Epoch 00019: val_loss did not improve from 23.39840\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 24.5190 - val_loss: 25.5775\n",
      "Epoch 20/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 24.2260\n",
      "Epoch 00020: val_loss improved from 23.39840 to 22.79469, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 24.2249 - val_loss: 22.7947\n",
      "Epoch 21/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 23.8681\n",
      "Epoch 00021: val_loss did not improve from 22.79469\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 23.8661 - val_loss: 23.1859\n",
      "Epoch 22/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 23.5507\n",
      "Epoch 00022: val_loss did not improve from 22.79469\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.5533 - val_loss: 26.8369\n",
      "Epoch 23/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 23.4046\n",
      "Epoch 00023: val_loss did not improve from 22.79469\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 23.4050 - val_loss: 23.7323\n",
      "Epoch 24/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 23.2304\n",
      "Epoch 00024: val_loss improved from 22.79469 to 22.75491, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.2300 - val_loss: 22.7549\n",
      "Epoch 25/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 23.0369\n",
      "Epoch 00025: val_loss did not improve from 22.75491\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.0390 - val_loss: 23.1539\n",
      "Epoch 26/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 22.8391\n",
      "Epoch 00026: val_loss did not improve from 22.75491\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 22.8379 - val_loss: 22.8986\n",
      "Epoch 27/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 22.6938\n",
      "Epoch 00027: val_loss improved from 22.75491 to 22.28883, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 22.6940 - val_loss: 22.2888\n",
      "Epoch 28/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 22.4351\n",
      "Epoch 00028: val_loss improved from 22.28883 to 22.21381, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 22.4340 - val_loss: 22.2138\n",
      "Epoch 29/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 22.2691\n",
      "Epoch 00029: val_loss did not improve from 22.21381\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.2691 - val_loss: 22.8657\n",
      "Epoch 30/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 22.1817\n",
      "Epoch 00030: val_loss improved from 22.21381 to 21.60143, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 22.1835 - val_loss: 21.6014\n",
      "Epoch 31/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 22.1610\n",
      "Epoch 00031: val_loss did not improve from 21.60143\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 22.1612 - val_loss: 22.3841\n",
      "Epoch 32/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 21.8244\n",
      "Epoch 00032: val_loss did not improve from 21.60143\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 21.8239 - val_loss: 22.4192\n",
      "Epoch 33/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 21.6941\n",
      "Epoch 00033: val_loss did not improve from 21.60143\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 21.6940 - val_loss: 22.1294\n",
      "Epoch 34/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 21.6676\n",
      "Epoch 00034: val_loss did not improve from 21.60143\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 21.6662 - val_loss: 23.0233\n",
      "Epoch 35/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 21.6160\n",
      "Epoch 00035: val_loss did not improve from 21.60143\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 21.6203 - val_loss: 21.9719\n",
      "Epoch 36/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 21.3985\n",
      "Epoch 00036: val_loss improved from 21.60143 to 21.12750, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.3991 - val_loss: 21.1275\n",
      "Epoch 37/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 21.3349\n",
      "Epoch 00037: val_loss did not improve from 21.12750\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 21.3389 - val_loss: 22.4233\n",
      "Epoch 38/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 21.1828\n",
      "Epoch 00038: val_loss did not improve from 21.12750\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.1826 - val_loss: 21.7619\n",
      "Epoch 39/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 21.1462\n",
      "Epoch 00039: val_loss did not improve from 21.12750\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 21.1450 - val_loss: 22.8475\n",
      "Epoch 40/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6460/6477 [============================>.] - ETA: 0s - loss: 20.9935\n",
      "Epoch 00040: val_loss improved from 21.12750 to 21.02038, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 20.9872 - val_loss: 21.0204\n",
      "Epoch 41/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 21.0199\n",
      "Epoch 00041: val_loss did not improve from 21.02038\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.0191 - val_loss: 23.8246\n",
      "Epoch 42/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 20.8224\n",
      "Epoch 00042: val_loss did not improve from 21.02038\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 20.8262 - val_loss: 21.1062\n",
      "Epoch 43/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 20.6714\n",
      "Epoch 00043: val_loss did not improve from 21.02038\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.6716 - val_loss: 23.0641\n",
      "Epoch 44/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 20.6257\n",
      "Epoch 00044: val_loss did not improve from 21.02038\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.6322 - val_loss: 25.0017\n",
      "Epoch 45/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 20.5757\n",
      "Epoch 00045: val_loss improved from 21.02038 to 20.91462, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 20.5755 - val_loss: 20.9146\n",
      "Epoch 46/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 20.5606\n",
      "Epoch 00046: val_loss did not improve from 20.91462\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 20.5578 - val_loss: 22.3427\n",
      "Epoch 47/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 20.4877\n",
      "Epoch 00047: val_loss improved from 20.91462 to 20.62225, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 20.4877 - val_loss: 20.6222\n",
      "Epoch 48/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 20.4061\n",
      "Epoch 00048: val_loss improved from 20.62225 to 20.19909, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 20.4078 - val_loss: 20.1991\n",
      "Epoch 49/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 20.3864\n",
      "Epoch 00049: val_loss did not improve from 20.19909\n",
      "6477/6477 [==============================] - 31s 5ms/step - loss: 20.3866 - val_loss: 21.2576\n",
      "Epoch 50/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 20.2383\n",
      "Epoch 00050: val_loss improved from 20.19909 to 19.80251, saving model to ./experiment_set_12\\results_20\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.2381 - val_loss: 19.8025\n",
      "Epoch 51/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 20.1745\n",
      "Epoch 00051: val_loss did not improve from 19.80251\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.1745 - val_loss: 20.8489\n",
      "Epoch 52/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 20.1184\n",
      "Epoch 00052: val_loss did not improve from 19.80251\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.1170 - val_loss: 20.3970\n",
      "Epoch 53/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 20.1136\n",
      "Epoch 00053: val_loss did not improve from 19.80251\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.1136 - val_loss: 22.7205\n",
      "Epoch 54/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 19.9243\n",
      "Epoch 00054: val_loss did not improve from 19.80251\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 19.9256 - val_loss: 29.0238\n",
      "Epoch 55/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 19.8819\n",
      "Epoch 00055: val_loss did not improve from 19.80251\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.8846 - val_loss: 25.4535\n",
      "Epoch 56/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 19.9054\n",
      "Epoch 00056: val_loss did not improve from 19.80251\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.9055 - val_loss: 20.7323\n",
      "Epoch 57/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 19.7782\n",
      "Epoch 00057: val_loss did not improve from 19.80251\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.7775 - val_loss: 20.1276\n",
      "Epoch 58/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 19.7621\n",
      "Epoch 00058: val_loss did not improve from 19.80251\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.7673 - val_loss: 23.9560\n",
      "Epoch 59/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 19.7274\n",
      "Epoch 00059: val_loss did not improve from 19.80251\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.7243 - val_loss: 20.2018\n",
      "Epoch 60/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 19.6542\n",
      "Epoch 00060: val_loss did not improve from 19.80251\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.6554 - val_loss: 20.9511\n",
      "Epoch 00060: early stopping\n",
      "Saved training history to file: ./experiment_set_12\\results_20\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 19.78\n",
      "RMSE: 4.45\n",
      "CMAPSS score: 1.36\n",
      "\n",
      "Saved object to file: ./experiment_set_12\\results_20\\split_1\\scaler.pkl\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 256)               5376      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 235,649\n",
      "Trainable params: 235,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 147.2881\n",
      "Epoch 00001: val_loss improved from inf to 49.17262, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 147.2203 - val_loss: 49.1726\n",
      "Epoch 2/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 46.9613\n",
      "Epoch 00002: val_loss improved from 49.17262 to 43.82445, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 46.9628 - val_loss: 43.8245\n",
      "Epoch 3/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 40.8519\n",
      "Epoch 00003: val_loss improved from 43.82445 to 36.40833, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 40.8501 - val_loss: 36.4083\n",
      "Epoch 4/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 37.1212\n",
      "Epoch 00004: val_loss improved from 36.40833 to 34.00444, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 37.1147 - val_loss: 34.0044\n",
      "Epoch 5/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 34.6399\n",
      "Epoch 00005: val_loss improved from 34.00444 to 31.37191, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 34.6389 - val_loss: 31.3719\n",
      "Epoch 6/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 32.7175\n",
      "Epoch 00006: val_loss did not improve from 31.37191\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 32.7153 - val_loss: 32.2364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 31.3846\n",
      "Epoch 00007: val_loss improved from 31.37191 to 30.82571, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 31.3852 - val_loss: 30.8257\n",
      "Epoch 8/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 30.0960\n",
      "Epoch 00008: val_loss improved from 30.82571 to 29.10625, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 30.0953 - val_loss: 29.1062\n",
      "Epoch 9/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 29.2506\n",
      "Epoch 00009: val_loss did not improve from 29.10625\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 29.2509 - val_loss: 33.8425\n",
      "Epoch 10/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 28.3411\n",
      "Epoch 00010: val_loss improved from 29.10625 to 27.76653, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 28.3411 - val_loss: 27.7665\n",
      "Epoch 11/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 27.6451\n",
      "Epoch 00011: val_loss did not improve from 27.76653\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 27.6453 - val_loss: 29.3158\n",
      "Epoch 12/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 27.1659\n",
      "Epoch 00012: val_loss did not improve from 27.76653\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 27.1701 - val_loss: 28.7645\n",
      "Epoch 13/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 26.7065\n",
      "Epoch 00013: val_loss improved from 27.76653 to 27.09303, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 26.7063 - val_loss: 27.0930\n",
      "Epoch 14/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 26.1191\n",
      "Epoch 00014: val_loss improved from 27.09303 to 26.95239, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 26.1201 - val_loss: 26.9524\n",
      "Epoch 15/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 25.8162\n",
      "Epoch 00015: val_loss did not improve from 26.95239\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 25.8174 - val_loss: 29.6296\n",
      "Epoch 16/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 25.2986\n",
      "Epoch 00016: val_loss improved from 26.95239 to 24.27329, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 25.2988 - val_loss: 24.2733\n",
      "Epoch 17/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 25.0667\n",
      "Epoch 00017: val_loss improved from 24.27329 to 24.08500, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 25.0669 - val_loss: 24.0850\n",
      "Epoch 18/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 24.6943\n",
      "Epoch 00018: val_loss did not improve from 24.08500\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.6954 - val_loss: 31.8445\n",
      "Epoch 19/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 24.3552\n",
      "Epoch 00019: val_loss did not improve from 24.08500\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.3552 - val_loss: 25.0870\n",
      "Epoch 20/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 24.2669\n",
      "Epoch 00020: val_loss did not improve from 24.08500\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.2662 - val_loss: 26.7911\n",
      "Epoch 21/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 24.0481\n",
      "Epoch 00021: val_loss did not improve from 24.08500\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.0455 - val_loss: 32.4561\n",
      "Epoch 22/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 23.6600\n",
      "Epoch 00022: val_loss did not improve from 24.08500\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.6608 - val_loss: 25.2321\n",
      "Epoch 23/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 23.4592\n",
      "Epoch 00023: val_loss did not improve from 24.08500\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.4563 - val_loss: 24.5704\n",
      "Epoch 24/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 23.1656\n",
      "Epoch 00024: val_loss did not improve from 24.08500\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.1640 - val_loss: 25.1967\n",
      "Epoch 25/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 23.1025\n",
      "Epoch 00025: val_loss improved from 24.08500 to 23.08713, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 23.1026 - val_loss: 23.0871\n",
      "Epoch 26/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 22.6624\n",
      "Epoch 00026: val_loss did not improve from 23.08713\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.6655 - val_loss: 23.4173\n",
      "Epoch 27/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 22.5829\n",
      "Epoch 00027: val_loss did not improve from 23.08713\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.5822 - val_loss: 24.4987\n",
      "Epoch 28/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 22.5325\n",
      "Epoch 00028: val_loss did not improve from 23.08713\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.5330 - val_loss: 24.8012\n",
      "Epoch 29/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 22.3061\n",
      "Epoch 00029: val_loss improved from 23.08713 to 22.56345, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.3049 - val_loss: 22.5634\n",
      "Epoch 30/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 22.2993\n",
      "Epoch 00030: val_loss did not improve from 22.56345\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.2986 - val_loss: 22.8927\n",
      "Epoch 31/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 21.9775\n",
      "Epoch 00031: val_loss did not improve from 22.56345\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.9755 - val_loss: 23.0288\n",
      "Epoch 32/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 21.8207\n",
      "Epoch 00032: val_loss did not improve from 22.56345\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 21.8203 - val_loss: 24.4402\n",
      "Epoch 33/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 21.6200\n",
      "Epoch 00033: val_loss improved from 22.56345 to 22.38169, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.6200 - val_loss: 22.3817\n",
      "Epoch 34/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 21.6361\n",
      "Epoch 00034: val_loss did not improve from 22.38169\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.6360 - val_loss: 24.3387\n",
      "Epoch 35/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 21.5216\n",
      "Epoch 00035: val_loss improved from 22.38169 to 21.90135, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.5214 - val_loss: 21.9014\n",
      "Epoch 36/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 21.3667\n",
      "Epoch 00036: val_loss improved from 21.90135 to 21.58322, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.3674 - val_loss: 21.5832\n",
      "Epoch 37/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 21.3587\n",
      "Epoch 00037: val_loss improved from 21.58322 to 21.52358, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.3587 - val_loss: 21.5236\n",
      "Epoch 38/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 21.1853\n",
      "Epoch 00038: val_loss improved from 21.52358 to 20.18096, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.1801 - val_loss: 20.1810\n",
      "Epoch 39/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 20.9859\n",
      "Epoch 00039: val_loss did not improve from 20.18096\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 20.9893 - val_loss: 21.6168\n",
      "Epoch 40/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 20.8999\n",
      "Epoch 00040: val_loss did not improve from 20.18096\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 20.8996 - val_loss: 22.7093\n",
      "Epoch 41/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 20.9722\n",
      "Epoch 00041: val_loss did not improve from 20.18096\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 20.9729 - val_loss: 22.8455\n",
      "Epoch 42/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 20.7414\n",
      "Epoch 00042: val_loss did not improve from 20.18096\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.7422 - val_loss: 21.0164\n",
      "Epoch 43/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 20.7008\n",
      "Epoch 00043: val_loss did not improve from 20.18096\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.7020 - val_loss: 21.2164\n",
      "Epoch 44/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 20.6537\n",
      "Epoch 00044: val_loss did not improve from 20.18096\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.6538 - val_loss: 21.9083\n",
      "Epoch 45/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 20.4489\n",
      "Epoch 00045: val_loss improved from 20.18096 to 20.01595, saving model to ./experiment_set_12\\results_20\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.4498 - val_loss: 20.0160\n",
      "Epoch 46/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 20.4564\n",
      "Epoch 00046: val_loss did not improve from 20.01595\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 20.4560 - val_loss: 21.4553\n",
      "Epoch 47/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 20.4633\n",
      "Epoch 00047: val_loss did not improve from 20.01595\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 20.4627 - val_loss: 21.9263\n",
      "Epoch 48/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 20.3500\n",
      "Epoch 00048: val_loss did not improve from 20.01595\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.3500 - val_loss: 21.2917\n",
      "Epoch 49/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 20.1955\n",
      "Epoch 00049: val_loss did not improve from 20.01595\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.1974 - val_loss: 21.8837\n",
      "Epoch 50/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 20.1847\n",
      "Epoch 00050: val_loss did not improve from 20.01595\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.1842 - val_loss: 20.8075\n",
      "Epoch 51/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 20.0747\n",
      "Epoch 00051: val_loss did not improve from 20.01595\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.0761 - val_loss: 24.6881\n",
      "Epoch 52/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 20.0486\n",
      "Epoch 00052: val_loss did not improve from 20.01595\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 20.0472 - val_loss: 20.6103\n",
      "Epoch 53/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 19.9642\n",
      "Epoch 00053: val_loss did not improve from 20.01595\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.9621 - val_loss: 21.0676\n",
      "Epoch 54/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 19.8503\n",
      "Epoch 00054: val_loss did not improve from 20.01595\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.8495 - val_loss: 20.9053\n",
      "Epoch 55/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 19.8515\n",
      "Epoch 00055: val_loss did not improve from 20.01595\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 19.8501 - val_loss: 20.2885\n",
      "Epoch 00055: early stopping\n",
      "Saved training history to file: ./experiment_set_12\\results_20\\split_1\\history.pkl\n",
      "Test set:\n",
      "MSE: 20.03\n",
      "RMSE: 4.47\n",
      "CMAPSS score: 1.37\n",
      "\n",
      "Saved object to file: ./experiment_set_12\\results_20\\split_2\\scaler.pkl\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 256)               5376      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 235,649\n",
      "Trainable params: 235,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 146.6924\n",
      "Epoch 00001: val_loss improved from inf to 55.02745, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 146.5927 - val_loss: 55.0274\n",
      "Epoch 2/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 47.0948\n",
      "Epoch 00002: val_loss improved from 55.02745 to 45.79939, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 47.0948 - val_loss: 45.7994\n",
      "Epoch 3/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 41.0705\n",
      "Epoch 00003: val_loss improved from 45.79939 to 38.32472, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 41.0647 - val_loss: 38.3247\n",
      "Epoch 4/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 37.3173\n",
      "Epoch 00004: val_loss improved from 38.32472 to 36.55722, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 37.3174 - val_loss: 36.5572\n",
      "Epoch 5/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 34.8691\n",
      "Epoch 00005: val_loss did not improve from 36.55722\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 34.8691 - val_loss: 45.8646\n",
      "Epoch 6/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 33.0803\n",
      "Epoch 00006: val_loss improved from 36.55722 to 32.91503, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 33.0830 - val_loss: 32.9150\n",
      "Epoch 7/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 31.6305\n",
      "Epoch 00007: val_loss improved from 32.91503 to 31.21021, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 31.6305 - val_loss: 31.2102\n",
      "Epoch 8/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 30.4353\n",
      "Epoch 00008: val_loss did not improve from 31.21021\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 30.4366 - val_loss: 35.6263\n",
      "Epoch 9/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 29.3012\n",
      "Epoch 00009: val_loss improved from 31.21021 to 27.84164, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 29.3011 - val_loss: 27.8416\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6476/6477 [============================>.] - ETA: 0s - loss: 28.5017\n",
      "Epoch 00010: val_loss did not improve from 27.84164\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 28.5025 - val_loss: 27.9643\n",
      "Epoch 11/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 27.9256\n",
      "Epoch 00011: val_loss did not improve from 27.84164\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 27.9256 - val_loss: 28.8579\n",
      "Epoch 12/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 27.4038\n",
      "Epoch 00012: val_loss improved from 27.84164 to 26.03102, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 27.4042 - val_loss: 26.0310\n",
      "Epoch 13/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 26.6460\n",
      "Epoch 00013: val_loss did not improve from 26.03102\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 26.6454 - val_loss: 27.2927\n",
      "Epoch 14/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 26.3297\n",
      "Epoch 00014: val_loss did not improve from 26.03102\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 26.3286 - val_loss: 26.9352\n",
      "Epoch 15/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 25.8719\n",
      "Epoch 00015: val_loss improved from 26.03102 to 25.22142, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 25.8715 - val_loss: 25.2214\n",
      "Epoch 16/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 25.5153\n",
      "Epoch 00016: val_loss did not improve from 25.22142\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 25.5149 - val_loss: 30.3448\n",
      "Epoch 17/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 25.1573\n",
      "Epoch 00017: val_loss did not improve from 25.22142\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 25.1612 - val_loss: 30.0770\n",
      "Epoch 18/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 24.8044\n",
      "Epoch 00018: val_loss improved from 25.22142 to 23.89708, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.8028 - val_loss: 23.8971\n",
      "Epoch 19/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 24.5592\n",
      "Epoch 00019: val_loss did not improve from 23.89708\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 24.5582 - val_loss: 24.2600\n",
      "Epoch 20/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 24.2249\n",
      "Epoch 00020: val_loss improved from 23.89708 to 22.86987, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 24.2228 - val_loss: 22.8699\n",
      "Epoch 21/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 23.9504\n",
      "Epoch 00021: val_loss improved from 22.86987 to 22.71029, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 23.9504 - val_loss: 22.7103\n",
      "Epoch 22/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 23.7146\n",
      "Epoch 00022: val_loss did not improve from 22.71029\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.7147 - val_loss: 24.2442\n",
      "Epoch 23/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 23.3335\n",
      "Epoch 00023: val_loss did not improve from 22.71029\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.3348 - val_loss: 25.0833\n",
      "Epoch 24/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 23.2797\n",
      "Epoch 00024: val_loss did not improve from 22.71029\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.2817 - val_loss: 25.1368\n",
      "Epoch 25/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 23.0525\n",
      "Epoch 00025: val_loss did not improve from 22.71029\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 23.0498 - val_loss: 23.1368\n",
      "Epoch 26/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 22.9400\n",
      "Epoch 00026: val_loss did not improve from 22.71029\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 22.9371 - val_loss: 23.3982\n",
      "Epoch 27/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 22.6802\n",
      "Epoch 00027: val_loss did not improve from 22.71029\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 22.6802 - val_loss: 23.6920\n",
      "Epoch 28/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 22.4508\n",
      "Epoch 00028: val_loss did not improve from 22.71029\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 22.4503 - val_loss: 22.8066\n",
      "Epoch 29/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 22.4429\n",
      "Epoch 00029: val_loss did not improve from 22.71029\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.4442 - val_loss: 23.9805\n",
      "Epoch 30/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 22.1929\n",
      "Epoch 00030: val_loss improved from 22.71029 to 22.64499, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.1936 - val_loss: 22.6450\n",
      "Epoch 31/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 22.0146\n",
      "Epoch 00031: val_loss did not improve from 22.64499\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.0138 - val_loss: 23.6432\n",
      "Epoch 32/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 21.9227\n",
      "Epoch 00032: val_loss did not improve from 22.64499\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.9237 - val_loss: 23.4875\n",
      "Epoch 33/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 21.8060\n",
      "Epoch 00033: val_loss improved from 22.64499 to 21.46049, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 21.8046 - val_loss: 21.4605\n",
      "Epoch 34/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 21.6403\n",
      "Epoch 00034: val_loss did not improve from 21.46049\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.6411 - val_loss: 22.2780\n",
      "Epoch 35/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 21.5626\n",
      "Epoch 00035: val_loss did not improve from 21.46049\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.5665 - val_loss: 25.7068\n",
      "Epoch 36/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 21.3673\n",
      "Epoch 00036: val_loss improved from 21.46049 to 20.95192, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.3673 - val_loss: 20.9519\n",
      "Epoch 37/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 21.3578\n",
      "Epoch 00037: val_loss did not improve from 20.95192\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 21.3571 - val_loss: 21.3760\n",
      "Epoch 38/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 21.2925\n",
      "Epoch 00038: val_loss did not improve from 20.95192\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.2933 - val_loss: 22.7026\n",
      "Epoch 39/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 21.1054\n",
      "Epoch 00039: val_loss did not improve from 20.95192\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 21.1049 - val_loss: 23.3651\n",
      "Epoch 40/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 20.9936\n",
      "Epoch 00040: val_loss did not improve from 20.95192\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.9954 - val_loss: 21.5644\n",
      "Epoch 41/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 21.0027\n",
      "Epoch 00041: val_loss did not improve from 20.95192\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.0035 - val_loss: 21.4821\n",
      "Epoch 42/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 20.9003\n",
      "Epoch 00042: val_loss improved from 20.95192 to 20.52106, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.8993 - val_loss: 20.5211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 20.7571\n",
      "Epoch 00043: val_loss did not improve from 20.52106\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.7580 - val_loss: 20.8996\n",
      "Epoch 44/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 20.8210\n",
      "Epoch 00044: val_loss did not improve from 20.52106\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 20.8210 - val_loss: 23.1103\n",
      "Epoch 45/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 20.7061\n",
      "Epoch 00045: val_loss did not improve from 20.52106\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.7065 - val_loss: 22.9679\n",
      "Epoch 46/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 20.5547\n",
      "Epoch 00046: val_loss improved from 20.52106 to 19.89269, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 20.5537 - val_loss: 19.8927\n",
      "Epoch 47/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 20.4798\n",
      "Epoch 00047: val_loss did not improve from 19.89269\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.4794 - val_loss: 20.9388\n",
      "Epoch 48/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 20.4724\n",
      "Epoch 00048: val_loss did not improve from 19.89269\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.4714 - val_loss: 21.2988\n",
      "Epoch 49/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 20.3440\n",
      "Epoch 00049: val_loss did not improve from 19.89269\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.3422 - val_loss: 20.0306\n",
      "Epoch 50/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 20.2382\n",
      "Epoch 00050: val_loss did not improve from 19.89269\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.2394 - val_loss: 20.8677\n",
      "Epoch 51/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 20.3513\n",
      "Epoch 00051: val_loss did not improve from 19.89269\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 20.3513 - val_loss: 21.4229\n",
      "Epoch 52/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 20.1521\n",
      "Epoch 00052: val_loss did not improve from 19.89269\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.1557 - val_loss: 22.2462\n",
      "Epoch 53/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 20.0983\n",
      "Epoch 00053: val_loss did not improve from 19.89269\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.0963 - val_loss: 20.6988\n",
      "Epoch 54/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 20.0151\n",
      "Epoch 00054: val_loss did not improve from 19.89269\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.0149 - val_loss: 21.2586\n",
      "Epoch 55/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 20.0432\n",
      "Epoch 00055: val_loss did not improve from 19.89269\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.0432 - val_loss: 20.3793\n",
      "Epoch 56/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 19.8921\n",
      "Epoch 00056: val_loss improved from 19.89269 to 19.51612, saving model to ./experiment_set_12\\results_20\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.8912 - val_loss: 19.5161\n",
      "Epoch 57/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 19.7915\n",
      "Epoch 00057: val_loss did not improve from 19.51612\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.7937 - val_loss: 21.1003\n",
      "Epoch 58/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 19.8564\n",
      "Epoch 00058: val_loss did not improve from 19.51612\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.8556 - val_loss: 20.6165\n",
      "Epoch 59/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 19.7410\n",
      "Epoch 00059: val_loss did not improve from 19.51612\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.7449 - val_loss: 22.8820\n",
      "Epoch 60/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 19.6980\n",
      "Epoch 00060: val_loss did not improve from 19.51612\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.6980 - val_loss: 20.8641\n",
      "Epoch 61/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 19.6891\n",
      "Epoch 00061: val_loss did not improve from 19.51612\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.6891 - val_loss: 20.6831\n",
      "Epoch 62/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 19.5072\n",
      "Epoch 00062: val_loss did not improve from 19.51612\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.5059 - val_loss: 20.1800\n",
      "Epoch 63/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 19.5200\n",
      "Epoch 00063: val_loss did not improve from 19.51612\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.5192 - val_loss: 20.0122\n",
      "Epoch 64/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 19.5477\n",
      "Epoch 00064: val_loss did not improve from 19.51612\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 19.5507 - val_loss: 24.5197\n",
      "Epoch 65/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 19.3921\n",
      "Epoch 00065: val_loss did not improve from 19.51612\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.3891 - val_loss: 19.7262\n",
      "Epoch 66/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 19.4245\n",
      "Epoch 00066: val_loss did not improve from 19.51612\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.4252 - val_loss: 20.9306\n",
      "Epoch 00066: early stopping\n",
      "Saved training history to file: ./experiment_set_12\\results_20\\split_2\\history.pkl\n",
      "Test set:\n",
      "MSE: 19.57\n",
      "RMSE: 4.42\n",
      "CMAPSS score: 1.36\n",
      "\n",
      "Saved object to file: ./experiment_set_12\\results_25\\split_0\\scaler.pkl\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 256)               6656      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 236,929\n",
      "Trainable params: 236,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 144.8650\n",
      "Epoch 00001: val_loss improved from inf to 55.97738, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 144.7575 - val_loss: 55.9774\n",
      "Epoch 2/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 44.7009\n",
      "Epoch 00002: val_loss improved from 55.97738 to 38.87692, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 44.7000 - val_loss: 38.8769\n",
      "Epoch 3/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 38.3064\n",
      "Epoch 00003: val_loss improved from 38.87692 to 36.39020, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 38.3040 - val_loss: 36.3902\n",
      "Epoch 4/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 34.8027\n",
      "Epoch 00004: val_loss improved from 36.39020 to 32.27013, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 34.7947 - val_loss: 32.2701\n",
      "Epoch 5/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 32.1926\n",
      "Epoch 00005: val_loss did not improve from 32.27013\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 32.1919 - val_loss: 32.2906\n",
      "Epoch 6/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 29.6036\n",
      "Epoch 00006: val_loss improved from 32.27013 to 26.65068, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 29.6026 - val_loss: 26.6507\n",
      "Epoch 7/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 27.1746\n",
      "Epoch 00007: val_loss did not improve from 26.65068\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 27.1766 - val_loss: 28.9114\n",
      "Epoch 8/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 25.3034\n",
      "Epoch 00008: val_loss improved from 26.65068 to 21.89624, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 25.2991 - val_loss: 21.8962\n",
      "Epoch 9/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 23.6562\n",
      "Epoch 00009: val_loss improved from 21.89624 to 20.70257, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 23.6559 - val_loss: 20.7026\n",
      "Epoch 10/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 22.5392\n",
      "Epoch 00010: val_loss did not improve from 20.70257\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.5419 - val_loss: 26.8646\n",
      "Epoch 11/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 21.9021\n",
      "Epoch 00011: val_loss did not improve from 20.70257\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.9021 - val_loss: 20.7828\n",
      "Epoch 12/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 21.1005\n",
      "Epoch 00012: val_loss improved from 20.70257 to 18.69277, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.0960 - val_loss: 18.6928\n",
      "Epoch 13/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 20.4980\n",
      "Epoch 00013: val_loss improved from 18.69277 to 17.08147, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 20.4971 - val_loss: 17.0815\n",
      "Epoch 14/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 19.8158\n",
      "Epoch 00014: val_loss did not improve from 17.08147\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.8205 - val_loss: 21.9002\n",
      "Epoch 15/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 19.48 - ETA: 0s - loss: 19.4820\n",
      "Epoch 00015: val_loss improved from 17.08147 to 16.27856, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 19.4802 - val_loss: 16.2786\n",
      "Epoch 16/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 19.1251\n",
      "Epoch 00016: val_loss did not improve from 16.27856\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.1250 - val_loss: 19.3925\n",
      "Epoch 17/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 18.6341\n",
      "Epoch 00017: val_loss did not improve from 16.27856\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.6381 - val_loss: 29.4479\n",
      "Epoch 18/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 18.3372\n",
      "Epoch 00018: val_loss did not improve from 16.27856\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.3360 - val_loss: 16.2872\n",
      "Epoch 19/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 18.0674\n",
      "Epoch 00019: val_loss did not improve from 16.27856\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.0652 - val_loss: 17.2374\n",
      "Epoch 20/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 17.7389\n",
      "Epoch 00020: val_loss did not improve from 16.27856\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.7387 - val_loss: 17.4558\n",
      "Epoch 21/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 17.4712\n",
      "Epoch 00021: val_loss improved from 16.27856 to 14.86077, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 17.4710 - val_loss: 14.8608\n",
      "Epoch 22/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 17.0993\n",
      "Epoch 00022: val_loss did not improve from 14.86077\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.0979 - val_loss: 16.0551\n",
      "Epoch 23/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 16.9452\n",
      "Epoch 00023: val_loss did not improve from 14.86077\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.9457 - val_loss: 19.6435\n",
      "Epoch 24/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 16.5126\n",
      "Epoch 00024: val_loss improved from 14.86077 to 14.63181, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 16.5112 - val_loss: 14.6318\n",
      "Epoch 25/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 16.5568\n",
      "Epoch 00025: val_loss did not improve from 14.63181\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.5565 - val_loss: 14.9304\n",
      "Epoch 26/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 16.4160\n",
      "Epoch 00026: val_loss did not improve from 14.63181\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 16.4167 - val_loss: 24.3205\n",
      "Epoch 27/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 16.1206\n",
      "Epoch 00027: val_loss improved from 14.63181 to 13.53006, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 16.1202 - val_loss: 13.5301\n",
      "Epoch 28/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 16.0121\n",
      "Epoch 00028: val_loss improved from 13.53006 to 12.75359, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 16.0118 - val_loss: 12.7536\n",
      "Epoch 29/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 15.9897\n",
      "Epoch 00029: val_loss did not improve from 12.75359\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 15.9820 - val_loss: 14.3714\n",
      "Epoch 30/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 15.3373\n",
      "Epoch 00030: val_loss did not improve from 12.75359\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 15.3356 - val_loss: 13.5529\n",
      "Epoch 31/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 15.4424\n",
      "Epoch 00031: val_loss did not improve from 12.75359\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.4525 - val_loss: 13.3875\n",
      "Epoch 32/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 15.1581\n",
      "Epoch 00032: val_loss improved from 12.75359 to 12.72082, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.1570 - val_loss: 12.7208\n",
      "Epoch 33/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 15.1102\n",
      "Epoch 00033: val_loss did not improve from 12.72082\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.1079 - val_loss: 16.9309\n",
      "Epoch 34/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 14.9370\n",
      "Epoch 00034: val_loss did not improve from 12.72082\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.9359 - val_loss: 13.2633\n",
      "Epoch 35/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 14.9420\n",
      "Epoch 00035: val_loss did not improve from 12.72082\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.9399 - val_loss: 16.3902\n",
      "Epoch 36/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 14.8718\n",
      "Epoch 00036: val_loss did not improve from 12.72082\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.8714 - val_loss: 15.5936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 14.6074\n",
      "Epoch 00037: val_loss did not improve from 12.72082\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.6072 - val_loss: 14.8911\n",
      "Epoch 38/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 14.4183\n",
      "Epoch 00038: val_loss improved from 12.72082 to 11.93506, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.4196 - val_loss: 11.9351\n",
      "Epoch 39/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 14.2804\n",
      "Epoch 00039: val_loss improved from 11.93506 to 11.23587, saving model to ./experiment_set_12\\results_25\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.2778 - val_loss: 11.2359\n",
      "Epoch 40/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 14.3265\n",
      "Epoch 00040: val_loss did not improve from 11.23587\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.3259 - val_loss: 11.9479\n",
      "Epoch 41/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 14.3384\n",
      "Epoch 00041: val_loss did not improve from 11.23587\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.3366 - val_loss: 13.6763\n",
      "Epoch 42/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 14.1800\n",
      "Epoch 00042: val_loss did not improve from 11.23587\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.1807 - val_loss: 16.5356\n",
      "Epoch 43/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 14.2487\n",
      "Epoch 00043: val_loss did not improve from 11.23587\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.2487 - val_loss: 18.9594\n",
      "Epoch 44/200\n",
      "6460/6477 [============================>.] - ETA: 0s - loss: 13.9775\n",
      "Epoch 00044: val_loss did not improve from 11.23587\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.9760 - val_loss: 11.6637\n",
      "Epoch 45/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 13.8280\n",
      "Epoch 00045: val_loss did not improve from 11.23587\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.8278 - val_loss: 13.4706\n",
      "Epoch 46/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 13.5792\n",
      "Epoch 00046: val_loss did not improve from 11.23587\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.5774 - val_loss: 12.4949\n",
      "Epoch 47/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 13.6428\n",
      "Epoch 00047: val_loss did not improve from 11.23587\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.6428 - val_loss: 14.9459\n",
      "Epoch 48/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 13.6775\n",
      "Epoch 00048: val_loss did not improve from 11.23587\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 13.6772 - val_loss: 13.0974\n",
      "Epoch 49/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 13.3706\n",
      "Epoch 00049: val_loss did not improve from 11.23587\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 13.3706 - val_loss: 11.6465\n",
      "Epoch 00049: early stopping\n",
      "Saved training history to file: ./experiment_set_12\\results_25\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 11.19\n",
      "RMSE: 3.34\n",
      "CMAPSS score: 1.23\n",
      "\n",
      "Saved object to file: ./experiment_set_12\\results_25\\split_1\\scaler.pkl\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 256)               6656      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 236,929\n",
      "Trainable params: 236,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 146.3923\n",
      "Epoch 00001: val_loss improved from inf to 48.92677, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 146.1780 - val_loss: 48.9268\n",
      "Epoch 2/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 44.8924\n",
      "Epoch 00002: val_loss improved from 48.92677 to 41.06991, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 44.8922 - val_loss: 41.0699\n",
      "Epoch 3/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 38.8299\n",
      "Epoch 00003: val_loss improved from 41.06991 to 34.98220, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 38.8253 - val_loss: 34.9822\n",
      "Epoch 4/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 35.2883\n",
      "Epoch 00004: val_loss improved from 34.98220 to 30.86410, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 35.2880 - val_loss: 30.8641\n",
      "Epoch 5/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 32.5247\n",
      "Epoch 00005: val_loss improved from 30.86410 to 30.53497, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 32.5239 - val_loss: 30.5350\n",
      "Epoch 6/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 30.3730\n",
      "Epoch 00006: val_loss improved from 30.53497 to 29.10414, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 30.3687 - val_loss: 29.1041\n",
      "Epoch 7/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 28.5177\n",
      "Epoch 00007: val_loss did not improve from 29.10414\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 28.5168 - val_loss: 30.3500\n",
      "Epoch 8/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 26.2583\n",
      "Epoch 00008: val_loss improved from 29.10414 to 23.17361, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 26.2567 - val_loss: 23.1736\n",
      "Epoch 9/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 24.5517\n",
      "Epoch 00009: val_loss did not improve from 23.17361\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.5543 - val_loss: 38.8354\n",
      "Epoch 10/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 23.3843\n",
      "Epoch 00010: val_loss did not improve from 23.17361\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.3833 - val_loss: 24.5929\n",
      "Epoch 11/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 22.2174\n",
      "Epoch 00011: val_loss improved from 23.17361 to 18.30498, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 22.2098 - val_loss: 18.3050\n",
      "Epoch 12/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 21.4523\n",
      "Epoch 00012: val_loss did not improve from 18.30498\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.4538 - val_loss: 22.8149\n",
      "Epoch 13/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 20.7595\n",
      "Epoch 00013: val_loss did not improve from 18.30498\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.7577 - val_loss: 19.7155\n",
      "Epoch 14/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 20.2700\n",
      "Epoch 00014: val_loss did not improve from 18.30498\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.2658 - val_loss: 19.1219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 19.6631\n",
      "Epoch 00015: val_loss did not improve from 18.30498\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.6629 - val_loss: 19.4316\n",
      "Epoch 16/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 19.1580\n",
      "Epoch 00016: val_loss did not improve from 18.30498\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 19.1580 - val_loss: 18.3650\n",
      "Epoch 17/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 18.6769\n",
      "Epoch 00017: val_loss improved from 18.30498 to 15.08604, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.6780 - val_loss: 15.0860\n",
      "Epoch 18/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 18.3044\n",
      "Epoch 00018: val_loss did not improve from 15.08604\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.3016 - val_loss: 16.7103\n",
      "Epoch 19/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 18.0082\n",
      "Epoch 00019: val_loss did not improve from 15.08604\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 18.0054 - val_loss: 17.3569\n",
      "Epoch 20/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 18.1568\n",
      "Epoch 00020: val_loss did not improve from 15.08604\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 18.1563 - val_loss: 16.7997\n",
      "Epoch 21/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 17.5227\n",
      "Epoch 00021: val_loss improved from 15.08604 to 14.10415, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.5170 - val_loss: 14.1041\n",
      "Epoch 22/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 17.2696\n",
      "Epoch 00022: val_loss did not improve from 14.10415\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 17.2672 - val_loss: 14.5102\n",
      "Epoch 23/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 17.0008\n",
      "Epoch 00023: val_loss did not improve from 14.10415\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 16.9990 - val_loss: 22.2059\n",
      "Epoch 24/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 16.6535\n",
      "Epoch 00024: val_loss did not improve from 14.10415\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.6561 - val_loss: 15.8751\n",
      "Epoch 25/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 16.5065\n",
      "Epoch 00025: val_loss did not improve from 14.10415\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 16.5111 - val_loss: 19.2000\n",
      "Epoch 26/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 16.0705\n",
      "Epoch 00026: val_loss improved from 14.10415 to 13.07560, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.0685 - val_loss: 13.0756\n",
      "Epoch 27/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 16.0248\n",
      "Epoch 00027: val_loss did not improve from 13.07560\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 16.0319 - val_loss: 18.8508\n",
      "Epoch 28/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 15.8842\n",
      "Epoch 00028: val_loss did not improve from 13.07560\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.8842 - val_loss: 17.1846\n",
      "Epoch 29/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 15.5208\n",
      "Epoch 00029: val_loss improved from 13.07560 to 12.82669, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.5208 - val_loss: 12.8267\n",
      "Epoch 30/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 15.4411\n",
      "Epoch 00030: val_loss did not improve from 12.82669\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 15.4520 - val_loss: 16.0780\n",
      "Epoch 31/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 15.4134\n",
      "Epoch 00031: val_loss did not improve from 12.82669\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 15.4132 - val_loss: 17.5097\n",
      "Epoch 32/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 15.1856\n",
      "Epoch 00032: val_loss did not improve from 12.82669\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 15.1858 - val_loss: 17.6392\n",
      "Epoch 33/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 15.2207\n",
      "Epoch 00033: val_loss improved from 12.82669 to 12.50406, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.2199 - val_loss: 12.5041\n",
      "Epoch 34/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 15.1254\n",
      "Epoch 00034: val_loss did not improve from 12.50406\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 15.1192 - val_loss: 15.2436\n",
      "Epoch 35/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 14.9117\n",
      "Epoch 00035: val_loss did not improve from 12.50406\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 14.9141 - val_loss: 16.2311\n",
      "Epoch 36/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 14.9249\n",
      "Epoch 00036: val_loss did not improve from 12.50406\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 14.9233 - val_loss: 12.8292\n",
      "Epoch 37/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 14.7541\n",
      "Epoch 00037: val_loss did not improve from 12.50406\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 14.7485 - val_loss: 12.7275\n",
      "Epoch 38/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 14.6901\n",
      "Epoch 00038: val_loss did not improve from 12.50406\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 14.6880 - val_loss: 16.8274\n",
      "Epoch 39/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 14.2514\n",
      "Epoch 00039: val_loss improved from 12.50406 to 12.44888, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.2468 - val_loss: 12.4489\n",
      "Epoch 40/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 14.3382\n",
      "Epoch 00040: val_loss improved from 12.44888 to 12.44480, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.3348 - val_loss: 12.4448\n",
      "Epoch 41/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 14.1467\n",
      "Epoch 00041: val_loss did not improve from 12.44480\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 14.1472 - val_loss: 13.8406\n",
      "Epoch 42/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 14.1731\n",
      "Epoch 00042: val_loss did not improve from 12.44480\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.1859 - val_loss: 15.1254\n",
      "Epoch 43/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 14.0504\n",
      "Epoch 00043: val_loss improved from 12.44480 to 11.98028, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.0498 - val_loss: 11.9803\n",
      "Epoch 44/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 14.1052\n",
      "Epoch 00044: val_loss did not improve from 11.98028\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.1109 - val_loss: 16.7002\n",
      "Epoch 45/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 13.7829\n",
      "Epoch 00045: val_loss did not improve from 11.98028\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.7777 - val_loss: 12.9300\n",
      "Epoch 46/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 13.7861\n",
      "Epoch 00046: val_loss did not improve from 11.98028\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 13.7860 - val_loss: 13.4869\n",
      "Epoch 47/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 13.6560\n",
      "Epoch 00047: val_loss did not improve from 11.98028\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 13.6555 - val_loss: 16.2297\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6464/6477 [============================>.] - ETA: 0s - loss: 13.5005\n",
      "Epoch 00048: val_loss did not improve from 11.98028\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.4980 - val_loss: 12.3325\n",
      "Epoch 49/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 13.6189\n",
      "Epoch 00049: val_loss did not improve from 11.98028\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 13.6190 - val_loss: 39.2370\n",
      "Epoch 50/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 13.2739\n",
      "Epoch 00050: val_loss did not improve from 11.98028\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.2717 - val_loss: 17.5590\n",
      "Epoch 51/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 13.4507\n",
      "Epoch 00051: val_loss improved from 11.98028 to 11.89515, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.4465 - val_loss: 11.8951\n",
      "Epoch 52/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 13.4419\n",
      "Epoch 00052: val_loss improved from 11.89515 to 11.23854, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 13.4379 - val_loss: 11.2385\n",
      "Epoch 53/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 13.1873\n",
      "Epoch 00053: val_loss did not improve from 11.23854\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.1844 - val_loss: 13.6458\n",
      "Epoch 54/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 12.9921\n",
      "Epoch 00054: val_loss did not improve from 11.23854\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.9892 - val_loss: 12.4378\n",
      "Epoch 55/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 13.1467\n",
      "Epoch 00055: val_loss did not improve from 11.23854\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.1439 - val_loss: 11.6800\n",
      "Epoch 56/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 13.0291\n",
      "Epoch 00056: val_loss did not improve from 11.23854\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 13.0299 - val_loss: 17.7427\n",
      "Epoch 57/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 12.9329\n",
      "Epoch 00057: val_loss did not improve from 11.23854\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.9338 - val_loss: 18.5426\n",
      "Epoch 58/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 12.8573\n",
      "Epoch 00058: val_loss did not improve from 11.23854\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.8577 - val_loss: 12.4478\n",
      "Epoch 59/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 12.7437\n",
      "Epoch 00059: val_loss did not improve from 11.23854\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.7436 - val_loss: 12.5502\n",
      "Epoch 60/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 12.6903\n",
      "Epoch 00060: val_loss did not improve from 11.23854\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.7014 - val_loss: 18.5804\n",
      "Epoch 61/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 12.8755\n",
      "Epoch 00061: val_loss did not improve from 11.23854\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.8734 - val_loss: 13.3705\n",
      "Epoch 62/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 12.6428\n",
      "Epoch 00062: val_loss improved from 11.23854 to 10.82660, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.6373 - val_loss: 10.8266\n",
      "Epoch 63/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 12.6459\n",
      "Epoch 00063: val_loss did not improve from 10.82660\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.6477 - val_loss: 16.5892\n",
      "Epoch 64/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 12.5997\n",
      "Epoch 00064: val_loss improved from 10.82660 to 10.64676, saving model to ./experiment_set_12\\results_25\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.6011 - val_loss: 10.6468\n",
      "Epoch 65/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 12.4891\n",
      "Epoch 00065: val_loss did not improve from 10.64676\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.4894 - val_loss: 11.1046\n",
      "Epoch 66/200\n",
      "6460/6477 [============================>.] - ETA: 0s - loss: 12.3432\n",
      "Epoch 00066: val_loss did not improve from 10.64676\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.3366 - val_loss: 11.0023\n",
      "Epoch 67/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 12.4219\n",
      "Epoch 00067: val_loss did not improve from 10.64676\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.4232 - val_loss: 12.0689\n",
      "Epoch 68/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 12.3292\n",
      "Epoch 00068: val_loss did not improve from 10.64676\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.3393 - val_loss: 15.5599\n",
      "Epoch 69/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 12.2410\n",
      "Epoch 00069: val_loss did not improve from 10.64676\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.2438 - val_loss: 23.1085\n",
      "Epoch 70/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 12.3687\n",
      "Epoch 00070: val_loss did not improve from 10.64676\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.3739 - val_loss: 18.2389\n",
      "Epoch 71/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 12.3391\n",
      "Epoch 00071: val_loss did not improve from 10.64676\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.3375 - val_loss: 11.6450\n",
      "Epoch 72/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 12.0202\n",
      "Epoch 00072: val_loss did not improve from 10.64676\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.0218 - val_loss: 12.3241\n",
      "Epoch 73/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 12.3262\n",
      "Epoch 00073: val_loss did not improve from 10.64676\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.3255 - val_loss: 14.1253\n",
      "Epoch 74/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 12.0964\n",
      "Epoch 00074: val_loss did not improve from 10.64676\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.0964 - val_loss: 17.3488\n",
      "Epoch 00074: early stopping\n",
      "Saved training history to file: ./experiment_set_12\\results_25\\split_1\\history.pkl\n",
      "Test set:\n",
      "MSE: 10.71\n",
      "RMSE: 3.27\n",
      "CMAPSS score: 1.22\n",
      "\n",
      "Saved object to file: ./experiment_set_12\\results_25\\split_2\\scaler.pkl\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 256)               6656      \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 236,929\n",
      "Trainable params: 236,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 142.7425\n",
      "Epoch 00001: val_loss improved from inf to 54.01491, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 142.5906 - val_loss: 54.0149\n",
      "Epoch 2/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 44.0225\n",
      "Epoch 00002: val_loss improved from 54.01491 to 40.79404, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6477/6477 [==============================] - 25s 4ms/step - loss: 44.0210 - val_loss: 40.7940\n",
      "Epoch 3/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 37.8316\n",
      "Epoch 00003: val_loss improved from 40.79404 to 37.13881, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 37.8316 - val_loss: 37.1388\n",
      "Epoch 4/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 34.3400\n",
      "Epoch 00004: val_loss improved from 37.13881 to 32.89642, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 34.3400 - val_loss: 32.8964\n",
      "Epoch 5/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 31.9844\n",
      "Epoch 00005: val_loss did not improve from 32.89642\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 31.9825 - val_loss: 34.0758\n",
      "Epoch 6/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 30.0936\n",
      "Epoch 00006: val_loss did not improve from 32.89642\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 30.0933 - val_loss: 35.4375\n",
      "Epoch 7/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 28.0371\n",
      "Epoch 00007: val_loss improved from 32.89642 to 32.13426, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 28.0369 - val_loss: 32.1343\n",
      "Epoch 8/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 26.3757\n",
      "Epoch 00008: val_loss improved from 32.13426 to 25.23662, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 26.3739 - val_loss: 25.2366\n",
      "Epoch 9/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 24.3862\n",
      "Epoch 00009: val_loss improved from 25.23662 to 22.54068, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.3936 - val_loss: 22.5407\n",
      "Epoch 10/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 23.3284\n",
      "Epoch 00010: val_loss improved from 22.54068 to 19.54539, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.3288 - val_loss: 19.5454\n",
      "Epoch 11/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 22.0626\n",
      "Epoch 00011: val_loss did not improve from 19.54539\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.0650 - val_loss: 25.8621\n",
      "Epoch 12/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 21.3309\n",
      "Epoch 00012: val_loss did not improve from 19.54539\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.3308 - val_loss: 21.0824\n",
      "Epoch 13/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 20.3938\n",
      "Epoch 00013: val_loss did not improve from 19.54539\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.3937 - val_loss: 20.4138\n",
      "Epoch 14/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 19.8963\n",
      "Epoch 00014: val_loss improved from 19.54539 to 18.36850, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.8953 - val_loss: 18.3685\n",
      "Epoch 15/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 19.5520\n",
      "Epoch 00015: val_loss improved from 18.36850 to 15.61217, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.5520 - val_loss: 15.6122\n",
      "Epoch 16/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 19.2756\n",
      "Epoch 00016: val_loss did not improve from 15.61217\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.2724 - val_loss: 17.4732\n",
      "Epoch 17/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 18.8130\n",
      "Epoch 00017: val_loss did not improve from 15.61217\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.8126 - val_loss: 17.2943\n",
      "Epoch 18/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 18.3463\n",
      "Epoch 00018: val_loss improved from 15.61217 to 14.98886, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.3452 - val_loss: 14.9889\n",
      "Epoch 19/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 17.9791\n",
      "Epoch 00019: val_loss did not improve from 14.98886\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.9783 - val_loss: 21.0037\n",
      "Epoch 20/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 17.6779\n",
      "Epoch 00020: val_loss improved from 14.98886 to 14.30963, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.6774 - val_loss: 14.3096\n",
      "Epoch 21/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 17.2133\n",
      "Epoch 00021: val_loss did not improve from 14.30963\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.2133 - val_loss: 24.0412\n",
      "Epoch 22/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 17.2201\n",
      "Epoch 00022: val_loss did not improve from 14.30963\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.2191 - val_loss: 14.3832\n",
      "Epoch 23/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 16.7544\n",
      "Epoch 00023: val_loss improved from 14.30963 to 14.22961, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 16.7578 - val_loss: 14.2296\n",
      "Epoch 24/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 16.8108\n",
      "Epoch 00024: val_loss did not improve from 14.22961\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.8177 - val_loss: 19.5344\n",
      "Epoch 25/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 16.3514\n",
      "Epoch 00025: val_loss improved from 14.22961 to 13.07183, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 16.3514 - val_loss: 13.0718\n",
      "Epoch 26/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 16.1384\n",
      "Epoch 00026: val_loss did not improve from 13.07183\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.1415 - val_loss: 19.7291\n",
      "Epoch 27/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 16.0005\n",
      "Epoch 00027: val_loss did not improve from 13.07183\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.9988 - val_loss: 14.8895\n",
      "Epoch 28/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 16.0814\n",
      "Epoch 00028: val_loss did not improve from 13.07183\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.0802 - val_loss: 13.6223\n",
      "Epoch 29/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 15.6129\n",
      "Epoch 00029: val_loss did not improve from 13.07183\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.6131 - val_loss: 14.9533\n",
      "Epoch 30/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 15.3430\n",
      "Epoch 00030: val_loss did not improve from 13.07183\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.3436 - val_loss: 17.3542\n",
      "Epoch 31/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 15.3924\n",
      "Epoch 00031: val_loss improved from 13.07183 to 12.44178, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.3916 - val_loss: 12.4418\n",
      "Epoch 32/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 15.1557\n",
      "Epoch 00032: val_loss did not improve from 12.44178\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.1585 - val_loss: 15.4024\n",
      "Epoch 33/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 14.9110\n",
      "Epoch 00033: val_loss did not improve from 12.44178\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.9110 - val_loss: 12.4534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 14.6795\n",
      "Epoch 00034: val_loss improved from 12.44178 to 11.55132, saving model to ./experiment_set_12\\results_25\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.6769 - val_loss: 11.5513\n",
      "Epoch 35/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 14.7747\n",
      "Epoch 00035: val_loss did not improve from 11.55132\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.7737 - val_loss: 24.4454\n",
      "Epoch 36/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 14.8554\n",
      "Epoch 00036: val_loss did not improve from 11.55132\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.8554 - val_loss: 14.2762\n",
      "Epoch 37/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 14.4701\n",
      "Epoch 00037: val_loss did not improve from 11.55132\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.4742 - val_loss: 30.6293\n",
      "Epoch 38/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 14.6320\n",
      "Epoch 00038: val_loss did not improve from 11.55132\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.6438 - val_loss: 17.8084\n",
      "Epoch 39/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 14.3475\n",
      "Epoch 00039: val_loss did not improve from 11.55132\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 14.3479 - val_loss: 12.8165\n",
      "Epoch 40/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 14.2658\n",
      "Epoch 00040: val_loss did not improve from 11.55132\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.2658 - val_loss: 13.1102\n",
      "Epoch 41/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 14.2757\n",
      "Epoch 00041: val_loss did not improve from 11.55132\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.2729 - val_loss: 12.0123\n",
      "Epoch 42/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 14.0252\n",
      "Epoch 00042: val_loss did not improve from 11.55132\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.0236 - val_loss: 13.0428\n",
      "Epoch 43/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 13.9264\n",
      "Epoch 00043: val_loss did not improve from 11.55132\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.9308 - val_loss: 22.3037\n",
      "Epoch 44/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 14.0023\n",
      "Epoch 00044: val_loss did not improve from 11.55132\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.0007 - val_loss: 12.8103\n",
      "Epoch 00044: early stopping\n",
      "Saved training history to file: ./experiment_set_12\\results_25\\split_2\\history.pkl\n",
      "Test set:\n",
      "MSE: 11.52\n",
      "RMSE: 3.39\n",
      "CMAPSS score: 1.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# Test effect of mutual information ranking\n",
    "###########################################\n",
    "NUM_TRIALS = 3\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "layer_sizes = [256, 256, 512, 64]\n",
    "\n",
    "num_selected_columns = [10, 15, 20, 25]\n",
    "\n",
    "results_file = os.path.join(output_path, \"results_mi_ranking.csv\")\n",
    "with open(results_file, \"w\") as file:\n",
    "    file.write(\"selected_features,num_features,mse,rmse,cmapss,mse(mean),mse(std),rmse(mean),rmse(std),cmapss(mean),cmapss(std)\\n\")\n",
    "\n",
    "\n",
    "for n in num_selected_columns:\n",
    "    selected_columns = get_mi_ranked_features(mutual_info_series, n)\n",
    "    x_train_feature_selection = x_train[selected_columns]\n",
    "    \n",
    "    mse_vals = []\n",
    "    rmse_vals = []\n",
    "    cmapss_vals = []\n",
    "    \n",
    "    for random_seed in range(NUM_TRIALS):\n",
    "        # Train-validation split for early stopping\n",
    "        x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train_feature_selection, \n",
    "                                                                                  y_train, \n",
    "                                                                                  test_size=0.1, \n",
    "                                                                                  random_state=random_seed)\n",
    "        # Create output path\n",
    "        results_folder =\"results_{}\".format(n)\n",
    "        results_path_crr_num = os.path.join(output_path, results_folder)\n",
    "        results_path_crr_split = os.path.join(results_path_crr_num, \"split_{}\".format(random_seed))\n",
    "        if not os.path.exists(results_path_crr_split):\n",
    "            os.makedirs(results_path_crr_split)\n",
    "\n",
    "        # Standardization\n",
    "        scaler_file = os.path.join(results_path_crr_split, 'scaler.pkl')\n",
    "        scaler = StandardScaler()\n",
    "        x_train_scaled = scaler.fit_transform(x_train_split)\n",
    "        x_val_scaled = scaler.transform(x_val_split)\n",
    "        input_dim = x_train_scaled.shape[1]\n",
    "        save_object(scaler, scaler_file)\n",
    "\n",
    "        # Create model\n",
    "        weights_file = os.path.join(results_path_crr_num, 'mlp_initial_weights.h5')\n",
    "        model_path = os.path.join(results_path_crr_split, 'mlp_model_trained.h5')\n",
    "        \n",
    "        # Save initial weights\n",
    "        if random_seed == 0:\n",
    "            model = create_mlp_model(input_dim, layer_sizes, activation='tanh',\n",
    "                                     output_weights_file=weights_file)\n",
    "        else:\n",
    "            model = create_mlp_model(input_dim, layer_sizes, activation='tanh')\n",
    "        model.summary()\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint(model_path, monitor='val_loss', mode='min', verbose=2, \n",
    "                             save_best_only=True)\n",
    "\n",
    "        # Train model\n",
    "        history = train_model_existing_weights(model, weights_file, \n",
    "                                               x_train_scaled, y_train_split, \n",
    "                                               x_val_scaled, y_val_split, \n",
    "                                               batch_size=batch_size, \n",
    "                                               epochs=epochs, \n",
    "                                               callbacks=[es, mc])\n",
    "\n",
    "        history_file = os.path.join(results_path_crr_split, \"history.pkl\")\n",
    "        save_history(history, history_file)\n",
    "\n",
    "        # Performance evaluation\n",
    "        x_holdout_feature_selection = x_holdout[selected_columns]\n",
    "        x_holdout_scaled = scaler.transform(x_holdout_feature_selection)\n",
    "\n",
    "        loaded_model = load_model(model_path)\n",
    "        predictions_holdout = loaded_model.predict(x_holdout_scaled).flatten()\n",
    "        mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_holdout, y_holdout)\n",
    "        \n",
    "        mse_vals.append(mse)\n",
    "        rmse_vals.append(rmse)\n",
    "        cmapss_vals.append(cmapss_score)\n",
    "    \n",
    "    mse_mean = np.mean(mse_vals)\n",
    "    mse_std = np.std(mse_vals)\n",
    "    rmse_mean = np.mean(rmse_vals)\n",
    "    rmse_std = np.std(rmse_vals)\n",
    "    cmapss_mean = np.mean(cmapss_vals)\n",
    "    cmapss_std = np.std(cmapss_vals)\n",
    "    \n",
    "    with open(results_file, \"a\") as file:\n",
    "        file.write(f\"{feature_list_to_string(selected_columns)}, {len(selected_columns)}, {numbers_list_to_string(mse_vals)}, {numbers_list_to_string(rmse_vals)}, {numbers_list_to_string(cmapss_vals)}, {mse_mean}, {mse_std}, {rmse_mean}, {rmse_std}, {cmapss_mean}, {cmapss_std}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "MSE: 395.70\n",
      "RMSE: 19.89\n",
      "CMAPSS score: 5.95\n",
      "\n",
      "Test set:\n",
      "MSE: 395.56\n",
      "RMSE: 19.89\n",
      "CMAPSS score: 5.95\n",
      "\n",
      "Test set:\n",
      "MSE: 396.45\n",
      "RMSE: 19.91\n",
      "CMAPSS score: 5.95\n",
      "\n",
      "Test set:\n",
      "MSE: 395.70\n",
      "RMSE: 19.89\n",
      "CMAPSS score: 5.95\n",
      "\n",
      "Test set:\n",
      "MSE: 395.56\n",
      "RMSE: 19.89\n",
      "CMAPSS score: 5.95\n",
      "\n",
      "Test set:\n",
      "MSE: 396.45\n",
      "RMSE: 19.91\n",
      "CMAPSS score: 5.95\n",
      "\n",
      "Test set:\n",
      "MSE: 70.49\n",
      "RMSE: 8.40\n",
      "CMAPSS score: 1.88\n",
      "\n",
      "Test set:\n",
      "MSE: 67.36\n",
      "RMSE: 8.21\n",
      "CMAPSS score: 1.85\n",
      "\n",
      "Test set:\n",
      "MSE: 71.10\n",
      "RMSE: 8.43\n",
      "CMAPSS score: 1.88\n",
      "\n",
      "Test set:\n",
      "MSE: 36.42\n",
      "RMSE: 6.04\n",
      "CMAPSS score: 1.54\n",
      "\n",
      "Test set:\n",
      "MSE: 38.33\n",
      "RMSE: 6.19\n",
      "CMAPSS score: 1.54\n",
      "\n",
      "Test set:\n",
      "MSE: 33.33\n",
      "RMSE: 5.77\n",
      "CMAPSS score: 1.51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# Results on test set for mutual info\n",
    "#####################################\n",
    "NUM_TRIALS = 3\n",
    "\n",
    "num_selected_columns = [10, 15, 20, 25]\n",
    "\n",
    "results_file = os.path.join(output_path, \"results_mi_ranking_test_set.csv\")\n",
    "with open(results_file, \"w\") as file:\n",
    "    file.write(\"selected_features,num_features,mse,rmse,cmapss,mse(mean),mse(std),rmse(mean),rmse(std),cmapss(mean),cmapss(std)\\n\")\n",
    "\n",
    "\n",
    "for n in num_selected_columns:\n",
    "    selected_columns = get_mi_ranked_features(mutual_info_series, n)\n",
    "    \n",
    "    mse_vals = []\n",
    "    rmse_vals = []\n",
    "    cmapss_vals = []\n",
    "    \n",
    "    for random_seed in range(NUM_TRIALS):\n",
    "        results_folder =\"results_{}\".format(n)\n",
    "        results_path_crr_th = os.path.join(output_path, results_folder)\n",
    "        results_path_crr_split = os.path.join(results_path_crr_th, \"split_{}\".format(random_seed))\n",
    "        \n",
    "        scaler_file = os.path.join(results_path_crr_split, 'scaler.pkl')\n",
    "        scaler = load_object(scaler_file)\n",
    "\n",
    "        model_path = os.path.join(results_path_crr_split, 'mlp_model_trained.h5')\n",
    "        \n",
    "        # Performance evaluation\n",
    "        x_test_feature_selection = x_test[selected_columns]\n",
    "        x_test_scaled = scaler.transform(x_test_feature_selection)\n",
    "\n",
    "        loaded_model = load_model(model_path)\n",
    "        predictions_test = loaded_model.predict(x_test_scaled).flatten()\n",
    "        mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)\n",
    "        \n",
    "        mse_vals.append(mse)\n",
    "        rmse_vals.append(rmse)\n",
    "        cmapss_vals.append(cmapss_score)\n",
    "    \n",
    "    mse_mean = np.mean(mse_vals)\n",
    "    mse_std = np.std(mse_vals)\n",
    "    rmse_mean = np.mean(rmse_vals)\n",
    "    rmse_std = np.std(rmse_vals)\n",
    "    cmapss_mean = np.mean(cmapss_vals)\n",
    "    cmapss_std = np.std(cmapss_vals)\n",
    "    \n",
    "    with open(results_file, \"a\") as file:\n",
    "        file.write(f\"{feature_list_to_string(selected_columns)}, {len(selected_columns)}, {numbers_list_to_string(mse_vals)}, {numbers_list_to_string(rmse_vals)}, {numbers_list_to_string(cmapss_vals)}, {mse_mean}, {mse_std}, {rmse_mean}, {rmse_std}, {cmapss_mean}, {cmapss_std}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved object to file: ./experiment_set_12_1\\results_0\\split_0\\scaler.pkl\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 238,721\n",
      "Trainable params: 238,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 139.0327\n",
      "Epoch 00001: val_loss improved from inf to 54.70755, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 138.9425 - val_loss: 54.7075\n",
      "Epoch 2/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 40.9899 ETA: 0s - loss: 41.\n",
      "Epoch 00002: val_loss improved from 54.70755 to 35.11496, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 40.9815 - val_loss: 35.1150\n",
      "Epoch 3/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 34.5450\n",
      "Epoch 00003: val_loss improved from 35.11496 to 30.60088, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 34.5437 - val_loss: 30.6009\n",
      "Epoch 4/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 30.1781\n",
      "Epoch 00004: val_loss improved from 30.60088 to 26.57525, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 30.1730 - val_loss: 26.5753\n",
      "Epoch 5/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 26.7803\n",
      "Epoch 00005: val_loss improved from 26.57525 to 25.82915, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 26.7791 - val_loss: 25.8292\n",
      "Epoch 6/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 24.4844\n",
      "Epoch 00006: val_loss improved from 25.82915 to 19.83926, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.4827 - val_loss: 19.8393\n",
      "Epoch 7/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 23.0462\n",
      "Epoch 00007: val_loss did not improve from 19.83926\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 23.0460 - val_loss: 20.6687\n",
      "Epoch 8/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 21.6918\n",
      "Epoch 00008: val_loss improved from 19.83926 to 19.08710, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 21.6846 - val_loss: 19.0871\n",
      "Epoch 9/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 20.8775\n",
      "Epoch 00009: val_loss did not improve from 19.08710\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 20.8775 - val_loss: 21.4279\n",
      "Epoch 10/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 20.2265\n",
      "Epoch 00010: val_loss did not improve from 19.08710\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 20.2284 - val_loss: 20.4352\n",
      "Epoch 11/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 19.5159\n",
      "Epoch 00011: val_loss improved from 19.08710 to 16.54864, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 19.5155 - val_loss: 16.5486\n",
      "Epoch 12/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 19.0299\n",
      "Epoch 00012: val_loss improved from 16.54864 to 15.50362, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 19.0283 - val_loss: 15.5036\n",
      "Epoch 13/200\n",
      "6460/6477 [============================>.] - ETA: 0s - loss: 18.7704\n",
      "Epoch 00013: val_loss did not improve from 15.50362\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 18.7718 - val_loss: 16.2693\n",
      "Epoch 14/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 18.1325\n",
      "Epoch 00014: val_loss did not improve from 15.50362\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 18.1341 - val_loss: 20.1294\n",
      "Epoch 15/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 17.9497\n",
      "Epoch 00015: val_loss did not improve from 15.50362\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 17.9485 - val_loss: 17.4894\n",
      "Epoch 16/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 17.4528\n",
      "Epoch 00016: val_loss did not improve from 15.50362\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 17.4528 - val_loss: 16.0777\n",
      "Epoch 17/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 17.2562\n",
      "Epoch 00017: val_loss did not improve from 15.50362\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 17.2563 - val_loss: 16.1152\n",
      "Epoch 18/200\n",
      "6460/6477 [============================>.] - ETA: 0s - loss: 16.9023\n",
      "Epoch 00018: val_loss did not improve from 15.50362\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 16.9147 - val_loss: 36.7866\n",
      "Epoch 19/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 16.7059\n",
      "Epoch 00019: val_loss did not improve from 15.50362\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 16.7051 - val_loss: 19.3698\n",
      "Epoch 20/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 16.3576\n",
      "Epoch 00020: val_loss improved from 15.50362 to 15.25133, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 16.3547 - val_loss: 15.2513\n",
      "Epoch 21/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 15.9659\n",
      "Epoch 00021: val_loss improved from 15.25133 to 14.47789, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.9646 - val_loss: 14.4779\n",
      "Epoch 22/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 15.6040\n",
      "Epoch 00022: val_loss did not improve from 14.47789\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.6059 - val_loss: 14.8574\n",
      "Epoch 23/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 15.3840\n",
      "Epoch 00023: val_loss did not improve from 14.47789\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 15.4010 - val_loss: 18.7024\n",
      "Epoch 24/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 15.5045\n",
      "Epoch 00024: val_loss improved from 14.47789 to 13.91636, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.5023 - val_loss: 13.9164\n",
      "Epoch 25/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 15.1038\n",
      "Epoch 00025: val_loss improved from 13.91636 to 13.82843, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.1036 - val_loss: 13.8284\n",
      "Epoch 26/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 14.9122\n",
      "Epoch 00026: val_loss did not improve from 13.82843\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 14.9143 - val_loss: 19.5356\n",
      "Epoch 27/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 14.6615\n",
      "Epoch 00027: val_loss did not improve from 13.82843\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 14.6685 - val_loss: 32.6612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 14.5480\n",
      "Epoch 00028: val_loss did not improve from 13.82843\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 14.5483 - val_loss: 13.9927\n",
      "Epoch 29/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 14.4885\n",
      "Epoch 00029: val_loss did not improve from 13.82843\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 14.4911 - val_loss: 15.9742\n",
      "Epoch 30/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 14.3001\n",
      "Epoch 00030: val_loss improved from 13.82843 to 12.58283, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 14.2978 - val_loss: 12.5828\n",
      "Epoch 31/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 14.2915\n",
      "Epoch 00031: val_loss did not improve from 12.58283\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.2921 - val_loss: 13.7353\n",
      "Epoch 32/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 13.9285\n",
      "Epoch 00032: val_loss did not improve from 12.58283\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 13.9344 - val_loss: 14.8323\n",
      "Epoch 33/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 13.7917\n",
      "Epoch 00033: val_loss did not improve from 12.58283\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 13.7888 - val_loss: 14.5338\n",
      "Epoch 34/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 13.5996\n",
      "Epoch 00034: val_loss improved from 12.58283 to 12.39988, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 13.5982 - val_loss: 12.3999\n",
      "Epoch 35/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 13.6397\n",
      "Epoch 00035: val_loss improved from 12.39988 to 11.25725, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.6384 - val_loss: 11.2573\n",
      "Epoch 36/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 13.6176\n",
      "Epoch 00036: val_loss did not improve from 11.25725\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.6145 - val_loss: 12.9332\n",
      "Epoch 37/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 13.4875\n",
      "Epoch 00037: val_loss did not improve from 11.25725\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 13.4896 - val_loss: 15.6441\n",
      "Epoch 38/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 13.1976\n",
      "Epoch 00038: val_loss did not improve from 11.25725\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 13.1985 - val_loss: 12.0049\n",
      "Epoch 39/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 13.1416\n",
      "Epoch 00039: val_loss did not improve from 11.25725\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.1416 - val_loss: 12.7037\n",
      "Epoch 40/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 13.2368\n",
      "Epoch 00040: val_loss did not improve from 11.25725\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 13.2356 - val_loss: 11.7141\n",
      "Epoch 41/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 13.0474\n",
      "Epoch 00041: val_loss did not improve from 11.25725\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 13.0441 - val_loss: 13.4160\n",
      "Epoch 42/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 12.8489\n",
      "Epoch 00042: val_loss did not improve from 11.25725\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.8489 - val_loss: 11.6326\n",
      "Epoch 43/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 12.7403\n",
      "Epoch 00043: val_loss improved from 11.25725 to 9.80575, saving model to ./experiment_set_12_1\\results_0\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.7360 - val_loss: 9.8058\n",
      "Epoch 44/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 12.4928\n",
      "Epoch 00044: val_loss did not improve from 9.80575\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.4916 - val_loss: 10.9205\n",
      "Epoch 45/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 12.6122\n",
      "Epoch 00045: val_loss did not improve from 9.80575\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.6087 - val_loss: 11.4083\n",
      "Epoch 46/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 12.5665\n",
      "Epoch 00046: val_loss did not improve from 9.80575\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.5657 - val_loss: 19.8578\n",
      "Epoch 47/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 12.3060\n",
      "Epoch 00047: val_loss did not improve from 9.80575\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.3060 - val_loss: 13.3327\n",
      "Epoch 48/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 12.4362\n",
      "Epoch 00048: val_loss did not improve from 9.80575\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.4371 - val_loss: 12.2205\n",
      "Epoch 49/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 12.3534\n",
      "Epoch 00049: val_loss did not improve from 9.80575\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.3531 - val_loss: 10.0938\n",
      "Epoch 50/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 12.4483\n",
      "Epoch 00050: val_loss did not improve from 9.80575\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.4512 - val_loss: 17.5793\n",
      "Epoch 51/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 12.0268\n",
      "Epoch 00051: val_loss did not improve from 9.80575\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 12.0275 - val_loss: 11.3904\n",
      "Epoch 52/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 11.7441\n",
      "Epoch 00052: val_loss did not improve from 9.80575\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 11.7401 - val_loss: 13.0567\n",
      "Epoch 53/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 12.0746\n",
      "Epoch 00053: val_loss did not improve from 9.80575\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.0738 - val_loss: 12.1949\n",
      "Epoch 00053: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training history to file: ./experiment_set_12_1\\results_0\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 9.77\n",
      "RMSE: 3.13\n",
      "CMAPSS score: 1.21\n",
      "\n",
      "Saved object to file: ./experiment_set_12_1\\results_0\\split_1\\scaler.pkl\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_65 (Dense)             (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 238,721\n",
      "Trainable params: 238,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 141.1682\n",
      "Epoch 00001: val_loss improved from inf to 50.98642, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 141.0599 - val_loss: 50.9864\n",
      "Epoch 2/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 42.3993\n",
      "Epoch 00002: val_loss improved from 50.98642 to 37.49691, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 42.3968 - val_loss: 37.4969\n",
      "Epoch 3/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 34.5535\n",
      "Epoch 00003: val_loss improved from 37.49691 to 35.69128, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 34.5483 - val_loss: 35.6913\n",
      "Epoch 4/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 29.4134\n",
      "Epoch 00004: val_loss improved from 35.69128 to 25.10777, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 29.4087 - val_loss: 25.1078\n",
      "Epoch 5/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 26.1260\n",
      "Epoch 00005: val_loss did not improve from 25.10777\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 26.1227 - val_loss: 25.2876\n",
      "Epoch 6/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 24.2854\n",
      "Epoch 00006: val_loss improved from 25.10777 to 21.11782, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.2874 - val_loss: 21.1178\n",
      "Epoch 7/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 22.8520\n",
      "Epoch 00007: val_loss did not improve from 21.11782\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.8547 - val_loss: 27.9002\n",
      "Epoch 8/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 22.0133\n",
      "Epoch 00008: val_loss improved from 21.11782 to 19.87539, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.0092 - val_loss: 19.8754\n",
      "Epoch 9/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 21.0733\n",
      "Epoch 00009: val_loss did not improve from 19.87539\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.0792 - val_loss: 29.1470\n",
      "Epoch 10/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 20.4803\n",
      "Epoch 00010: val_loss did not improve from 19.87539\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.4780 - val_loss: 22.6724\n",
      "Epoch 11/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 19.7651\n",
      "Epoch 00011: val_loss improved from 19.87539 to 17.29612, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 19.7586 - val_loss: 17.2961\n",
      "Epoch 12/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 19.1117\n",
      "Epoch 00012: val_loss did not improve from 17.29612\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.1133 - val_loss: 21.9386\n",
      "Epoch 13/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 18.3618\n",
      "Epoch 00013: val_loss improved from 17.29612 to 14.80240, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 18.3574 - val_loss: 14.8024\n",
      "Epoch 14/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 17.8741\n",
      "Epoch 00014: val_loss did not improve from 14.80240\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.8741 - val_loss: 16.8052\n",
      "Epoch 15/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 17.8744\n",
      "Epoch 00015: val_loss did not improve from 14.80240\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.8746 - val_loss: 22.4761\n",
      "Epoch 16/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 17.4373\n",
      "Epoch 00016: val_loss improved from 14.80240 to 14.38191, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 17.4371 - val_loss: 14.3819\n",
      "Epoch 17/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 17.1798\n",
      "Epoch 00017: val_loss did not improve from 14.38191\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 17.1806 - val_loss: 16.7623\n",
      "Epoch 18/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 16.6324\n",
      "Epoch 00018: val_loss did not improve from 14.38191\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.6299 - val_loss: 15.3930\n",
      "Epoch 19/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 16.3737\n",
      "Epoch 00019: val_loss did not improve from 14.38191\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.3705 - val_loss: 15.0403\n",
      "Epoch 20/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 16.1944\n",
      "Epoch 00020: val_loss did not improve from 14.38191\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.1915 - val_loss: 19.9980\n",
      "Epoch 21/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 16.0096\n",
      "Epoch 00021: val_loss did not improve from 14.38191\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.0096 - val_loss: 15.2619\n",
      "Epoch 22/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 15.8386\n",
      "Epoch 00022: val_loss did not improve from 14.38191\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.8373 - val_loss: 21.0746\n",
      "Epoch 23/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 15.2409\n",
      "Epoch 00023: val_loss did not improve from 14.38191\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.2413 - val_loss: 18.4833\n",
      "Epoch 24/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 15.1400\n",
      "Epoch 00024: val_loss did not improve from 14.38191\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.1384 - val_loss: 14.9509\n",
      "Epoch 25/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 15.1735\n",
      "Epoch 00025: val_loss did not improve from 14.38191\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.1754 - val_loss: 18.9926\n",
      "Epoch 26/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 14.8072\n",
      "Epoch 00026: val_loss improved from 14.38191 to 12.92452, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.8000 - val_loss: 12.9245\n",
      "Epoch 27/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 14.4373\n",
      "Epoch 00027: val_loss did not improve from 12.92452\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.4377 - val_loss: 13.1473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 14.3186\n",
      "Epoch 00028: val_loss did not improve from 12.92452\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 14.3176 - val_loss: 14.8721\n",
      "Epoch 29/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 14.1041\n",
      "Epoch 00029: val_loss did not improve from 12.92452\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.1021 - val_loss: 18.2243\n",
      "Epoch 30/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 14.0832\n",
      "Epoch 00030: val_loss did not improve from 12.92452\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.0873 - val_loss: 13.1655\n",
      "Epoch 31/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 13.8916\n",
      "Epoch 00031: val_loss did not improve from 12.92452\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.8925 - val_loss: 14.6521\n",
      "Epoch 32/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 13.3999\n",
      "Epoch 00032: val_loss did not improve from 12.92452\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.4012 - val_loss: 24.7525\n",
      "Epoch 33/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 13.8564\n",
      "Epoch 00033: val_loss improved from 12.92452 to 11.99669, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 13.8561 - val_loss: 11.9967\n",
      "Epoch 34/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 13.5067\n",
      "Epoch 00034: val_loss did not improve from 11.99669\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.5093 - val_loss: 13.9795\n",
      "Epoch 35/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 13.4242\n",
      "Epoch 00035: val_loss did not improve from 11.99669\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.4272 - val_loss: 22.4938\n",
      "Epoch 36/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 13.1049\n",
      "Epoch 00036: val_loss improved from 11.99669 to 10.75199, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 13.1018 - val_loss: 10.7520\n",
      "Epoch 37/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 13.0508\n",
      "Epoch 00037: val_loss improved from 10.75199 to 10.65083, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.0462 - val_loss: 10.6508\n",
      "Epoch 38/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 13.0371\n",
      "Epoch 00038: val_loss did not improve from 10.65083\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.0371 - val_loss: 12.6183\n",
      "Epoch 39/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 12.8716\n",
      "Epoch 00039: val_loss did not improve from 10.65083\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 12.8709 - val_loss: 11.5059\n",
      "Epoch 40/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 12.9080\n",
      "Epoch 00040: val_loss did not improve from 10.65083\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.9038 - val_loss: 10.8924\n",
      "Epoch 41/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 12.5290\n",
      "Epoch 00041: val_loss did not improve from 10.65083\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 12.5292 - val_loss: 14.8566\n",
      "Epoch 42/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 12.4661\n",
      "Epoch 00042: val_loss improved from 10.65083 to 9.74168, saving model to ./experiment_set_12_1\\results_0\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.4661 - val_loss: 9.7417\n",
      "Epoch 43/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 12.3928\n",
      "Epoch 00043: val_loss did not improve from 9.74168\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.3898 - val_loss: 10.2299\n",
      "Epoch 44/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 12.4679\n",
      "Epoch 00044: val_loss did not improve from 9.74168\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.4679 - val_loss: 12.0154\n",
      "Epoch 45/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 12.2301\n",
      "Epoch 00045: val_loss did not improve from 9.74168\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.2296 - val_loss: 10.7411\n",
      "Epoch 46/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 12.0789\n",
      "Epoch 00046: val_loss did not improve from 9.74168\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.0773 - val_loss: 10.2696\n",
      "Epoch 47/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 12.0398\n",
      "Epoch 00047: val_loss did not improve from 9.74168\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.0388 - val_loss: 10.5210\n",
      "Epoch 48/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 12.1798\n",
      "Epoch 00048: val_loss did not improve from 9.74168\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.1821 - val_loss: 13.1235\n",
      "Epoch 49/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 11.9500\n",
      "Epoch 00049: val_loss did not improve from 9.74168\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 11.9533 - val_loss: 16.0192\n",
      "Epoch 50/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 11.9497\n",
      "Epoch 00050: val_loss did not improve from 9.74168\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 11.9468 - val_loss: 9.9426\n",
      "Epoch 51/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 11.6074\n",
      "Epoch 00051: val_loss did not improve from 9.74168\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 11.6075 - val_loss: 11.2880\n",
      "Epoch 52/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 11.7696\n",
      "Epoch 00052: val_loss did not improve from 9.74168\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 11.7690 - val_loss: 15.8415\n",
      "Epoch 00052: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training history to file: ./experiment_set_12_1\\results_0\\split_1\\history.pkl\n",
      "Test set:\n",
      "MSE: 9.77\n",
      "RMSE: 3.13\n",
      "CMAPSS score: 1.20\n",
      "\n",
      "Saved object to file: ./experiment_set_12_1\\results_0\\split_2\\scaler.pkl\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_70 (Dense)             (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 238,721\n",
      "Trainable params: 238,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 138.8442\n",
      "Epoch 00001: val_loss improved from inf to 48.25817, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 29s 4ms/step - loss: 138.8366 - val_loss: 48.2582\n",
      "Epoch 2/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 42.9719\n",
      "Epoch 00002: val_loss improved from 48.25817 to 36.47898, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 42.9646 - val_loss: 36.4790\n",
      "Epoch 3/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 35.3713\n",
      "Epoch 00003: val_loss improved from 36.47898 to 31.60381, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 35.3677 - val_loss: 31.6038\n",
      "Epoch 4/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 29.7911\n",
      "Epoch 00004: val_loss improved from 31.60381 to 26.28889, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 29.7838 - val_loss: 26.2889\n",
      "Epoch 5/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 26.5659\n",
      "Epoch 00005: val_loss did not improve from 26.28889\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 26.5694 - val_loss: 41.7753\n",
      "Epoch 6/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 24.5769\n",
      "Epoch 00006: val_loss improved from 26.28889 to 21.17235, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 24.5739 - val_loss: 21.1723\n",
      "Epoch 7/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 23.0328\n",
      "Epoch 00007: val_loss improved from 21.17235 to 19.34734, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 23.0306 - val_loss: 19.3473\n",
      "Epoch 8/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 22.5050\n",
      "Epoch 00008: val_loss did not improve from 19.34734\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 22.5047 - val_loss: 22.1150\n",
      "Epoch 9/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 21.2360\n",
      "Epoch 00009: val_loss did not improve from 19.34734\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 21.2404 - val_loss: 29.3865\n",
      "Epoch 10/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 20.3144\n",
      "Epoch 00010: val_loss improved from 19.34734 to 16.70863, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 20.3175 - val_loss: 16.7086\n",
      "Epoch 11/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 19.7193\n",
      "Epoch 00011: val_loss did not improve from 16.70863\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 19.7209 - val_loss: 17.2656\n",
      "Epoch 12/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 19.2206\n",
      "Epoch 00012: val_loss improved from 16.70863 to 16.69723, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 19.2189 - val_loss: 16.6972\n",
      "Epoch 13/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 18.6945\n",
      "Epoch 00013: val_loss improved from 16.69723 to 16.38209, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 18.6899 - val_loss: 16.3821\n",
      "Epoch 14/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 18.1445\n",
      "Epoch 00014: val_loss did not improve from 16.38209\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 18.1434 - val_loss: 16.5184\n",
      "Epoch 15/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 17.6994\n",
      "Epoch 00015: val_loss improved from 16.38209 to 14.20859, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 17.6964 - val_loss: 14.2086\n",
      "Epoch 16/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 17.4228\n",
      "Epoch 00016: val_loss did not improve from 14.20859\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 17.4308 - val_loss: 15.2655\n",
      "Epoch 17/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 16.9955\n",
      "Epoch 00017: val_loss improved from 14.20859 to 14.15712, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 16.9934 - val_loss: 14.1571\n",
      "Epoch 18/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 16.5515\n",
      "Epoch 00018: val_loss did not improve from 14.15712\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.5492 - val_loss: 19.1758\n",
      "Epoch 19/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 16.4063\n",
      "Epoch 00019: val_loss improved from 14.15712 to 13.02519, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 16.4032 - val_loss: 13.0252\n",
      "Epoch 20/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 16.1388\n",
      "Epoch 00020: val_loss did not improve from 13.02519\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 16.1396 - val_loss: 20.6003\n",
      "Epoch 21/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 15.8555\n",
      "Epoch 00021: val_loss did not improve from 13.02519\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 15.8590 - val_loss: 15.9193\n",
      "Epoch 22/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 15.5585\n",
      "Epoch 00022: val_loss improved from 13.02519 to 12.53092, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 15.5581 - val_loss: 12.5309\n",
      "Epoch 23/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 15.0825\n",
      "Epoch 00023: val_loss did not improve from 12.53092\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 15.0860 - val_loss: 12.6199\n",
      "Epoch 24/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 14.9453\n",
      "Epoch 00024: val_loss did not improve from 12.53092\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.9459 - val_loss: 17.6926\n",
      "Epoch 25/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 14.8807\n",
      "Epoch 00025: val_loss did not improve from 12.53092\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.8791 - val_loss: 14.0135\n",
      "Epoch 26/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 14.8694\n",
      "Epoch 00026: val_loss did not improve from 12.53092\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.8706 - val_loss: 20.9359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 14.5753\n",
      "Epoch 00027: val_loss improved from 12.53092 to 12.31679, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.5717 - val_loss: 12.3168\n",
      "Epoch 28/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 14.2401\n",
      "Epoch 00028: val_loss did not improve from 12.31679\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.2404 - val_loss: 14.2043\n",
      "Epoch 29/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 14.2574\n",
      "Epoch 00029: val_loss did not improve from 12.31679\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.2594 - val_loss: 14.3012\n",
      "Epoch 30/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 14.1167\n",
      "Epoch 00030: val_loss did not improve from 12.31679\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 14.1156 - val_loss: 16.6902\n",
      "Epoch 31/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 13.9645\n",
      "Epoch 00031: val_loss did not improve from 12.31679\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.9631 - val_loss: 14.2753\n",
      "Epoch 32/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 13.4635\n",
      "Epoch 00032: val_loss improved from 12.31679 to 11.19252, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 13.4640 - val_loss: 11.1925\n",
      "Epoch 33/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 13.5895\n",
      "Epoch 00033: val_loss did not improve from 11.19252\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 13.5864 - val_loss: 11.6864\n",
      "Epoch 34/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 13.4830\n",
      "Epoch 00034: val_loss did not improve from 11.19252\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 13.4828 - val_loss: 12.1572\n",
      "Epoch 35/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 13.2213\n",
      "Epoch 00035: val_loss did not improve from 11.19252\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 13.2236 - val_loss: 13.3216\n",
      "Epoch 36/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 13.2086\n",
      "Epoch 00036: val_loss did not improve from 11.19252\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 13.2142 - val_loss: 18.0289\n",
      "Epoch 37/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 13.1051\n",
      "Epoch 00037: val_loss did not improve from 11.19252\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 13.1043 - val_loss: 19.6013\n",
      "Epoch 38/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 12.9049\n",
      "Epoch 00038: val_loss did not improve from 11.19252\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 12.9049 - val_loss: 13.3057\n",
      "Epoch 39/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 12.8500\n",
      "Epoch 00039: val_loss did not improve from 11.19252\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.8500 - val_loss: 14.0544\n",
      "Epoch 40/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 12.8011\n",
      "Epoch 00040: val_loss improved from 11.19252 to 10.60338, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 12.8006 - val_loss: 10.6034\n",
      "Epoch 41/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 12.5458\n",
      "Epoch 00041: val_loss did not improve from 10.60338\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.5443 - val_loss: 12.1184\n",
      "Epoch 42/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 12.4540\n",
      "Epoch 00042: val_loss did not improve from 10.60338\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 12.4531 - val_loss: 12.5710\n",
      "Epoch 43/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 12.4221\n",
      "Epoch 00043: val_loss did not improve from 10.60338\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 12.4212 - val_loss: 11.6927\n",
      "Epoch 44/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 12.4258\n",
      "Epoch 00044: val_loss did not improve from 10.60338\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 12.4265 - val_loss: 17.4367\n",
      "Epoch 45/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 12.1971\n",
      "Epoch 00045: val_loss did not improve from 10.60338\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 12.2028 - val_loss: 13.4321\n",
      "Epoch 46/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 12.0270\n",
      "Epoch 00046: val_loss did not improve from 10.60338\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 12.0263 - val_loss: 10.7960\n",
      "Epoch 47/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 12.0877\n",
      "Epoch 00047: val_loss did not improve from 10.60338\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 12.0858 - val_loss: 10.8311\n",
      "Epoch 48/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 11.9646\n",
      "Epoch 00048: val_loss improved from 10.60338 to 9.68346, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 11.9648 - val_loss: 9.6835\n",
      "Epoch 49/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 11.9313\n",
      "Epoch 00049: val_loss did not improve from 9.68346\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 11.9290 - val_loss: 12.5119\n",
      "Epoch 50/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 11.8541\n",
      "Epoch 00050: val_loss did not improve from 9.68346\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 11.8508 - val_loss: 11.4528\n",
      "Epoch 51/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 11.8084\n",
      "Epoch 00051: val_loss did not improve from 9.68346\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 11.8102 - val_loss: 12.5278\n",
      "Epoch 52/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 11.7754\n",
      "Epoch 00052: val_loss did not improve from 9.68346\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 11.7731 - val_loss: 13.7481\n",
      "Epoch 53/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 11.8024\n",
      "Epoch 00053: val_loss did not improve from 9.68346\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 11.8030 - val_loss: 11.6040\n",
      "Epoch 54/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 11.4692\n",
      "Epoch 00054: val_loss did not improve from 9.68346\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 11.4695 - val_loss: 11.5945\n",
      "Epoch 55/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 11.4572\n",
      "Epoch 00055: val_loss improved from 9.68346 to 9.52346, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 11.4567 - val_loss: 9.5235\n",
      "Epoch 56/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 11.2556\n",
      "Epoch 00056: val_loss did not improve from 9.52346\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 11.2552 - val_loss: 9.8837\n",
      "Epoch 57/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 11.3046\n",
      "Epoch 00057: val_loss did not improve from 9.52346\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 11.3046 - val_loss: 19.5947\n",
      "Epoch 58/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 11.3418\n",
      "Epoch 00058: val_loss did not improve from 9.52346\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 11.3470 - val_loss: 12.1240\n",
      "Epoch 59/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 11.2967\n",
      "Epoch 00059: val_loss improved from 9.52346 to 9.41543, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 28s 4ms/step - loss: 11.2960 - val_loss: 9.4154\n",
      "Epoch 60/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 11.1483\n",
      "Epoch 00060: val_loss did not improve from 9.41543\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 11.1483 - val_loss: 25.1799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 11.1640\n",
      "Epoch 00061: val_loss did not improve from 9.41543\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 11.1627 - val_loss: 13.6644\n",
      "Epoch 62/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 11.2106\n",
      "Epoch 00062: val_loss improved from 9.41543 to 8.84009, saving model to ./experiment_set_12_1\\results_0\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 11.2108 - val_loss: 8.8401\n",
      "Epoch 63/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 11.1462\n",
      "Epoch 00063: val_loss did not improve from 8.84009\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 11.1489 - val_loss: 18.2914\n",
      "Epoch 64/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 11.1035\n",
      "Epoch 00064: val_loss did not improve from 8.84009\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 11.1084 - val_loss: 25.7421\n",
      "Epoch 65/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 10.8205\n",
      "Epoch 00065: val_loss did not improve from 8.84009\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 10.8255 - val_loss: 20.0209\n",
      "Epoch 66/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 10.8319\n",
      "Epoch 00066: val_loss did not improve from 8.84009\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 10.8319 - val_loss: 14.1325\n",
      "Epoch 67/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 10.7368\n",
      "Epoch 00067: val_loss did not improve from 8.84009\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 10.7343 - val_loss: 10.6688\n",
      "Epoch 68/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 10.8452\n",
      "Epoch 00068: val_loss did not improve from 8.84009\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 10.8480 - val_loss: 11.0851\n",
      "Epoch 69/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 10.6726\n",
      "Epoch 00069: val_loss did not improve from 8.84009\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 10.6724 - val_loss: 13.1794\n",
      "Epoch 70/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 10.5442\n",
      "Epoch 00070: val_loss did not improve from 8.84009\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 10.5442 - val_loss: 15.2389\n",
      "Epoch 71/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 10.7166\n",
      "Epoch 00071: val_loss did not improve from 8.84009\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 10.7202 - val_loss: 14.5219\n",
      "Epoch 72/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 10.5555\n",
      "Epoch 00072: val_loss did not improve from 8.84009\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 10.5578 - val_loss: 9.5613\n",
      "Epoch 00072: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training history to file: ./experiment_set_12_1\\results_0\\split_2\\history.pkl\n",
      "Test set:\n",
      "MSE: 8.79\n",
      "RMSE: 2.96\n",
      "CMAPSS score: 1.19\n",
      "\n",
      "Saved object to file: ./experiment_set_12_1\\results_1\\split_0\\scaler.pkl\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 256)               4864      \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 235,137\n",
      "Trainable params: 235,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 140.3945\n",
      "Epoch 00001: val_loss improved from inf to 57.88875, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 140.3194 - val_loss: 57.8888\n",
      "Epoch 2/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 48.6375\n",
      "Epoch 00002: val_loss improved from 57.88875 to 42.00227, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 48.6302 - val_loss: 42.0023\n",
      "Epoch 3/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 42.9337\n",
      "Epoch 00003: val_loss improved from 42.00227 to 38.53376, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 42.9325 - val_loss: 38.5338\n",
      "Epoch 4/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 38.6922\n",
      "Epoch 00004: val_loss improved from 38.53376 to 33.97899, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 38.6855 - val_loss: 33.9790\n",
      "Epoch 5/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 35.4343\n",
      "Epoch 00005: val_loss improved from 33.97899 to 30.91519, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 35.4332 - val_loss: 30.9152\n",
      "Epoch 6/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 33.5182\n",
      "Epoch 00006: val_loss did not improve from 30.91519\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 33.5201 - val_loss: 39.3176\n",
      "Epoch 7/200\n",
      "6460/6477 [============================>.] - ETA: 0s - loss: 32.0481\n",
      "Epoch 00007: val_loss improved from 30.91519 to 30.10994, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 32.0380 - val_loss: 30.1099\n",
      "Epoch 8/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 30.5842\n",
      "Epoch 00008: val_loss improved from 30.10994 to 28.11050, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 30.5830 - val_loss: 28.1105\n",
      "Epoch 9/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 29.2363\n",
      "Epoch 00009: val_loss did not improve from 28.11050\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 29.2427 - val_loss: 29.3998\n",
      "Epoch 10/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 28.2934\n",
      "Epoch 00010: val_loss improved from 28.11050 to 27.81247, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 28.2978 - val_loss: 27.8125\n",
      "Epoch 11/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 27.5950\n",
      "Epoch 00011: val_loss improved from 27.81247 to 25.08047, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 27.5965 - val_loss: 25.0805\n",
      "Epoch 12/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 26.7350\n",
      "Epoch 00012: val_loss did not improve from 25.08047\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 26.7354 - val_loss: 26.0943\n",
      "Epoch 13/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 26.1160\n",
      "Epoch 00013: val_loss improved from 25.08047 to 23.70299, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 26.1147 - val_loss: 23.7030\n",
      "Epoch 14/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 25.6400\n",
      "Epoch 00014: val_loss did not improve from 23.70299\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 25.6386 - val_loss: 28.5623\n",
      "Epoch 15/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 24.9238\n",
      "Epoch 00015: val_loss improved from 23.70299 to 21.58464, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.9245 - val_loss: 21.5846\n",
      "Epoch 16/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 24.5496\n",
      "Epoch 00016: val_loss did not improve from 21.58464\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.5459 - val_loss: 25.6152\n",
      "Epoch 17/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 24.0780\n",
      "Epoch 00017: val_loss did not improve from 21.58464\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.0780 - val_loss: 24.1336\n",
      "Epoch 18/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 23.8130\n",
      "Epoch 00018: val_loss did not improve from 21.58464\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.8093 - val_loss: 21.9002\n",
      "Epoch 19/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 23.3742\n",
      "Epoch 00019: val_loss did not improve from 21.58464\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.3752 - val_loss: 25.0469\n",
      "Epoch 20/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 23.1152\n",
      "Epoch 00020: val_loss improved from 21.58464 to 21.15164, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 23.1161 - val_loss: 21.1516\n",
      "Epoch 21/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 22.4896\n",
      "Epoch 00021: val_loss did not improve from 21.15164\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.4901 - val_loss: 23.2805\n",
      "Epoch 22/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 22.1727\n",
      "Epoch 00022: val_loss did not improve from 21.15164\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.1736 - val_loss: 22.6973\n",
      "Epoch 23/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 21.7119\n",
      "Epoch 00023: val_loss did not improve from 21.15164\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.7204 - val_loss: 22.6734\n",
      "Epoch 24/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 21.6588\n",
      "Epoch 00024: val_loss did not improve from 21.15164\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.6589 - val_loss: 26.1530\n",
      "Epoch 25/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 21.1617\n",
      "Epoch 00025: val_loss improved from 21.15164 to 20.49631, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.1611 - val_loss: 20.4963\n",
      "Epoch 26/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 20.9943\n",
      "Epoch 00026: val_loss did not improve from 20.49631\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.9943 - val_loss: 23.3371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 20.8384\n",
      "Epoch 00027: val_loss did not improve from 20.49631\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.8390 - val_loss: 27.3601\n",
      "Epoch 28/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 20.5560\n",
      "Epoch 00028: val_loss did not improve from 20.49631\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.5542 - val_loss: 21.1505\n",
      "Epoch 29/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 20.2973\n",
      "Epoch 00029: val_loss did not improve from 20.49631\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.2956 - val_loss: 20.7407\n",
      "Epoch 30/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 20.0608\n",
      "Epoch 00030: val_loss improved from 20.49631 to 19.56222, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 20.0619 - val_loss: 19.5622\n",
      "Epoch 31/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 19.9364\n",
      "Epoch 00031: val_loss did not improve from 19.56222\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.9345 - val_loss: 19.8697\n",
      "Epoch 32/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 19.7004\n",
      "Epoch 00032: val_loss improved from 19.56222 to 19.00756, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 19.7017 - val_loss: 19.0076\n",
      "Epoch 33/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 19.6497\n",
      "Epoch 00033: val_loss did not improve from 19.00756\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.6525 - val_loss: 22.3805\n",
      "Epoch 34/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 19.3418\n",
      "Epoch 00034: val_loss did not improve from 19.00756\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.3414 - val_loss: 21.8636\n",
      "Epoch 35/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 19.2247\n",
      "Epoch 00035: val_loss improved from 19.00756 to 17.01117, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.2212 - val_loss: 17.0112\n",
      "Epoch 36/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 18.9681\n",
      "Epoch 00036: val_loss improved from 17.01117 to 16.70079, saving model to ./experiment_set_12_1\\results_1\\split_0\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.9683 - val_loss: 16.7008\n",
      "Epoch 37/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 18.8750\n",
      "Epoch 00037: val_loss did not improve from 16.70079\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.8790 - val_loss: 22.3192\n",
      "Epoch 38/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 18.9211\n",
      "Epoch 00038: val_loss did not improve from 16.70079\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.9210 - val_loss: 16.8237\n",
      "Epoch 39/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 18.7613\n",
      "Epoch 00039: val_loss did not improve from 16.70079\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.7583 - val_loss: 19.9539\n",
      "Epoch 40/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 18.4098\n",
      "Epoch 00040: val_loss did not improve from 16.70079\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.4089 - val_loss: 18.7562\n",
      "Epoch 41/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 18.1815\n",
      "Epoch 00041: val_loss did not improve from 16.70079\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.1795 - val_loss: 19.9965\n",
      "Epoch 42/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 18.2704\n",
      "Epoch 00042: val_loss did not improve from 16.70079\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.2713 - val_loss: 19.2493\n",
      "Epoch 43/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 18.1177\n",
      "Epoch 00043: val_loss did not improve from 16.70079\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.1177 - val_loss: 18.1731\n",
      "Epoch 44/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 18.0069\n",
      "Epoch 00044: val_loss did not improve from 16.70079\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.0117 - val_loss: 19.3742\n",
      "Epoch 45/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 17.8915\n",
      "Epoch 00045: val_loss did not improve from 16.70079\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.8924 - val_loss: 19.3253\n",
      "Epoch 46/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 17.8709\n",
      "Epoch 00046: val_loss did not improve from 16.70079\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.8704 - val_loss: 18.4441\n",
      "Epoch 00046: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training history to file: ./experiment_set_12_1\\results_1\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 16.69\n",
      "RMSE: 4.08\n",
      "CMAPSS score: 1.31\n",
      "\n",
      "Saved object to file: ./experiment_set_12_1\\results_1\\split_1\\scaler.pkl\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_80 (Dense)             (None, 256)               4864      \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 235,137\n",
      "Trainable params: 235,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 141.2986\n",
      "Epoch 00001: val_loss improved from inf to 50.36244, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 141.1734 - val_loss: 50.3624\n",
      "Epoch 2/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 49.7277\n",
      "Epoch 00002: val_loss improved from 50.36244 to 49.25936, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 49.7278 - val_loss: 49.2594\n",
      "Epoch 3/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 43.9799\n",
      "Epoch 00003: val_loss improved from 49.25936 to 38.57597, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 43.9766 - val_loss: 38.5760\n",
      "Epoch 4/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 39.3524\n",
      "Epoch 00004: val_loss improved from 38.57597 to 36.94670, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 39.3524 - val_loss: 36.9467\n",
      "Epoch 5/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 36.2193\n",
      "Epoch 00005: val_loss improved from 36.94670 to 33.96153, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 36.2151 - val_loss: 33.9615\n",
      "Epoch 6/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 34.1518\n",
      "Epoch 00006: val_loss improved from 33.96153 to 30.64763, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 34.1484 - val_loss: 30.6476\n",
      "Epoch 7/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 32.5919\n",
      "Epoch 00007: val_loss did not improve from 30.64763\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 32.5920 - val_loss: 33.2606\n",
      "Epoch 8/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 31.2595\n",
      "Epoch 00008: val_loss improved from 30.64763 to 30.59297, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 31.2568 - val_loss: 30.5930\n",
      "Epoch 9/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 30.3362\n",
      "Epoch 00009: val_loss improved from 30.59297 to 30.07522, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 30.3341 - val_loss: 30.0752\n",
      "Epoch 10/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 29.3308\n",
      "Epoch 00010: val_loss improved from 30.07522 to 26.40808, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 29.3281 - val_loss: 26.4081\n",
      "Epoch 11/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 28.5629\n",
      "Epoch 00011: val_loss did not improve from 26.40808\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 28.5629 - val_loss: 30.1884\n",
      "Epoch 12/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 27.8745\n",
      "Epoch 00012: val_loss did not improve from 26.40808\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 27.8744 - val_loss: 26.6674\n",
      "Epoch 13/200\n",
      "6460/6477 [============================>.] - ETA: 0s - loss: 27.1495\n",
      "Epoch 00013: val_loss did not improve from 26.40808\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 27.1489 - val_loss: 30.1167\n",
      "Epoch 14/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 26.7172\n",
      "Epoch 00014: val_loss did not improve from 26.40808\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 26.7181 - val_loss: 27.4701\n",
      "Epoch 15/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 26.1538\n",
      "Epoch 00015: val_loss improved from 26.40808 to 23.93523, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 26.1530 - val_loss: 23.9352\n",
      "Epoch 16/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 25.6936\n",
      "Epoch 00016: val_loss did not improve from 23.93523\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 25.6941 - val_loss: 25.6221\n",
      "Epoch 17/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 25.1009\n",
      "Epoch 00017: val_loss did not improve from 23.93523\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 25.1026 - val_loss: 24.8174\n",
      "Epoch 18/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 24.5802\n",
      "Epoch 00018: val_loss did not improve from 23.93523\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.5801 - val_loss: 30.3971\n",
      "Epoch 19/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 24.3395\n",
      "Epoch 00019: val_loss improved from 23.93523 to 23.90337, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.3372 - val_loss: 23.9034\n",
      "Epoch 20/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 23.8146\n",
      "Epoch 00020: val_loss did not improve from 23.90337\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.8172 - val_loss: 24.4553\n",
      "Epoch 21/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 23.3820\n",
      "Epoch 00021: val_loss did not improve from 23.90337\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.3917 - val_loss: 32.5635\n",
      "Epoch 22/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 23.0746\n",
      "Epoch 00022: val_loss did not improve from 23.90337\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.0734 - val_loss: 25.1382\n",
      "Epoch 23/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 22.7471\n",
      "Epoch 00023: val_loss did not improve from 23.90337\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.7475 - val_loss: 31.9014\n",
      "Epoch 24/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 22.4626\n",
      "Epoch 00024: val_loss did not improve from 23.90337\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 22.4628 - val_loss: 25.7848\n",
      "Epoch 25/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 22.2407\n",
      "Epoch 00025: val_loss improved from 23.90337 to 20.34726, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.2392 - val_loss: 20.3473\n",
      "Epoch 26/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 21.8826\n",
      "Epoch 00026: val_loss improved from 20.34726 to 20.20627, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 21.8822 - val_loss: 20.2063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 21.5305\n",
      "Epoch 00027: val_loss did not improve from 20.20627\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.5467 - val_loss: 31.0986\n",
      "Epoch 28/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 21.3524\n",
      "Epoch 00028: val_loss improved from 20.20627 to 18.59582, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.3498 - val_loss: 18.5958\n",
      "Epoch 29/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 21.1956\n",
      "Epoch 00029: val_loss did not improve from 18.59582\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 21.1943 - val_loss: 19.5440\n",
      "Epoch 30/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 20.8600\n",
      "Epoch 00030: val_loss did not improve from 18.59582\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.8584 - val_loss: 20.8247\n",
      "Epoch 31/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 20.7498\n",
      "Epoch 00031: val_loss did not improve from 18.59582\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.7503 - val_loss: 24.1212\n",
      "Epoch 32/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 20.4802\n",
      "Epoch 00032: val_loss did not improve from 18.59582\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.4792 - val_loss: 19.9062\n",
      "Epoch 33/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 20.1861\n",
      "Epoch 00033: val_loss did not improve from 18.59582\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.1841 - val_loss: 19.1275\n",
      "Epoch 34/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 20.2309\n",
      "Epoch 00034: val_loss did not improve from 18.59582\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.2309 - val_loss: 20.7706\n",
      "Epoch 35/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 20.0033\n",
      "Epoch 00035: val_loss did not improve from 18.59582\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.0033 - val_loss: 26.9111\n",
      "Epoch 36/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 19.7256\n",
      "Epoch 00036: val_loss improved from 18.59582 to 17.47535, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.7252 - val_loss: 17.4753\n",
      "Epoch 37/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 19.7276\n",
      "Epoch 00037: val_loss did not improve from 17.47535\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.7255 - val_loss: 26.0972\n",
      "Epoch 38/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 19.3130\n",
      "Epoch 00038: val_loss did not improve from 17.47535\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.3130 - val_loss: 18.8221\n",
      "Epoch 39/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 18.9796\n",
      "Epoch 00039: val_loss did not improve from 17.47535\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.9800 - val_loss: 22.7311\n",
      "Epoch 40/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 18.9400\n",
      "Epoch 00040: val_loss did not improve from 17.47535\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.9391 - val_loss: 19.9392\n",
      "Epoch 41/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 18.8657\n",
      "Epoch 00041: val_loss did not improve from 17.47535\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.8673 - val_loss: 18.0413\n",
      "Epoch 42/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 18.9065\n",
      "Epoch 00042: val_loss did not improve from 17.47535\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.9069 - val_loss: 19.5902\n",
      "Epoch 43/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 18.5848\n",
      "Epoch 00043: val_loss did not improve from 17.47535\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.5864 - val_loss: 19.9304\n",
      "Epoch 44/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 18.3907\n",
      "Epoch 00044: val_loss improved from 17.47535 to 16.92105, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 18.3902 - val_loss: 16.9210\n",
      "Epoch 45/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 18.3328\n",
      "Epoch 00045: val_loss did not improve from 16.92105\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.3324 - val_loss: 18.1257\n",
      "Epoch 46/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 18.3634\n",
      "Epoch 00046: val_loss did not improve from 16.92105\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.3628 - val_loss: 21.0306\n",
      "Epoch 47/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 18.2321\n",
      "Epoch 00047: val_loss improved from 16.92105 to 16.66669, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.2324 - val_loss: 16.6667\n",
      "Epoch 48/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 17.8245\n",
      "Epoch 00048: val_loss did not improve from 16.66669\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.8262 - val_loss: 22.6069\n",
      "Epoch 49/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 17.9692\n",
      "Epoch 00049: val_loss did not improve from 16.66669\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 17.9694 - val_loss: 17.9357\n",
      "Epoch 50/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 17.8395\n",
      "Epoch 00050: val_loss did not improve from 16.66669\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.8420 - val_loss: 21.9229\n",
      "Epoch 51/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 17.7182\n",
      "Epoch 00051: val_loss did not improve from 16.66669\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.7179 - val_loss: 17.1615\n",
      "Epoch 52/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 17.5569\n",
      "Epoch 00052: val_loss improved from 16.66669 to 15.55540, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.5562 - val_loss: 15.5554\n",
      "Epoch 53/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 17.4324\n",
      "Epoch 00053: val_loss improved from 15.55540 to 15.16584, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 17.4275 - val_loss: 15.1658\n",
      "Epoch 54/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 17.4178\n",
      "Epoch 00054: val_loss did not improve from 15.16584\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.4172 - val_loss: 17.3564\n",
      "Epoch 55/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 17.4035\n",
      "Epoch 00055: val_loss did not improve from 15.16584\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.4034 - val_loss: 17.2744\n",
      "Epoch 56/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 17.2837\n",
      "Epoch 00056: val_loss did not improve from 15.16584\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.2883 - val_loss: 18.1537\n",
      "Epoch 57/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 17.0992\n",
      "Epoch 00057: val_loss did not improve from 15.16584\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.0984 - val_loss: 16.0187\n",
      "Epoch 58/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 17.1367\n",
      "Epoch 00058: val_loss did not improve from 15.16584\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.1393 - val_loss: 16.5332\n",
      "Epoch 59/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 17.0493\n",
      "Epoch 00059: val_loss improved from 15.16584 to 14.91361, saving model to ./experiment_set_12_1\\results_1\\split_1\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.0490 - val_loss: 14.9136\n",
      "Epoch 60/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 16.8403\n",
      "Epoch 00060: val_loss did not improve from 14.91361\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.8383 - val_loss: 20.1295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 16.7764\n",
      "Epoch 00061: val_loss did not improve from 14.91361\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.7767 - val_loss: 18.3173\n",
      "Epoch 62/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 16.8006\n",
      "Epoch 00062: val_loss did not improve from 14.91361\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.7960 - val_loss: 15.6114\n",
      "Epoch 63/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 16.6149\n",
      "Epoch 00063: val_loss did not improve from 14.91361\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.6126 - val_loss: 16.3546\n",
      "Epoch 64/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 16.7708\n",
      "Epoch 00064: val_loss did not improve from 14.91361\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.7708 - val_loss: 15.1624\n",
      "Epoch 65/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 16.6127\n",
      "Epoch 00065: val_loss did not improve from 14.91361\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.6149 - val_loss: 16.6635\n",
      "Epoch 66/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 16.5706\n",
      "Epoch 00066: val_loss did not improve from 14.91361\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.5689 - val_loss: 15.9731\n",
      "Epoch 67/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 16.3982\n",
      "Epoch 00067: val_loss did not improve from 14.91361\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.3961 - val_loss: 14.9686\n",
      "Epoch 68/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 16.4146\n",
      "Epoch 00068: val_loss did not improve from 14.91361\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.4147 - val_loss: 17.4456\n",
      "Epoch 69/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 16.3133\n",
      "Epoch 00069: val_loss did not improve from 14.91361\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.3133 - val_loss: 19.9247\n",
      "Epoch 00069: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training history to file: ./experiment_set_12_1\\results_1\\split_1\\history.pkl\n",
      "Test set:\n",
      "MSE: 14.95\n",
      "RMSE: 3.87\n",
      "CMAPSS score: 1.28\n",
      "\n",
      "Saved object to file: ./experiment_set_12_1\\results_1\\split_2\\scaler.pkl\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_85 (Dense)             (None, 256)               4864      \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 235,137\n",
      "Trainable params: 235,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 138.4517\n",
      "Epoch 00001: val_loss improved from inf to 61.76411, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 138.4184 - val_loss: 61.7641\n",
      "Epoch 2/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 48.6996\n",
      "Epoch 00002: val_loss improved from 61.76411 to 49.97570, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 48.6984 - val_loss: 49.9757\n",
      "Epoch 3/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 42.3179\n",
      "Epoch 00003: val_loss improved from 49.97570 to 37.54865, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 42.3142 - val_loss: 37.5487\n",
      "Epoch 4/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 38.6243\n",
      "Epoch 00004: val_loss improved from 37.54865 to 34.51751, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 38.6238 - val_loss: 34.5175\n",
      "Epoch 5/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 36.1408\n",
      "Epoch 00005: val_loss did not improve from 34.51751\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 36.1392 - val_loss: 35.0897\n",
      "Epoch 6/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 34.2990\n",
      "Epoch 00006: val_loss did not improve from 34.51751\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 34.3006 - val_loss: 36.1538\n",
      "Epoch 7/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 32.9691\n",
      "Epoch 00007: val_loss improved from 34.51751 to 31.62424, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 32.9709 - val_loss: 31.6242\n",
      "Epoch 8/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 31.7254\n",
      "Epoch 00008: val_loss did not improve from 31.62424\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 31.7209 - val_loss: 33.3237\n",
      "Epoch 9/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 30.5249\n",
      "Epoch 00009: val_loss improved from 31.62424 to 29.84570, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 30.5252 - val_loss: 29.8457\n",
      "Epoch 10/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 29.6526\n",
      "Epoch 00010: val_loss did not improve from 29.84570\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 29.6537 - val_loss: 36.3663\n",
      "Epoch 11/200\n",
      "6474/6477 [============================>.] - ETA: 0s - loss: 28.9761\n",
      "Epoch 00011: val_loss improved from 29.84570 to 28.60419, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 28.9774 - val_loss: 28.6042\n",
      "Epoch 12/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 28.2270\n",
      "Epoch 00012: val_loss did not improve from 28.60419\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 28.2276 - val_loss: 30.5599\n",
      "Epoch 13/200\n",
      "6462/6477 [============================>.] - ETA: 0s - loss: 27.6166\n",
      "Epoch 00013: val_loss improved from 28.60419 to 27.45324, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 27.6128 - val_loss: 27.4532\n",
      "Epoch 14/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 26.8084\n",
      "Epoch 00014: val_loss improved from 27.45324 to 24.95335, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 26.8066 - val_loss: 24.9534\n",
      "Epoch 15/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 26.2397\n",
      "Epoch 00015: val_loss improved from 24.95335 to 23.62657, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 26.2379 - val_loss: 23.6266\n",
      "Epoch 16/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 25.9331\n",
      "Epoch 00016: val_loss did not improve from 23.62657\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 25.9333 - val_loss: 24.3430\n",
      "Epoch 17/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 25.2432\n",
      "Epoch 00017: val_loss did not improve from 23.62657\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 25.2431 - val_loss: 24.8125\n",
      "Epoch 18/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 24.6711\n",
      "Epoch 00018: val_loss improved from 23.62657 to 22.96579, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.6698 - val_loss: 22.9658\n",
      "Epoch 19/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 24.5987\n",
      "Epoch 00019: val_loss did not improve from 22.96579\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 24.5981 - val_loss: 26.8205\n",
      "Epoch 20/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 23.9154\n",
      "Epoch 00020: val_loss improved from 22.96579 to 22.80000, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.9136 - val_loss: 22.8000\n",
      "Epoch 21/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 23.7070\n",
      "Epoch 00021: val_loss improved from 22.80000 to 21.59955, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.7060 - val_loss: 21.5995\n",
      "Epoch 22/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 23.2946\n",
      "Epoch 00022: val_loss did not improve from 21.59955\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 23.2964 - val_loss: 24.6264\n",
      "Epoch 23/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 22.8682\n",
      "Epoch 00023: val_loss did not improve from 21.59955\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.8688 - val_loss: 21.7925\n",
      "Epoch 24/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 22.5294\n",
      "Epoch 00024: val_loss did not improve from 21.59955\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 22.5301 - val_loss: 23.8599\n",
      "Epoch 25/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 22.3649\n",
      "Epoch 00025: val_loss improved from 21.59955 to 21.02999, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 22.3625 - val_loss: 21.0300\n",
      "Epoch 26/200\n",
      "6466/6477 [============================>.] - ETA: 0s - loss: 21.9767\n",
      "Epoch 00026: val_loss did not improve from 21.02999\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.9731 - val_loss: 22.7073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 21.7824\n",
      "Epoch 00027: val_loss did not improve from 21.02999\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 21.7823 - val_loss: 21.8719\n",
      "Epoch 28/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 21.3933\n",
      "Epoch 00028: val_loss did not improve from 21.02999\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 21.3944 - val_loss: 22.4624\n",
      "Epoch 29/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 21.1560\n",
      "Epoch 00029: val_loss improved from 21.02999 to 18.51604, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.1555 - val_loss: 18.5160\n",
      "Epoch 30/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 21.1281\n",
      "Epoch 00030: val_loss did not improve from 18.51604\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 21.1403 - val_loss: 28.6914\n",
      "Epoch 31/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 20.9880\n",
      "Epoch 00031: val_loss did not improve from 18.51604\n",
      "6477/6477 [==============================] - 24s 4ms/step - loss: 20.9921 - val_loss: 21.7394\n",
      "Epoch 32/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 20.5028\n",
      "Epoch 00032: val_loss did not improve from 18.51604\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.5028 - val_loss: 22.3602\n",
      "Epoch 33/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 20.4295\n",
      "Epoch 00033: val_loss did not improve from 18.51604\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 20.4234 - val_loss: 18.8236\n",
      "Epoch 34/200\n",
      "6461/6477 [============================>.] - ETA: 0s - loss: 19.9802\n",
      "Epoch 00034: val_loss did not improve from 18.51604\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 19.9783 - val_loss: 20.5638\n",
      "Epoch 35/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 19.9506\n",
      "Epoch 00035: val_loss did not improve from 18.51604\n",
      "6477/6477 [==============================] - 27s 4ms/step - loss: 19.9570 - val_loss: 20.4185\n",
      "Epoch 36/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 19.7540\n",
      "Epoch 00036: val_loss improved from 18.51604 to 17.77511, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 19.7547 - val_loss: 17.7751\n",
      "Epoch 37/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 19.6753\n",
      "Epoch 00037: val_loss did not improve from 17.77511\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.6742 - val_loss: 21.9791\n",
      "Epoch 38/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 19.4613\n",
      "Epoch 00038: val_loss did not improve from 17.77511\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.4612 - val_loss: 21.1080\n",
      "Epoch 39/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 19.4073\n",
      "Epoch 00039: val_loss did not improve from 17.77511\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.4081 - val_loss: 21.8265\n",
      "Epoch 40/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 19.2853\n",
      "Epoch 00040: val_loss did not improve from 17.77511\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.2853 - val_loss: 18.0395\n",
      "Epoch 41/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 19.0050\n",
      "Epoch 00041: val_loss did not improve from 17.77511\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 19.0137 - val_loss: 31.1192\n",
      "Epoch 42/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 18.9592\n",
      "Epoch 00042: val_loss did not improve from 17.77511\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.9592 - val_loss: 27.0017\n",
      "Epoch 43/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 18.7362\n",
      "Epoch 00043: val_loss did not improve from 17.77511\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.7362 - val_loss: 20.6885\n",
      "Epoch 44/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 18.7891\n",
      "Epoch 00044: val_loss improved from 17.77511 to 16.39852, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 18.7854 - val_loss: 16.3985\n",
      "Epoch 45/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 18.4817\n",
      "Epoch 00045: val_loss improved from 16.39852 to 16.39623, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.4792 - val_loss: 16.3962\n",
      "Epoch 46/200\n",
      "6468/6477 [============================>.] - ETA: 0s - loss: 18.4695\n",
      "Epoch 00046: val_loss did not improve from 16.39623\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.4720 - val_loss: 21.3315\n",
      "Epoch 47/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 18.3720\n",
      "Epoch 00047: val_loss did not improve from 16.39623\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.3722 - val_loss: 19.6975\n",
      "Epoch 48/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 18.0074\n",
      "Epoch 00048: val_loss did not improve from 16.39623\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.0071 - val_loss: 17.1371\n",
      "Epoch 49/200\n",
      "6473/6477 [============================>.] - ETA: 0s - loss: 18.0394\n",
      "Epoch 00049: val_loss did not improve from 16.39623\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.0383 - val_loss: 21.5730\n",
      "Epoch 50/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 17.9472\n",
      "Epoch 00050: val_loss did not improve from 16.39623\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.9520 - val_loss: 17.1238\n",
      "Epoch 51/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 17.9933\n",
      "Epoch 00051: val_loss improved from 16.39623 to 15.18061, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.9901 - val_loss: 15.1806\n",
      "Epoch 52/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 18.0258\n",
      "Epoch 00052: val_loss did not improve from 15.18061\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 18.0254 - val_loss: 26.8567\n",
      "Epoch 53/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 17.7260\n",
      "Epoch 00053: val_loss did not improve from 15.18061\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.7268 - val_loss: 16.4013\n",
      "Epoch 54/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 17.4395\n",
      "Epoch 00054: val_loss did not improve from 15.18061\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.4381 - val_loss: 16.9356\n",
      "Epoch 55/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 17.5469\n",
      "Epoch 00055: val_loss did not improve from 15.18061\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.5466 - val_loss: 16.5406\n",
      "Epoch 56/200\n",
      "6464/6477 [============================>.] - ETA: 0s - loss: 17.2724\n",
      "Epoch 00056: val_loss did not improve from 15.18061\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.2715 - val_loss: 18.7796\n",
      "Epoch 57/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 17.3720\n",
      "Epoch 00057: val_loss did not improve from 15.18061\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.3718 - val_loss: 19.0583\n",
      "Epoch 58/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 17.0421\n",
      "Epoch 00058: val_loss did not improve from 15.18061\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.0442 - val_loss: 16.6040\n",
      "Epoch 59/200\n",
      "6472/6477 [============================>.] - ETA: 0s - loss: 17.0628\n",
      "Epoch 00059: val_loss did not improve from 15.18061\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 17.0632 - val_loss: 15.6893\n",
      "Epoch 60/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 16.9545\n",
      "Epoch 00060: val_loss improved from 15.18061 to 15.09325, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.9551 - val_loss: 15.0932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 16.9646\n",
      "Epoch 00061: val_loss improved from 15.09325 to 14.71774, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 26s 4ms/step - loss: 16.9608 - val_loss: 14.7177\n",
      "Epoch 62/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 16.7106\n",
      "Epoch 00062: val_loss did not improve from 14.71774\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.7106 - val_loss: 16.3046\n",
      "Epoch 63/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 16.6568\n",
      "Epoch 00063: val_loss did not improve from 14.71774\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.6560 - val_loss: 16.9355\n",
      "Epoch 64/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 16.8021\n",
      "Epoch 00064: val_loss did not improve from 14.71774\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.8019 - val_loss: 15.5794\n",
      "Epoch 65/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 16.5612\n",
      "Epoch 00065: val_loss did not improve from 14.71774\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.5571 - val_loss: 19.0184\n",
      "Epoch 66/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 16.5700\n",
      "Epoch 00066: val_loss did not improve from 14.71774\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.5719 - val_loss: 18.8805\n",
      "Epoch 67/200\n",
      "6471/6477 [============================>.] - ETA: 0s - loss: 16.4882\n",
      "Epoch 00067: val_loss improved from 14.71774 to 14.62319, saving model to ./experiment_set_12_1\\results_1\\split_2\\mlp_model_trained.h5\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.4886 - val_loss: 14.6232\n",
      "Epoch 68/200\n",
      "6467/6477 [============================>.] - ETA: 0s - loss: 16.3499\n",
      "Epoch 00068: val_loss did not improve from 14.62319\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.3522 - val_loss: 17.9379\n",
      "Epoch 69/200\n",
      "6475/6477 [============================>.] - ETA: 0s - loss: 16.4028\n",
      "Epoch 00069: val_loss did not improve from 14.62319\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.4029 - val_loss: 15.2347\n",
      "Epoch 70/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 16.2809\n",
      "Epoch 00070: val_loss did not improve from 14.62319\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.2781 - val_loss: 14.7974\n",
      "Epoch 71/200\n",
      "6463/6477 [============================>.] - ETA: 0s - loss: 16.3376\n",
      "Epoch 00071: val_loss did not improve from 14.62319\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.3364 - val_loss: 15.0740\n",
      "Epoch 72/200\n",
      "6477/6477 [==============================] - ETA: 0s - loss: 16.0842\n",
      "Epoch 00072: val_loss did not improve from 14.62319\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.0842 - val_loss: 16.1771\n",
      "Epoch 73/200\n",
      "6470/6477 [============================>.] - ETA: 0s - loss: 15.9856\n",
      "Epoch 00073: val_loss did not improve from 14.62319\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.9871 - val_loss: 19.0856\n",
      "Epoch 74/200\n",
      "6465/6477 [============================>.] - ETA: 0s - loss: 16.1111\n",
      "Epoch 00074: val_loss did not improve from 14.62319\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 16.1119 - val_loss: 16.4140\n",
      "Epoch 75/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 15.9639\n",
      "Epoch 00075: val_loss did not improve from 14.62319\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.9661 - val_loss: 20.4511\n",
      "Epoch 76/200\n",
      "6476/6477 [============================>.] - ETA: 0s - loss: 15.9330\n",
      "Epoch 00076: val_loss did not improve from 14.62319\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.9329 - val_loss: 15.4643\n",
      "Epoch 77/200\n",
      "6469/6477 [============================>.] - ETA: 0s - loss: 15.8523\n",
      "Epoch 00077: val_loss did not improve from 14.62319\n",
      "6477/6477 [==============================] - 25s 4ms/step - loss: 15.8528 - val_loss: 19.2889\n",
      "Epoch 00077: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training history to file: ./experiment_set_12_1\\results_1\\split_2\\history.pkl\n",
      "Test set:\n",
      "MSE: 14.62\n",
      "RMSE: 3.82\n",
      "CMAPSS score: 1.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# Test effect of adding virtual sensors\n",
    "#######################################\n",
    "NUM_TRIALS = 3\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "layer_sizes = [256, 256, 512, 64]\n",
    "\n",
    "output_path = './experiment_set_12_1'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "results_file = os.path.join(output_path, \"results_mi_ranking.csv\")\n",
    "with open(results_file, \"w\") as file:\n",
    "    file.write(\"selected_features,num_features,mse,rmse,cmapss,mse(mean),mse(std),rmse(mean),rmse(std),cmapss(mean),cmapss(std)\\n\")\n",
    "\n",
    "\n",
    "for n in range(2):\n",
    "    if n == 0:\n",
    "        selected_columns = columns_operating_conditions + columns_sensor_measurements + columns_virtual_sensors\n",
    "    else:\n",
    "        selected_columns = columns_operating_conditions + columns_sensor_measurements\n",
    "    x_train_feature_selection = x_train[selected_columns]\n",
    "    \n",
    "    mse_vals = []\n",
    "    rmse_vals = []\n",
    "    cmapss_vals = []\n",
    "    \n",
    "    for random_seed in range(NUM_TRIALS):\n",
    "        # Train-validation split for early stopping\n",
    "        x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train_feature_selection, \n",
    "                                                                                  y_train, \n",
    "                                                                                  test_size=0.1, \n",
    "                                                                                  random_state=random_seed)\n",
    "        # Create output path\n",
    "        results_folder =\"results_{}\".format(n)\n",
    "        results_path_crr_num = os.path.join(output_path, results_folder)\n",
    "        results_path_crr_split = os.path.join(results_path_crr_num, \"split_{}\".format(random_seed))\n",
    "        if not os.path.exists(results_path_crr_split):\n",
    "            os.makedirs(results_path_crr_split)\n",
    "\n",
    "        # Standardization\n",
    "        scaler_file = os.path.join(results_path_crr_split, 'scaler.pkl')\n",
    "        scaler = StandardScaler()\n",
    "        x_train_scaled = scaler.fit_transform(x_train_split)\n",
    "        x_val_scaled = scaler.transform(x_val_split)\n",
    "        input_dim = x_train_scaled.shape[1]\n",
    "        save_object(scaler, scaler_file)\n",
    "\n",
    "        # Create model\n",
    "        weights_file = os.path.join(results_path_crr_num, 'mlp_initial_weights.h5')\n",
    "        model_path = os.path.join(results_path_crr_split, 'mlp_model_trained.h5')\n",
    "        \n",
    "        # Save initial weights\n",
    "        if random_seed == 0:\n",
    "            model = create_mlp_model(input_dim, layer_sizes, activation='tanh',\n",
    "                                     output_weights_file=weights_file)\n",
    "        else:\n",
    "            model = create_mlp_model(input_dim, layer_sizes, activation='tanh')\n",
    "        model.summary()\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint(model_path, monitor='val_loss', mode='min', verbose=2, \n",
    "                             save_best_only=True)\n",
    "\n",
    "        # Train model\n",
    "        history = train_model_existing_weights(model, weights_file, \n",
    "                                               x_train_scaled, y_train_split, \n",
    "                                               x_val_scaled, y_val_split, \n",
    "                                               batch_size=batch_size, \n",
    "                                               epochs=epochs, \n",
    "                                               callbacks=[es, mc])\n",
    "\n",
    "        history_file = os.path.join(results_path_crr_split, \"history.pkl\")\n",
    "        plot_loss_curves(history.history)\n",
    "        save_history(history, history_file)\n",
    "\n",
    "        # Performance evaluation\n",
    "        x_holdout_feature_selection = x_holdout[selected_columns]\n",
    "        x_holdout_scaled = scaler.transform(x_holdout_feature_selection)\n",
    "\n",
    "        loaded_model = load_model(model_path)\n",
    "        predictions_holdout = loaded_model.predict(x_holdout_scaled).flatten()\n",
    "        mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_holdout, y_holdout)\n",
    "        \n",
    "        mse_vals.append(mse)\n",
    "        rmse_vals.append(rmse)\n",
    "        cmapss_vals.append(cmapss_score)\n",
    "    \n",
    "    mse_mean = np.mean(mse_vals)\n",
    "    mse_std = np.std(mse_vals)\n",
    "    rmse_mean = np.mean(rmse_vals)\n",
    "    rmse_std = np.std(rmse_vals)\n",
    "    cmapss_mean = np.mean(cmapss_vals)\n",
    "    cmapss_std = np.std(cmapss_vals)\n",
    "    \n",
    "    with open(results_file, \"a\") as file:\n",
    "        file.write(f\"{feature_list_to_string(selected_columns)}, {len(selected_columns)}, {numbers_list_to_string(mse_vals)}, {numbers_list_to_string(rmse_vals)}, {numbers_list_to_string(cmapss_vals)}, {mse_mean}, {mse_std}, {rmse_mean}, {rmse_std}, {cmapss_mean}, {cmapss_std}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "MSE: 49.07\n",
      "RMSE: 7.01\n",
      "CMAPSS score: 1.67\n",
      "\n",
      "Test set:\n",
      "MSE: 47.81\n",
      "RMSE: 6.91\n",
      "CMAPSS score: 1.64\n",
      "\n",
      "Test set:\n",
      "MSE: 44.60\n",
      "RMSE: 6.68\n",
      "CMAPSS score: 1.62\n",
      "\n",
      "Test set:\n",
      "MSE: 65.02\n",
      "RMSE: 8.06\n",
      "CMAPSS score: 1.82\n",
      "\n",
      "Test set:\n",
      "MSE: 69.64\n",
      "RMSE: 8.35\n",
      "CMAPSS score: 1.86\n",
      "\n",
      "Test set:\n",
      "MSE: 65.78\n",
      "RMSE: 8.11\n",
      "CMAPSS score: 1.81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# Results on test set for mutual info\n",
    "#####################################\n",
    "NUM_TRIALS = 3\n",
    "\n",
    "\n",
    "output_path = './experiment_set_12_1'\n",
    "results_file = os.path.join(output_path, \"results_mi_ranking_test_set.csv\")\n",
    "with open(results_file, \"w\") as file:\n",
    "    file.write(\"selected_features,num_features,mse,rmse,cmapss,mse(mean),mse(std),rmse(mean),rmse(std),cmapss(mean),cmapss(std)\\n\")\n",
    "\n",
    "\n",
    "for n in range(2):\n",
    "    if n == 0:\n",
    "        selected_columns = columns_operating_conditions + columns_sensor_measurements + columns_virtual_sensors\n",
    "    else:\n",
    "        selected_columns = columns_operating_conditions + columns_sensor_measurements\n",
    "    \n",
    "    mse_vals = []\n",
    "    rmse_vals = []\n",
    "    cmapss_vals = []\n",
    "    \n",
    "    for random_seed in range(NUM_TRIALS):\n",
    "        results_folder =\"results_{}\".format(n)\n",
    "        results_path_crr_th = os.path.join(output_path, results_folder)\n",
    "        results_path_crr_split = os.path.join(results_path_crr_th, \"split_{}\".format(random_seed))\n",
    "        \n",
    "        scaler_file = os.path.join(results_path_crr_split, 'scaler.pkl')\n",
    "        scaler = load_object(scaler_file)\n",
    "\n",
    "        model_path = os.path.join(results_path_crr_split, 'mlp_model_trained.h5')\n",
    "        \n",
    "        # Performance evaluation\n",
    "        x_test_feature_selection = x_test[selected_columns]\n",
    "        x_test_scaled = scaler.transform(x_test_feature_selection)\n",
    "\n",
    "        loaded_model = load_model(model_path)\n",
    "        predictions_test = loaded_model.predict(x_test_scaled).flatten()\n",
    "        mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)\n",
    "        \n",
    "        mse_vals.append(mse)\n",
    "        rmse_vals.append(rmse)\n",
    "        cmapss_vals.append(cmapss_score)\n",
    "    \n",
    "    mse_mean = np.mean(mse_vals)\n",
    "    mse_std = np.std(mse_vals)\n",
    "    rmse_mean = np.mean(rmse_vals)\n",
    "    rmse_std = np.std(rmse_vals)\n",
    "    cmapss_mean = np.mean(cmapss_vals)\n",
    "    cmapss_std = np.std(cmapss_vals)\n",
    "    \n",
    "    with open(results_file, \"a\") as file:\n",
    "        file.write(f\"{feature_list_to_string(selected_columns)}, {len(selected_columns)}, {numbers_list_to_string(mse_vals)}, {numbers_list_to_string(rmse_vals)}, {numbers_list_to_string(cmapss_vals)}, {mse_mean}, {mse_std}, {rmse_mean}, {rmse_std}, {cmapss_mean}, {cmapss_std}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved object to file: ./experiment_set_12_2\\scaler.pkl\n",
      "Epoch 1/200\n",
      "9244/9253 [============================>.] - ETA: 0s - loss: 113.4247\n",
      "Epoch 00001: val_loss improved from inf to 47.44125, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 113.3649 - val_loss: 47.4412\n",
      "Epoch 2/200\n",
      "9241/9253 [============================>.] - ETA: 0s - loss: 44.8193\n",
      "Epoch 00002: val_loss improved from 47.44125 to 41.57350, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 44.8219 - val_loss: 41.5735\n",
      "Epoch 3/200\n",
      "9250/9253 [============================>.] - ETA: 0s - loss: 38.1926\n",
      "Epoch 00003: val_loss improved from 41.57350 to 37.32309, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 38.1921 - val_loss: 37.3231\n",
      "Epoch 4/200\n",
      "9252/9253 [============================>.] - ETA: 0s - loss: 34.4822\n",
      "Epoch 00004: val_loss improved from 37.32309 to 35.94654, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 34.4823 - val_loss: 35.9465\n",
      "Epoch 5/200\n",
      "9249/9253 [============================>.] - ETA: 0s - loss: 32.1532\n",
      "Epoch 00005: val_loss improved from 35.94654 to 31.84000, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 32.1522 - val_loss: 31.8400\n",
      "Epoch 6/200\n",
      "9239/9253 [============================>.] - ETA: 0s - loss: 30.2374\n",
      "Epoch 00006: val_loss improved from 31.84000 to 31.17111, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 30.2380 - val_loss: 31.1711\n",
      "Epoch 7/200\n",
      "9242/9253 [============================>.] - ETA: 0s - loss: 28.7791\n",
      "Epoch 00007: val_loss did not improve from 31.17111\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 28.7789 - val_loss: 43.4912\n",
      "Epoch 8/200\n",
      "9241/9253 [============================>.] - ETA: 0s - loss: 27.5121\n",
      "Epoch 00008: val_loss did not improve from 31.17111\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 27.5141 - val_loss: 32.2135\n",
      "Epoch 9/200\n",
      "9243/9253 [============================>.] - ETA: 0s - loss: 26.4882\n",
      "Epoch 00009: val_loss improved from 31.17111 to 27.72216, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 26.4891 - val_loss: 27.7222\n",
      "Epoch 10/200\n",
      "9243/9253 [============================>.] - ETA: 0s - loss: 25.6158\n",
      "Epoch 00010: val_loss did not improve from 27.72216\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 25.6144 - val_loss: 28.0383\n",
      "Epoch 11/200\n",
      "9247/9253 [============================>.] - ETA: 0s - loss: 24.8802\n",
      "Epoch 00011: val_loss improved from 27.72216 to 26.28601, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 24.8801 - val_loss: 26.2860\n",
      "Epoch 12/200\n",
      "9247/9253 [============================>.] - ETA: 0s - loss: 24.4553\n",
      "Epoch 00012: val_loss did not improve from 26.28601\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 24.4543 - val_loss: 26.9120\n",
      "Epoch 13/200\n",
      "9241/9253 [============================>.] - ETA: 0s - loss: 23.6147\n",
      "Epoch 00013: val_loss improved from 26.28601 to 24.44132, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 23.6139 - val_loss: 24.4413\n",
      "Epoch 14/200\n",
      "9241/9253 [============================>.] - ETA: 0s - loss: 23.4406\n",
      "Epoch 00014: val_loss improved from 24.44132 to 22.45026, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 23.4408 - val_loss: 22.4503\n",
      "Epoch 15/200\n",
      "9248/9253 [============================>.] - ETA: 0s - loss: 22.6952\n",
      "Epoch 00015: val_loss did not improve from 22.45026\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 22.6934 - val_loss: 23.5616\n",
      "Epoch 16/200\n",
      "9246/9253 [============================>.] - ETA: 0s - loss: 22.1680\n",
      "Epoch 00016: val_loss improved from 22.45026 to 19.92382, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 36s 4ms/step - loss: 22.1656 - val_loss: 19.9238\n",
      "Epoch 17/200\n",
      "9253/9253 [==============================] - ETA: 0s - loss: 21.8387\n",
      "Epoch 00017: val_loss did not improve from 19.92382\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 21.8387 - val_loss: 19.9995\n",
      "Epoch 18/200\n",
      "9249/9253 [============================>.] - ETA: 0s - loss: 21.3991\n",
      "Epoch 00018: val_loss improved from 19.92382 to 19.51978, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 21.3986 - val_loss: 19.5198\n",
      "Epoch 19/200\n",
      "9243/9253 [============================>.] - ETA: 0s - loss: 21.1014\n",
      "Epoch 00019: val_loss did not improve from 19.51978\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 21.1005 - val_loss: 27.1994\n",
      "Epoch 20/200\n",
      "9245/9253 [============================>.] - ETA: 0s - loss: 20.7576\n",
      "Epoch 00020: val_loss did not improve from 19.51978\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 20.7548 - val_loss: 19.7409\n",
      "Epoch 21/200\n",
      "9250/9253 [============================>.] - ETA: 0s - loss: 20.4490\n",
      "Epoch 00021: val_loss improved from 19.51978 to 19.42801, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 20.4480 - val_loss: 19.4280\n",
      "Epoch 22/200\n",
      "9253/9253 [==============================] - ETA: 0s - loss: 20.0771\n",
      "Epoch 00022: val_loss did not improve from 19.42801\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 20.0771 - val_loss: 23.2073\n",
      "Epoch 23/200\n",
      "9253/9253 [==============================] - ETA: 0s - loss: 19.7899\n",
      "Epoch 00023: val_loss did not improve from 19.42801\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 19.7899 - val_loss: 30.6122\n",
      "Epoch 24/200\n",
      "9250/9253 [============================>.] - ETA: 0s - loss: 19.4862\n",
      "Epoch 00024: val_loss improved from 19.42801 to 19.10228, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 19.4855 - val_loss: 19.1023\n",
      "Epoch 25/200\n",
      "9243/9253 [============================>.] - ETA: 0s - loss: 19.3475\n",
      "Epoch 00025: val_loss did not improve from 19.10228\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 19.3460 - val_loss: 26.3475\n",
      "Epoch 26/200\n",
      "9248/9253 [============================>.] - ETA: 0s - loss: 19.1641\n",
      "Epoch 00026: val_loss did not improve from 19.10228\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 19.1646 - val_loss: 19.6101\n",
      "Epoch 27/200\n",
      "9242/9253 [============================>.] - ETA: 0s - loss: 18.7332\n",
      "Epoch 00027: val_loss improved from 19.10228 to 17.89070, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 18.7305 - val_loss: 17.8907\n",
      "Epoch 28/200\n",
      "9247/9253 [============================>.] - ETA: 0s - loss: 18.5303\n",
      "Epoch 00028: val_loss did not improve from 17.89070\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 18.5314 - val_loss: 25.0793\n",
      "Epoch 29/200\n",
      "9243/9253 [============================>.] - ETA: 0s - loss: 18.4220\n",
      "Epoch 00029: val_loss did not improve from 17.89070\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 18.4234 - val_loss: 18.8661\n",
      "Epoch 30/200\n",
      "9242/9253 [============================>.] - ETA: 0s - loss: 18.1842\n",
      "Epoch 00030: val_loss did not improve from 17.89070\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 18.1837 - val_loss: 18.6361\n",
      "Epoch 31/200\n",
      "9245/9253 [============================>.] - ETA: 0s - loss: 18.3428\n",
      "Epoch 00031: val_loss did not improve from 17.89070\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 18.3462 - val_loss: 24.6726\n",
      "Epoch 32/200\n",
      "9250/9253 [============================>.] - ETA: 0s - loss: 17.9074\n",
      "Epoch 00032: val_loss did not improve from 17.89070\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 17.9090 - val_loss: 21.0291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/200\n",
      "9240/9253 [============================>.] - ETA: 0s - loss: 17.7106\n",
      "Epoch 00033: val_loss improved from 17.89070 to 17.14805, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 17.7101 - val_loss: 17.1481\n",
      "Epoch 34/200\n",
      "9249/9253 [============================>.] - ETA: 0s - loss: 17.6368\n",
      "Epoch 00034: val_loss did not improve from 17.14805\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 17.6364 - val_loss: 17.4617\n",
      "Epoch 35/200\n",
      "9242/9253 [============================>.] - ETA: 0s - loss: 17.5594\n",
      "Epoch 00035: val_loss did not improve from 17.14805\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 17.5598 - val_loss: 18.2257\n",
      "Epoch 36/200\n",
      "9242/9253 [============================>.] - ETA: 0s - loss: 17.3221\n",
      "Epoch 00036: val_loss did not improve from 17.14805\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 17.3214 - val_loss: 18.1068\n",
      "Epoch 37/200\n",
      "9245/9253 [============================>.] - ETA: 0s - loss: 17.2008\n",
      "Epoch 00037: val_loss did not improve from 17.14805\n",
      "9253/9253 [==============================] - 36s 4ms/step - loss: 17.1999 - val_loss: 19.4812\n",
      "Epoch 38/200\n",
      "9240/9253 [============================>.] - ETA: 0s - loss: 16.9902\n",
      "Epoch 00038: val_loss did not improve from 17.14805\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 16.9907 - val_loss: 19.2035\n",
      "Epoch 39/200\n",
      "9253/9253 [==============================] - ETA: 0s - loss: 16.8937\n",
      "Epoch 00039: val_loss did not improve from 17.14805\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 16.8937 - val_loss: 18.9216\n",
      "Epoch 40/200\n",
      "9241/9253 [============================>.] - ETA: 0s - loss: 16.7935\n",
      "Epoch 00040: val_loss did not improve from 17.14805\n",
      "9253/9253 [==============================] - 38s 4ms/step - loss: 16.7952 - val_loss: 24.9765\n",
      "Epoch 41/200\n",
      "9252/9253 [============================>.] - ETA: 0s - loss: 16.6642\n",
      "Epoch 00041: val_loss did not improve from 17.14805\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 16.6641 - val_loss: 28.9505\n",
      "Epoch 42/200\n",
      "9251/9253 [============================>.] - ETA: 0s - loss: 16.4922\n",
      "Epoch 00042: val_loss improved from 17.14805 to 15.26836, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 16.4925 - val_loss: 15.2684\n",
      "Epoch 43/200\n",
      "9249/9253 [============================>.] - ETA: 0s - loss: 16.3881\n",
      "Epoch 00043: val_loss improved from 15.26836 to 14.32940, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 16.3884 - val_loss: 14.3294\n",
      "Epoch 44/200\n",
      "9247/9253 [============================>.] - ETA: 0s - loss: 16.2436\n",
      "Epoch 00044: val_loss improved from 14.32940 to 14.32768, saving model to ./experiment_set_12_2\\mlp_model_trained.h5\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 16.2420 - val_loss: 14.3277\n",
      "Epoch 45/200\n",
      "9245/9253 [============================>.] - ETA: 0s - loss: 16.1229\n",
      "Epoch 00045: val_loss did not improve from 14.32768\n",
      "9253/9253 [==============================] - 36s 4ms/step - loss: 16.1225 - val_loss: 18.9196\n",
      "Epoch 46/200\n",
      "9244/9253 [============================>.] - ETA: 0s - loss: 16.0834\n",
      "Epoch 00046: val_loss did not improve from 14.32768\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 16.0820 - val_loss: 16.5355\n",
      "Epoch 47/200\n",
      "9247/9253 [============================>.] - ETA: 0s - loss: 16.0476\n",
      "Epoch 00047: val_loss did not improve from 14.32768\n",
      "9253/9253 [==============================] - 39s 4ms/step - loss: 16.0470 - val_loss: 18.7099\n",
      "Epoch 48/200\n",
      "9247/9253 [============================>.] - ETA: 0s - loss: 16.0191\n",
      "Epoch 00048: val_loss did not improve from 14.32768\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 16.0186 - val_loss: 15.6392\n",
      "Epoch 49/200\n",
      "9246/9253 [============================>.] - ETA: 0s - loss: 15.7756\n",
      "Epoch 00049: val_loss did not improve from 14.32768\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 15.7752 - val_loss: 15.6964\n",
      "Epoch 50/200\n",
      "9238/9253 [============================>.] - ETA: 0s - loss: 15.6796\n",
      "Epoch 00050: val_loss did not improve from 14.32768\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 15.6807 - val_loss: 16.1027\n",
      "Epoch 51/200\n",
      "9244/9253 [============================>.] - ETA: 0s - loss: 15.6984\n",
      "Epoch 00051: val_loss did not improve from 14.32768\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 15.6987 - val_loss: 17.1839\n",
      "Epoch 52/200\n",
      "9250/9253 [============================>.] - ETA: 0s - loss: 15.6512\n",
      "Epoch 00052: val_loss did not improve from 14.32768\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 15.6512 - val_loss: 27.7544\n",
      "Epoch 53/200\n",
      "9240/9253 [============================>.] - ETA: 0s - loss: 15.6092\n",
      "Epoch 00053: val_loss did not improve from 14.32768\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 15.6152 - val_loss: 20.0077\n",
      "Epoch 54/200\n",
      "9238/9253 [============================>.] - ETA: 0s - loss: 15.4224\n",
      "Epoch 00054: val_loss did not improve from 14.32768\n",
      "9253/9253 [==============================] - 37s 4ms/step - loss: 15.4261 - val_loss: 17.9134\n",
      "Epoch 00054: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training history to file: ./experiment_set_12_2\\history.pkl\n",
      "Test set:\n",
      "MSE: 65.50\n",
      "RMSE: 8.09\n",
      "CMAPSS score: 1.81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Retrain on entire training set with W + Xs\n",
    "############################################\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "layer_sizes = [256, 256, 512, 64]\n",
    "\n",
    "# Feature selection\n",
    "selected_columns = columns_operating_conditions + columns_sensor_measurements\n",
    "x_train_feature_selection = x_train[selected_columns]\n",
    "\n",
    "# Train-validation split for early stopping\n",
    "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train_feature_selection, \n",
    "                                                                          y_train, \n",
    "                                                                          test_size=0.1, \n",
    "                                                                          random_state=seed)\n",
    "\n",
    "\n",
    "# Standardization\n",
    "scaler_file = os.path.join(output_path, 'scaler.pkl')\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_split)\n",
    "x_val_scaled = scaler.transform(x_val_split)\n",
    "input_dim = x_train_scaled.shape[1]\n",
    "save_object(scaler, scaler_file)\n",
    "\n",
    "# Create model\n",
    "weights_file = os.path.join(output_path, 'mlp_initial_weights.h5')\n",
    "model_path = os.path.join(output_path, 'mlp_model_trained.h5')\n",
    "        \n",
    "model = create_mlp_model(input_dim, layer_sizes, activation='tanh',\n",
    "                         output_weights_file=weights_file)\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint(model_path, monitor='val_loss', mode='min', verbose=2, \n",
    "                     save_best_only=True)\n",
    "\n",
    "# Train model\n",
    "history = train_model_existing_weights(model, weights_file, \n",
    "                                       x_train_scaled, y_train_split, \n",
    "                                       x_val_scaled, y_val_split, \n",
    "                                       batch_size=batch_size, \n",
    "                                       epochs=epochs, \n",
    "                                       callbacks=[es, mc])\n",
    "\n",
    "history_file = os.path.join(output_path, \"history.pkl\")\n",
    "plot_loss_curves(history.history)\n",
    "save_history(history, history_file)\n",
    "\n",
    "# Performance evaluation\n",
    "x_test_feature_selection = x_test[selected_columns]\n",
    "x_test_scaled = scaler.transform(x_test_feature_selection)\n",
    "\n",
    "loaded_model = load_model(model_path)\n",
    "predictions_test = loaded_model.predict(x_test_scaled).flatten()\n",
    "mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-keras-gpu",
   "language": "python",
   "name": "tf-keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
