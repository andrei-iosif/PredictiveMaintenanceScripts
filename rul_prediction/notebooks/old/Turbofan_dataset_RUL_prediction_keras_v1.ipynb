{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "seed = 42\n",
    "os.environ['PYTHONHASSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/turbofan_dataset/N-CMAPSS_DS02-006.h5'\n",
    "output_path = 'DS02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    \"\"\" Reads a dataset from a given .h5 file and compose (in memory) the train and test data. \n",
    "    Args:\n",
    "        filename(str): path to the .h5 file\n",
    "    Returns:\n",
    "        train_set(pd.DataFrame), test_set(pd.DataFrame)\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'r') as hdf:\n",
    "        # Development set\n",
    "        W_dev = np.array(hdf.get('W_dev'))             # W\n",
    "        X_s_dev = np.array(hdf.get('X_s_dev'))         # X_s\n",
    "        X_v_dev = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "        T_dev = np.array(hdf.get('T_dev'))             # T\n",
    "        Y_dev = np.array(hdf.get('Y_dev'))             # RUL  \n",
    "        A_dev = np.array(hdf.get('A_dev'))             # Auxiliary\n",
    "\n",
    "        # Test set\n",
    "        W_test = np.array(hdf.get('W_test'))           # W\n",
    "        X_s_test = np.array(hdf.get('X_s_test'))       # X_s\n",
    "        X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "        T_test = np.array(hdf.get('T_test'))           # T\n",
    "        Y_test = np.array(hdf.get('Y_test'))           # RUL  \n",
    "        A_test = np.array(hdf.get('A_test'))           # Auxiliary\n",
    "        \n",
    "        # Varnams\n",
    "        W_var = np.array(hdf.get('W_var'))\n",
    "        X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "        X_v_var = np.array(hdf.get('X_v_var')) \n",
    "        T_var = np.array(hdf.get('T_var'))\n",
    "        A_var = np.array(hdf.get('A_var'))\n",
    "        \n",
    "        columns = []\n",
    "        columns += list(np.array(A_var, dtype='U20'))\n",
    "        columns += list(np.array(T_var, dtype='U20'))\n",
    "        columns += list(np.array(X_s_var, dtype='U20'))\n",
    "        columns += list(np.array(X_v_var, dtype='U20'))\n",
    "        columns += list(np.array(W_var, dtype='U20'))\n",
    "        columns += ['RUL']\n",
    "        \n",
    "    train_set = np.concatenate((A_dev, T_dev, X_s_dev, X_v_dev, W_dev, Y_dev), axis=1)\n",
    "    test_set = np.concatenate((A_test, T_test, X_s_test, X_v_test, W_test, Y_test), axis=1)\n",
    "    \n",
    "    return pd.DataFrame(data=train_set, columns=columns), pd.DataFrame(data=test_set, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Operation time (sec):  3.421875\n",
      "\n",
      "Train set shape: (5263447, 47)\n",
      "Test set shape: (1253743, 47)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.process_time()  \n",
    "train_set, test_set = load_dataset(filename)\n",
    "print('')\n",
    "print(\"Operation time (sec): \" , (time.process_time() - start_time))\n",
    "print('')\n",
    "print (\"Train set shape: \" + str(train_set.shape))\n",
    "print (\"Test set shape: \" + str(test_set.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_cycle_info(df, compute_cycle_len=False):\n",
    "    unit_ids = np.unique(df['unit'])\n",
    "    print('Engine units in df: ', unit_ids)\n",
    "    for i in unit_ids:\n",
    "        num_cycles = len(np.unique(df.loc[df['unit'] == i, 'cycle']))\n",
    "        print('Unit: ', i, ' - Number of flight cycles: ', num_cycles)\n",
    "        \n",
    "    if compute_cycle_len:\n",
    "        cycle_ids = np.unique(df['cycle'])\n",
    "        print('Total number of cycles: ', len(cycle_ids))\n",
    "        min_len = np.inf\n",
    "        max_len = 0\n",
    "        for i in cycle_ids:\n",
    "            cycle_len = len(df.loc[df['cycle'] == i, 'cycle'])\n",
    "            if cycle_len < min_len:\n",
    "                min_len = cycle_len\n",
    "            elif cycle_len > max_len:\n",
    "                max_len = cycle_len\n",
    "        print('Min cycle length: ', min_len)\n",
    "        print('Max cycle length: ', max_len)\n",
    "    \n",
    "    return unit_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter constant and quasi-constant features\n",
    "def get_quasi_constant_features(dataset, variance_th=0.01, debug=True):\n",
    "    constant_filter = VarianceThreshold(threshold=variance_th)\n",
    "    constant_filter.fit(dataset)\n",
    "    constant_features = [col for col in dataset.columns \n",
    "                         if col not in dataset.columns[constant_filter.get_support()]]\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Number of non-constant features: \", len(dataset.columns[constant_filter.get_support()]))\n",
    "        \n",
    "        print(\"Number of quasi-constant features: \", len(constant_features))\n",
    "        print(\"Quasi-constant features: \")\n",
    "        for col in constant_features:\n",
    "            print(col)\n",
    "    return constant_features\n",
    "\n",
    "def get_non_correlated_features(dataset, corr_th=0.9, debug=True):\n",
    "    corr_mat = dataset.corr()\n",
    "    corr_mat = np.abs(corr_mat)\n",
    "    \n",
    "    N = corr_mat.shape[0]\n",
    "    columns = np.full((N,), True, dtype=bool)\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            if corr_mat.iloc[i, j] >= corr_th:\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    if debug:        \n",
    "        correlated_features = dataset.columns[~columns]\n",
    "        print(\"Number of correlated features: \", len(correlated_features))\n",
    "        print(\"Correlated features: \")\n",
    "        for col in correlated_features:\n",
    "            print(col)\n",
    "    \n",
    "    selected_columns = dataset.columns[columns]\n",
    "    return selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_ids = unit_cycle_info(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=0)\n",
    "\n",
    "train_idxs, val_idxs = next(gss.split(train_set, groups=train_set['unit']))\n",
    "\n",
    "train_split = train_set.iloc[train_idxs]\n",
    "val_split = train_set.iloc[val_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine units in df:  [ 2.  5. 16. 18.]\n",
      "Unit:  2.0  - Number of flight cycles:  75\n",
      "Unit:  5.0  - Number of flight cycles:  89\n",
      "Unit:  16.0  - Number of flight cycles:  63\n",
      "Unit:  18.0  - Number of flight cycles:  71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.,  5., 16., 18.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unit_cycle_info(train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine units in df:  [10. 20.]\n",
      "Unit:  10.0  - Number of flight cycles:  82\n",
      "Unit:  20.0  - Number of flight cycles:  66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([10., 20.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unit_cycle_info(val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_split = shuffle(train_split)\n",
    "val_split = shuffle(val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_split['RUL']\n",
    "x_train = train_split.drop(['RUL'], axis=1)\n",
    "\n",
    "y_val = val_split['RUL']\n",
    "x_val = val_split.drop(['RUL'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.tight_layout()\n",
    "y_train.plot(kind='hist', ax=ax1, density=1)\n",
    "ax1.set_title(\"Histogram of RUL for train data\")\n",
    "\n",
    "y_val.plot(kind='hist', ax=ax2, density=1)\n",
    "ax2.set_title(\"Histogram of RUL for validation data\")\n",
    "plt.savefig(os.path.join(output_path, 'hist_RUL_final.png'), format='png', dpi=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of RUL for test data')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEICAYAAACXo2mmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAe6UlEQVR4nO3df7xVdZ3v8dc7DH9mWFARSGAec7ApREKa0szqBmSeairhNmFmQ9xgHuM0994gm7TmwX1YM1n50CC8MUU/xB9MdioaM2fU6hFXMMlAIQ9IcYJBooQUBzz4uX+s79HFZp991j6sxdkn38/HYz3OXt9f67P2hvM567vWXksRgZmZWRmeM9ABmJnZnw4nFTMzK42TipmZlcZJxczMSuOkYmZmpXFSMTOz0jipWOkkrZd03kDHMZAkvVPSVkmPSTpzgGM5VtJ3Je2WdPNAxlIGSVskvXmg47D6nFSsKfX+Q0v6gKSf9KxHxBkRcWcf44yVFJKOqijUgfbPwLyIOCEi7qutTPv+eEo6v5V0taQhufqG73OT79+7gRcDL4yI9xzeboGk8yR1He44aaw7JX2ojLF6GT8knVrV+HYoJxX7k9QCyeplwPo+2rw6Ik4A3gBcBHywwlh+FRHdzXZsgffRBhknFStd/q9sSZMlrZG0R9IOSVenZnenn4+mv9ZfK+k5kj4h6deSHpG0TNLzc+POSnW7JP1DzXaulHSLpG9I2gN8IG37Z5IelbRd0rWShubGC0kfkfSQpD9K+kdJL0999ki6Kd++Zh/rxirpaEmPAUOAX0ja1Nf7FRGdwE+BCc2/241J+hTwSeCi9D5f2uh9zh0BXSrpN8C/14x3PPAD4KVpvMckvTSNOV/SpvT53CTpBanPMelz2ZU+i9WSXixpIXAOcG0a59pe9uH9uc/98pq6Xj9jST3/xn6Rxr9I0kmSvidpp6Q/pNejy3vHjYjw4qXwAmwB3lxT9gHgJ/XaAD8D3p9enwBMSa/HAgEclev3QaATOCW1/Vfg66luPPAY8HpgKNn00pO57VyZ1t9B9sfSscBZwBTgqLS9B4HLctsLoAM4ETgD2Afckbb/fOAB4OJe3odeY82NfWqD9/HpeuB0YDvwd0Xf53rvX4NtXQl8o+D73DPuMuB44Ng6450HdNWUXQasAkYDRwNfBm5IdR8GvgscR5ZszwJOTHV3Ah9qEHvP535uGvdqoDv3uRf5jE/Nrb8Q+MsUy/OAm4FbB/r/1Z/S4iMV649b01+Gj0p6FPhSg7ZPAqdKGh4Rj0XEqgZt3wdcHRGbI+IxYAEwI03BvBv4bkT8JCL2k/31XXvjup9FxK0R8VREPBER90bEqojojogtZL/o3lDT5zMRsSci1gPrgB+m7e8m+4u8t5PsjWIt6ueSHif7RXgnjd/HMhWJ/cqIeDwinig45oeByyOiKyL2kSWyd6cxnyT7ZX5qRBxIn8ueguO+G/heRNydxv0H4KmeyoKfMbn2uyJiRUTsjYg/AgsbtbfmOalYf7wjIob1LMBHGrS9FDgN2JCmPS5o0PalwK9z678m+wv0xalua09FROwFdtX035pfkXRamt74zzQl9n+A4TV9duReP1Fn/YR+xFrUxDT+RcDZZEcGPbqB59a0fy7ZL+jDVST2g97LAl4GfDv3h8aDwIE05teB24DlkrZJ+qyk2n1rFGv+c3+c3Ode8DMm1/44SV9O02l7yKZhhyl3kYQdHicVq1REPBQRM4EXAZ8Bbknz8vVuj72N7JdTjzFkv1x3kE0PPT33LelYsr9+D9pczfoiYAPQFhEnAh8H1P+9KRxrYZG5iWya8JO5qt+QTefkjePgZNBfRWJvdPvyenVbgWn5PzYi4piI+G1EPBkRn4qI8cBfABcAswpsB7LP/eSeFUnHcfDn3uxn/PfAK4CzU/tze4buIw4ryEnFKiXprySNiIingEdT8QFgJ9k0xim55jcAfydpnKQTyP7qvDGyq5ZuAd4u6S/SidhP0fcvgucBe4DHJJ0O/I+y9quPWPvjKmC2pJek9RuByySdrswksnMhy2v6HZ1OhPcsRf5PH27sO4AXKncRBbAYWCjpZQCSRkhqT6/fKOnP09HAHrKjrQO5sfL/BmrdAlwg6fXpc/80B//e6uszrh3/eWRHoI+mCwmuKLjPVpCTilVtKrBe2RVRXwRmRMR/pemrhcBP05TJFGAp2VTJ3cDDwH8BfwOQznn8Ddkv1e3AH4FHyE6u9+Z/Av89tb2e7Bd1WXqNtT8i4pfAXcD/SkXXA/9CdoJ7N9mJ88sj4t9quj5G9kuyZzm/6tgjYgNZYtqcPruXkn22HcAPJf2R7KT92anLS8iSwx6yabG7gG+kui+SnXv5g6Rr6mxrPTAX+BbZ5/4HIP8dmb4+4yuBr6U43wt8gewijt+lGGvfTztMivBDumzwSX9hP0o27fHwAIdjZomPVGzQkPT2dKL1eLJLin9JdumtmbUIJxUbTNrJTjJvA9rIptJ8qG3WQjz9ZWZmpfGRipmZleZZfbO44cOHx9ixYwc6DDOzQeXee+/9XUSMqFf3rE4qY8eOZc2aNQMdhpnZoCKp1y/hevrLzMxK46RiZmalcVIxM7PSOKmYmVlpKk0qkqZK2iipU9L8OvWSdE2qv1/SxFzdUmVPpVtX0+dGSWvTskXS2lQ+VtITubrFVe6bmZkdqrKrv9IdSa8D3kJ2A7jVkjoi4oFcs2lk34xuI7v53CKeuQndV4FryW6k97SIuCi3jc+R3Wyvx6aImFDqjpiZWWFVHqlMBjrT0+X2k91dtr2mTTuwLD1TYhXZw3JGAkTE3cDvextckoD3kt0t1czMWkCVSWUUBz89riuVNdumN+cAOyLioVzZOEn3SbpL0jn1OkmaLWmNpDU7d+4suCkzMyuiyqRS7wFKtTcaK9KmNzM5+ChlOzAmIs4EPgp8S9KJhwwesSQiJkXEpBEj6n4h1MzM+qnKb9R3kXsMKNmjYLf1o80hJB0FvAs4q6csIvaRHtgUEfdK2kT2bHR/Zd6sCWPnf3/Atr3lqrcN2LatHFUeqawG2tIjS4cCM8ieDJfXAcxKV4FNAXZHxPYCY78Z2BARTz8BLj2+dEh6fQrZyf/NZeyImZkVU9mRSkR0S5oH3AYMAZZGxHpJc1L9YmAlMB3oBPYCl/T0l3QDcB4wXFIXcEVEfCVVz+DQE/TnAp+W1E32/Os5EdHriX4zMytfpTeUjIiVZIkjX7Y49zrInj9dr+/MBuN+oE7ZCmBFf2Ptj4GaJvAUgZm1Kn+j3szMSuOkYmZmpXFSMTOz0jyrH9JlzfGlpmbWFx+pmJlZaZxUzMysNE4qZmZWGicVMzMrjZOKmZmVxknFzMxK46RiZmalcVIxM7PS+MuPg9BAfgnRzKwRH6mYmVlpnFTMzKw0TipmZlYaJxUzMyuNT9SbNeA7M5s1x0cqZmZWGicVMzMrjZOKmZmVptKkImmqpI2SOiXNr1MvSdek+vslTczVLZX0iKR1NX2ulPRbSWvTMj1XtyCNtVHSW6vcNzMzO1RlSUXSEOA6YBowHpgpaXxNs2lAW1pmA4tydV8FpvYy/OcjYkJaVqbtjQdmAGekfl9KMZiZ2RFS5ZHKZKAzIjZHxH5gOdBe06YdWBaZVcAwSSMBIuJu4PdNbK8dWB4R+yLiYaAzxWBmZkdIlUllFLA1t96VypptU8+8NF22VNJJzYwlabakNZLW7Ny5s8CmzMysqCqTiuqURT/a1FoEvByYAGwHPtfMWBGxJCImRcSkESNG9LEpMzNrRpVJpQs4Obc+GtjWjzYHiYgdEXEgIp4CrueZKa6mxzIzs3JVmVRWA22SxkkaSnYSvaOmTQcwK10FNgXYHRHbGw3ac84leSfQc3VYBzBD0tGSxpGd/L+njB0xM7NiKrtNS0R0S5oH3AYMAZZGxHpJc1L9YmAlMJ3spPpe4JKe/pJuAM4DhkvqAq6IiK8An5U0gWxqawvw4TTeekk3AQ8A3cDciDhQ1f6ZWfkG6rY4viVOeSq991e63HdlTdni3OsA5vbSd2Yv5e9vsL2FwMJ+BWtmZofN36g3M7PS+C7FNij4Ecpmg4OPVMzMrDROKmZmVhonFTMzK42TipmZlcZJxczMSuOkYmZmpXFSMTOz0jipmJlZaZxUzMysNP5GvVmL8l0EbDDykYqZmZXGScXMzErjpGJmZqVxUjEzs9I4qZiZWWmcVMzMrDROKmZmVhonFTMzK02lSUXSVEkbJXVKml+nXpKuSfX3S5qYq1sq6RFJ62r6/JOkDan9tyUNS+VjJT0haW1aFle5b2ZmdqjKkoqkIcB1wDRgPDBT0viaZtOAtrTMBhbl6r4KTK0z9O3AKyPiVcCvgAW5uk0RMSEtc0rZETMzK6zKI5XJQGdEbI6I/cByoL2mTTuwLDKrgGGSRgJExN3A72sHjYgfRkR3Wl0FjK5sD8zMrClVJpVRwNbcelcqa7ZNIx8EfpBbHyfpPkl3STqnmWDNzOzwVXlDSdUpi360qT+4dDnQDXwzFW0HxkTELklnAbdKOiMi9tT0m0021caYMWOKbMrMzAqq8kilCzg5tz4a2NaPNoeQdDFwAfC+iAiAiNgXEbvS63uBTcBptX0jYklETIqISSNGjGhid8zMrC9VJpXVQJukcZKGAjOAjpo2HcCsdBXYFGB3RGxvNKikqcDHgAsjYm+ufES6OABJp5Cd/N9c3u6YmVlfKpv+iohuSfOA24AhwNKIWC9pTqpfDKwEpgOdwF7gkp7+km4AzgOGS+oCroiIrwDXAkcDt0sCWJWu9DoX+LSkbuAAMCciDjnRb2Zm1an0IV0RsZIsceTLFudeBzC3l74zeyk/tZfyFcCKfgdrZmaHzd+oNzOz0jipmJlZaZxUzMysNE4qZmZWGicVMzMrjZOKmZmVxknFzMxK46RiZmalcVIxM7PSOKmYmVlpnFTMzKw0TipmZlaaQklF0iurDsTMzAa/okcqiyXdI+kjkoZVGZCZmQ1ehZJKRLweeB/ZUxrXSPqWpLdUGpmZmQ06hc+pRMRDwCfInrr4BuAaSRskvauq4MzMbHApek7lVZI+DzwInA+8PSL+LL3+fIXxmZnZIFL0yY/XAtcDH4+IJ3oKI2KbpE9UEpmZmQ06RZPKdOCJiDgAIOk5wDERsTcivl5ZdGZmNqgUPafyI+DY3PpxqczMzOxpRZPKMRHxWM9Ken1cNSGZmdlgVTSpPC5pYs+KpLOAJxq072k3VdJGSZ2S5tepl6RrUv39NdtYKukRSetq+rxA0u2SHko/T8rVLUhjbZT01oL7ZmZmJSmaVC4Dbpb0Y0k/Bm4E5jXqIGkIcB0wDRgPzJQ0vqbZNKAtLbOBRbm6rwJT6ww9H7gjItqAO9I6aewZwBmp35dSDGZmdoQUOlEfEaslnQ68AhCwISKe7KPbZKAzIjYDSFoOtAMP5Nq0A8siIoBVkoZJGhkR2yPibklj64zbDpyXXn8NuJPsuzPtwPKI2Ac8LKkzxfCzIvtoZmaHr5kbSr4GeBVwJtlRx6w+2o8CtubWu1JZs21qvTgitgOkny9qZixJsyWtkbRm586dfWzKzMyaUehIRdLXgZcDa4EDqTiAZY261SmLfrQpqtBYEbEEWAIwadKk/m7LzMzqKPo9lUnA+DRNVVQX2b3CeowGtvWjTa0dPVNkkkYCjxzGWGZmVqKi01/rgJc0OfZqoE3SOElDyU6id9S06QBmpavApgC7e6a2GugALk6vLwa+kyufIeloSePITv7f02TMZmZ2GIoeqQwHHpB0D7CvpzAiLuytQ0R0S5oH3AYMAZZGxHpJc1L9YmAl2bf1O4G9wCU9/SXdQHZCfrikLuCKiPgKcBVwk6RLgd8A70njrZd0E9mFAN3A3J47AJiZ2ZFRNKlc2Z/BI2IlWeLIly3OvQ5gbi99Z/ZSvgt4Uy91C4GF/YnVzMwOX9FLiu+S9DKgLSJ+JOk4sqMPMzOzpxW99f1fA7cAX05Fo4BbK4rJzMwGqaIn6ucCrwP2wNMP7HpRwx5mZvasUzSp7IuI/T0rko6i/98nMTOzP1FFk8pdkj4OHJueTX8z8N3qwjIzs8GoaFKZD+wEfgl8mOyKLj/x0czMDlL06q+nyB4nfH214ZiZ2WBW9N5fD1P/PlqnlB6RmZkNWs3c+6vHMWTfYn9B+eGYmdlgVuicSkTsyi2/jYgvAOdXG5qZmQ02Rae/JuZWn0N25PK8SiIyM7NBq+j01+dyr7uBLcB7S4/GzMwGtaJXf72x6kDMzGzwKzr99dFG9RFxdTnhmJnZYNbM1V+v4ZmHbL0duJuDnwlvZmbPcs08pGtiRPwRQNKVwM0R8aGqAjMzs8Gn6G1axgD7c+v7gbGlR2NmZoNa0SOVrwP3SPo22Tfr3wksqywqMzMblIpe/bVQ0g+Ac1LRJRFxX3VhmZnZYFR0+gvgOGBPRHwR6JI0rqKYzMxskCr6OOErgI8BC1LRc4FvVBWUmZkNTkWPVN4JXAg8DhAR2yhwmxZJUyVtlNQpaX6dekm6JtXfn78dTG99Jd0oaW1atkham8rHSnoiV7e44L6ZmVlJip6o3x8RISkAJB3fVwdJQ4DrgLcAXcBqSR0R8UCu2TSgLS1nA4uAsxv1jYiLctv4HLA7N96miJhQcJ/MzKxkRY9UbpL0ZWCYpL8GfkTfD+yaDHRGxOb0fPvlQHtNm3ZgWWRWpfFHFukrSWT3H7uh4D6YmVnF+jxSSb+8bwROB/YArwA+GRG399F1FAd/476L7GikrzajCvY9B9gREQ/lysZJui/F+YmI+HGd/ZkNzAYYM2ZMH7tgZmbN6DOppGmvWyPiLKCvRJKnesMVbFOk70wOPkrZDoyJiF2SzgJulXRGROw5aJCIJcASgEmTJh3yNEszM+u/otNfqyS9psmxu4CTc+ujgW0F2zTsK+ko4F1kR1AARMS+iNiVXt8LbAJOazJmMzM7DEWTyhvJEsumdJXWLyXd30ef1UCbpHGShgIzeOaGlD06gFnpKrApwO6I2F6g75uBDRHR1VMgaUQ6wY+kU8hO/m8uuH9mZlaChtNfksZExG/IrtJqSkR0S5oH3AYMAZZGxHpJc1L9YmAlMB3oBPYClzTqmxt+BoeeoD8X+LSkbuAAMCcift9s3GZm1n99nVO5lezuxL+WtCIi/rKZwSNiJVniyJctzr0OYG7Rvrm6D9QpWwGsaCY+MzMrV1/TX/kT5qdUGYiZmQ1+fSWV6OW1mZnZIfqa/nq1pD1kRyzHptek9YiIEyuNzszMBpWGSSUihhypQMzMbPBr5tb3ZmZmDTmpmJlZaZxUzMysNE4qZmZWGicVMzMrTdGHdJmZ/ckaO//7A7LdLVe9bUC2WyUfqZiZWWmcVMzMrDROKmZmVhonFTMzK42TipmZlcZJxczMSuOkYmZmpXFSMTOz0jipmJlZaZxUzMysNE4qZmZWmkqTiqSpkjZK6pQ0v069JF2T6u+XNLGvvpKulPRbSWvTMj1XtyC13yjprVXum5mZHaqyG0pKGgJcB7wF6AJWS+qIiAdyzaYBbWk5G1gEnF2g7+cj4p9rtjcemAGcAbwU+JGk0yLiQFX7aGZmB6vySGUy0BkRmyNiP7AcaK9p0w4si8wqYJikkQX71moHlkfEvoh4GOhM45iZ2RFSZVIZBWzNrXelsiJt+uo7L02XLZV0UhPbQ9JsSWskrdm5c2cz+2NmZn2oMqmoTlkUbNOo7yLg5cAEYDvwuSa2R0QsiYhJETFpxIgRdbqYmVl/VfmQri7g5Nz6aGBbwTZDe+sbETt6CiVdD3yvie2ZmVmFqjxSWQ20SRonaSjZSfSOmjYdwKx0FdgUYHdEbG/UN51z6fFOYF1urBmSjpY0juzk/z1V7ZyZmR2qsiOViOiWNA+4DRgCLI2I9ZLmpPrFwEpgOtlJ9b3AJY36pqE/K2kC2dTWFuDDqc96STcBDwDdwFxf+WVmdmRV+oz6iFhJljjyZYtzrwOYW7RvKn9/g+0tBBb2N14zMzs8/ka9mZmVxknFzMxK46RiZmalcVIxM7PSOKmYmVlpnFTMzKw0TipmZlYaJxUzMyuNk4qZmZXGScXMzErjpGJmZqVxUjEzs9I4qZiZWWmcVMzMrDROKmZmVhonFTMzK42TipmZlcZJxczMSuOkYmZmpXFSMTOz0jipmJlZaSpNKpKmStooqVPS/Dr1knRNqr9f0sS++kr6J0kbUvtvSxqWysdKekLS2rQsrnLfzMzsUJUlFUlDgOuAacB4YKak8TXNpgFtaZkNLCrQ93bglRHxKuBXwILceJsiYkJa5lSzZ2Zm1psqj1QmA50RsTki9gPLgfaaNu3AssisAoZJGtmob0T8MCK6U/9VwOgK98HMzJpQZVIZBWzNrXelsiJtivQF+CDwg9z6OEn3SbpL0jn1gpI0W9IaSWt27txZbE/MzKyQKpOK6pRFwTZ99pV0OdANfDMVbQfGRMSZwEeBb0k68ZBBIpZExKSImDRixIg+dsHMzJpxVIVjdwEn59ZHA9sKthnaqK+ki4ELgDdFRABExD5gX3p9r6RNwGnAmjJ2xszM+lblkcpqoE3SOElDgRlAR02bDmBWugpsCrA7IrY36itpKvAx4MKI2NszkKQR6QQ/kk4hO/m/ucL9MzOzGpUdqUREt6R5wG3AEGBpRKyXNCfVLwZWAtOBTmAvcEmjvmnoa4GjgdslAaxKV3qdC3xaUjdwAJgTEb+vav/MzOxQVU5/EREryRJHvmxx7nUAc4v2TeWn9tJ+BbDicOI1M7PD42/Um5lZaZxUzMysNE4qZmZWmkrPqZiZWe/Gzv/+gG17y1Vvq2RcH6mYmVlpnFTMzKw0TipmZlYaJxUzMyuNk4qZmZXGScXMzErjpGJmZqVxUjEzs9I4qZiZWWmcVMzMrDROKmZmVhonFTMzK42TipmZlcZJxczMSuOkYmZmpXFSMTOz0jipmJlZaSpNKpKmStooqVPS/Dr1knRNqr9f0sS++kp6gaTbJT2Ufp6Uq1uQ2m+U9NYq983MzA5VWVKRNAS4DpgGjAdmShpf02wa0JaW2cCiAn3nA3dERBtwR1on1c8AzgCmAl9K45iZ2RFS5ZHKZKAzIjZHxH5gOdBe06YdWBaZVcAwSSP76NsOfC29/hrwjlz58ojYFxEPA51pHDMzO0KOqnDsUcDW3HoXcHaBNqP66PviiNgOEBHbJb0oN9aqOmMdRNJssqMigMckbSy6Q3UMB353GP2r1urxQevH2OrxQevH2OrxwbMwRn3msLq/rLeKKpOK6pRFwTZF+vZne0TEEmBJH2MVImlNREwqY6wqtHp80Poxtnp80Poxtnp84BjLVOX0Vxdwcm59NLCtYJtGfXekKTLSz0ea2J6ZmVWoyqSyGmiTNE7SULKT6B01bTqAWekqsCnA7jS11ahvB3Bxen0x8J1c+QxJR0saR3by/56qds7MzA5V2fRXRHRLmgfcBgwBlkbEeklzUv1iYCUwneyk+l7gkkZ909BXATdJuhT4DfCe1Ge9pJuAB4BuYG5EHKhq/5JSptEq1OrxQevH2OrxQevH2OrxgWMsjSL6OlVhZmZWjL9Rb2ZmpXFSMTOz0jip9ENft58ZCJKWSnpE0rpcWa+3tBmA+E6W9B+SHpS0XtLftmCMx0i6R9IvUoyfarUYUzxDJN0n6XstGt8WSb+UtFbSmlaLUdIwSbdI2pD+Pb62xeJ7RXrvepY9ki5rpRgbcVJpUsHbzwyEr5Ldniav7i1tBkg38PcR8WfAFGBuet9aKcZ9wPkR8WpgAjA1XZXYSjEC/C3wYG691eIDeGNETMh9r6KVYvwi8G8RcTrwarL3smXii4iN6b2bAJxFdhHTt1spxoYiwksTC/Ba4Lbc+gJgwUDHlWIZC6zLrW8ERqbXI4GNAx1jLrbvAG9p1RiB44Cfk93JoWViJPv+1R3A+cD3WvFzBrYAw2vKWiJG4ETgYdJFSq0WX514/xvw01aOsXbxkUrzeru1TCs66JY2wIv6aH9ESBoLnAn8P1osxjS1tJbsS7W3R0SrxfgF4H8DT+XKWik+yO5k8UNJ96bbIkHrxHgKsBP4lzSF+H8lHd9C8dWaAdyQXrdqjAdxUmlef24hY4mkE4AVwGURsWeg46kVEQcim3YYDUyW9MoBDulpki4AHomIewc6lj68LiImkk0Rz5V07kAHlHMUMBFYFBFnAo/TotNI6YvfFwI3D3QszXBSad5guh1Mb7e0GRCSnkuWUL4ZEf+ailsqxh4R8ShwJ9l5qlaJ8XXAhZK2kN25+3xJ32ih+ACIiG3p5yNk5wIm0zoxdgFd6QgU4BayJNMq8eVNA34eETvSeivGeAgnleYVuf1Mq+jtljZHnCQBXwEejIirc1WtFOMIScPS62OBNwMbaJEYI2JBRIyOiLFk/+7+PSL+qlXiA5B0vKTn9bwmOyewjhaJMSL+E9gq6RWp6E1kd+FoifhqzOSZqS9ozRgPNdAndQbjQnZrmV8Bm4DLBzqeFNMNwHbgSbK/xi4FXkh2Uveh9PMFAxjf68mmCe8H1qZleovF+CrgvhTjOuCTqbxlYszFeh7PnKhvmfjIzln8Ii3re/5/tFiME4A16XO+FTipleJLMR4H7AKenytrqRh7W3ybFjMzK42nv8zMrDROKmZmVhonFTMzK42TipmZlcZJxczMSuOkYmZmpXFSMTOz0vx/YwmIcw30d0IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test.plot(kind='hist', density=1)\n",
    "plt.title(\"Histogram of RUL for test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-constant features:  34\n",
      "Number of quasi-constant features:  12\n",
      "Quasi-constant features: \n",
      "Fc\n",
      "fan_eff_mod\n",
      "fan_flow_mod\n",
      "LPC_eff_mod\n",
      "LPC_flow_mod\n",
      "HPC_eff_mod\n",
      "HPC_flow_mod\n",
      "HPT_eff_mod\n",
      "HPT_flow_mod\n",
      "LPT_eff_mod\n",
      "LPT_flow_mod\n",
      "Mach\n",
      "Train shape:  (3542576, 34)\n"
     ]
    }
   ],
   "source": [
    "constant_features = get_quasi_constant_features(x_train)\n",
    "x_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "x_val.drop(labels=constant_features, axis=1, inplace=True)\n",
    "print(\"Train shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correlated features:  26\n",
      "Correlated features: \n",
      "T48\n",
      "T50\n",
      "P2\n",
      "P21\n",
      "P24\n",
      "Ps30\n",
      "P40\n",
      "P50\n",
      "Nf\n",
      "Nc\n",
      "Wf\n",
      "T40\n",
      "P30\n",
      "P45\n",
      "W21\n",
      "W22\n",
      "W25\n",
      "W31\n",
      "W32\n",
      "W48\n",
      "W50\n",
      "SmHPC\n",
      "phi\n",
      "alt\n",
      "TRA\n",
      "T2\n",
      "Train shape:  (3542576, 8)\n"
     ]
    }
   ],
   "source": [
    "# Remove highly correlated features\n",
    "selected_columns = get_non_correlated_features(x_train)\n",
    "\n",
    "# sensors = ['Wf', 'Nf', 'Nc', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50']\n",
    "# scenario_descriptors = ['alt', 'TRA', 'T2']\n",
    "# selected_columns = sensors + scenario_descriptors\n",
    "\n",
    "x_train = x_train[selected_columns]\n",
    "x_val = x_val[selected_columns]\n",
    "print(\"Train shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop auxiliary data columns\n",
    "auxiliary_columns = ['cycle', 'hs', 'Fc']\n",
    "x_train.drop(labels=[x for x in auxiliary_columns if x in x_train.columns], axis=1, inplace=True)\n",
    "x_val.drop(labels=[x for x in auxiliary_columns if x in x_val.columns], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unit column\n",
    "x_train.drop(labels=['unit'], axis=1, inplace=True)\n",
    "x_val.drop(labels=['unit'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>P15</th>\n",
       "      <th>SmFan</th>\n",
       "      <th>SmLPC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182551</th>\n",
       "      <td>550.888980</td>\n",
       "      <td>1306.479774</td>\n",
       "      <td>9.622313</td>\n",
       "      <td>19.681756</td>\n",
       "      <td>8.384708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574647</th>\n",
       "      <td>561.303490</td>\n",
       "      <td>1325.186357</td>\n",
       "      <td>10.754276</td>\n",
       "      <td>19.751889</td>\n",
       "      <td>7.928827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649299</th>\n",
       "      <td>595.249862</td>\n",
       "      <td>1424.331179</td>\n",
       "      <td>15.149541</td>\n",
       "      <td>16.689345</td>\n",
       "      <td>9.464689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191956</th>\n",
       "      <td>551.963131</td>\n",
       "      <td>1320.353331</td>\n",
       "      <td>8.921765</td>\n",
       "      <td>18.452537</td>\n",
       "      <td>8.412098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595223</th>\n",
       "      <td>542.617587</td>\n",
       "      <td>1296.348414</td>\n",
       "      <td>8.096627</td>\n",
       "      <td>18.181070</td>\n",
       "      <td>7.602757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309041</th>\n",
       "      <td>549.595529</td>\n",
       "      <td>1314.793157</td>\n",
       "      <td>8.956166</td>\n",
       "      <td>18.669542</td>\n",
       "      <td>8.579318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4464277</th>\n",
       "      <td>541.695075</td>\n",
       "      <td>1299.578809</td>\n",
       "      <td>8.095847</td>\n",
       "      <td>18.381011</td>\n",
       "      <td>8.511837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3181795</th>\n",
       "      <td>578.083560</td>\n",
       "      <td>1385.723822</td>\n",
       "      <td>12.065196</td>\n",
       "      <td>18.075116</td>\n",
       "      <td>9.033319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3721018</th>\n",
       "      <td>564.360358</td>\n",
       "      <td>1351.631391</td>\n",
       "      <td>10.387135</td>\n",
       "      <td>18.454717</td>\n",
       "      <td>8.835110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3171821</th>\n",
       "      <td>544.705784</td>\n",
       "      <td>1300.438637</td>\n",
       "      <td>8.594127</td>\n",
       "      <td>19.012134</td>\n",
       "      <td>8.533609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3542576 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                T24          T30        P15      SmFan     SmLPC\n",
       "182551   550.888980  1306.479774   9.622313  19.681756  8.384708\n",
       "574647   561.303490  1325.186357  10.754276  19.751889  7.928827\n",
       "649299   595.249862  1424.331179  15.149541  16.689345  9.464689\n",
       "4191956  551.963131  1320.353331   8.921765  18.452537  8.412098\n",
       "3595223  542.617587  1296.348414   8.096627  18.181070  7.602757\n",
       "...             ...          ...        ...        ...       ...\n",
       "3309041  549.595529  1314.793157   8.956166  18.669542  8.579318\n",
       "4464277  541.695075  1299.578809   8.095847  18.381011  8.511837\n",
       "3181795  578.083560  1385.723822  12.065196  18.075116  9.033319\n",
       "3721018  564.360358  1351.631391  10.387135  18.454717  8.835110\n",
       "3171821  544.705784  1300.438637   8.594127  19.012134  8.533609\n",
       "\n",
       "[3542576 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_set['RUL']\n",
    "x_test = test_set.drop(['RUL'], axis=1)\n",
    "x_test = x_test[x_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def cmapss_score_function(actual, predictions):\n",
    "    diff = actual - predictions\n",
    "    alpha = np.full_like(diff, 1/10)\n",
    "    negative_diff_mask = diff < 0\n",
    "    alpha[negative_diff_mask] = 1/13\n",
    "    return np.sum(np.exp(alpha * np.abs(diff)))\n",
    "\n",
    "def evaluation(actual, predictions, label='Test'):\n",
    "    mse = mean_squared_error(actual, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    cmapss_score = cmapss_score_function(actual, predictions)\n",
    "    print('{} set:\\nMSE: {:.2f}\\nRMSE: {:.2f}\\nCMAPSS score: {:.2E}\\n'.format(label, mse, rmse, \n",
    "                                                                     Decimal(cmapss_score)))\n",
    "    \n",
    "def plot_loss_curves(history, output_path=''):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(output_path, 'loss_curves.png'), format='png', dpi=300) \n",
    "    \n",
    "def plot_rul(expected, predicted):\n",
    "    plt.plot(range(len(expected)), expected, label='Expected')\n",
    "    plt.plot(range(len(predicted)), predicted, label='Predicted')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(input_dim, hidden_layer_size, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_size, input_dim=input_dim, \n",
    "                    kernel_initializer='normal', activation=activation))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train_scaled.shape[1]\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint(os.path.join(output_path, 'mlp_model_baseline.h5'), monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "model = mlp_model(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train_scaled, y_train, validation_data=(x_val_scaled, y_val),\n",
    "                    epochs=200, batch_size=512, verbose=1, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [50, 100, 200, 300, 500]\n",
    "activation_functions = ['relu', 'tanh', 'sigmoid']\n",
    "batch_sizes = list(2**np.arange(9, 13))\n",
    "dropout_list = list(np.arange(0, 5) / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(input_dim, hidden_layer_size, dropout, weights_file, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_size, input_dim=input_dim, \n",
    "                    kernel_initializer='normal', activation=activation))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.save_weights(weights_file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iteration  1\n",
      "Layer size = 100, activation = relu, batch size = 4096, dropout = 0.3\n",
      "Epoch 1/50\n",
      "865/865 - 4s - loss: 818.4907 - val_loss: 398.3312\n",
      "Epoch 2/50\n",
      "865/865 - 3s - loss: 314.5118 - val_loss: 366.9552\n",
      "Epoch 3/50\n",
      "865/865 - 3s - loss: 285.8918 - val_loss: 356.8907\n",
      "Epoch 4/50\n",
      "865/865 - 3s - loss: 266.8344 - val_loss: 352.2576\n",
      "Epoch 5/50\n",
      "865/865 - 3s - loss: 249.4501 - val_loss: 343.9261\n",
      "Epoch 6/50\n",
      "865/865 - 3s - loss: 231.8464 - val_loss: 336.0140\n",
      "Epoch 7/50\n",
      "865/865 - 3s - loss: 214.7188 - val_loss: 331.4146\n",
      "Epoch 8/50\n",
      "865/865 - 3s - loss: 199.6379 - val_loss: 327.9305\n",
      "Epoch 9/50\n",
      "865/865 - 3s - loss: 186.9749 - val_loss: 325.3551\n",
      "Epoch 10/50\n",
      "865/865 - 3s - loss: 177.8873 - val_loss: 324.1916\n",
      "Epoch 11/50\n",
      "865/865 - 3s - loss: 171.5891 - val_loss: 322.8754\n",
      "Epoch 12/50\n",
      "865/865 - 3s - loss: 166.8181 - val_loss: 321.2634\n",
      "Epoch 13/50\n",
      "865/865 - 3s - loss: 163.0659 - val_loss: 319.0897\n",
      "Epoch 14/50\n",
      "865/865 - 3s - loss: 160.2722 - val_loss: 316.4202\n",
      "Epoch 15/50\n",
      "865/865 - 3s - loss: 157.7714 - val_loss: 314.1870\n",
      "Epoch 16/50\n",
      "865/865 - 4s - loss: 156.0444 - val_loss: 309.9568\n",
      "Epoch 17/50\n",
      "865/865 - 3s - loss: 154.4200 - val_loss: 308.8452\n",
      "Epoch 18/50\n",
      "865/865 - 3s - loss: 153.1131 - val_loss: 308.9088\n",
      "Epoch 19/50\n",
      "865/865 - 3s - loss: 152.1234 - val_loss: 307.8438\n",
      "Epoch 20/50\n",
      "865/865 - 3s - loss: 151.3152 - val_loss: 304.8785\n",
      "Epoch 21/50\n",
      "865/865 - 3s - loss: 150.5854 - val_loss: 303.7911\n",
      "Epoch 22/50\n",
      "865/865 - 3s - loss: 149.7548 - val_loss: 301.7032\n",
      "Epoch 23/50\n",
      "865/865 - 3s - loss: 148.9194 - val_loss: 301.4083\n",
      "Epoch 24/50\n",
      "865/865 - 3s - loss: 148.1274 - val_loss: 298.6971\n",
      "Epoch 25/50\n",
      "865/865 - 3s - loss: 147.5419 - val_loss: 297.0483\n",
      "Epoch 26/50\n",
      "865/865 - 3s - loss: 147.0204 - val_loss: 297.6126\n",
      "Epoch 27/50\n",
      "865/865 - 3s - loss: 146.4249 - val_loss: 295.3434\n",
      "Epoch 28/50\n",
      "865/865 - 3s - loss: 145.8771 - val_loss: 295.8482\n",
      "Epoch 29/50\n",
      "865/865 - 3s - loss: 145.2880 - val_loss: 293.5414\n",
      "Epoch 30/50\n",
      "865/865 - 3s - loss: 144.6930 - val_loss: 292.6897\n",
      "Epoch 31/50\n",
      "865/865 - 3s - loss: 144.5424 - val_loss: 292.7933\n",
      "Epoch 32/50\n",
      "865/865 - 3s - loss: 144.1739 - val_loss: 289.9221\n",
      "Epoch 33/50\n",
      "865/865 - 3s - loss: 143.9576 - val_loss: 290.6909\n",
      "Epoch 34/50\n",
      "865/865 - 3s - loss: 143.7105 - val_loss: 286.6270\n",
      "Epoch 35/50\n",
      "865/865 - 3s - loss: 143.2813 - val_loss: 285.1252\n",
      "Epoch 36/50\n",
      "865/865 - 4s - loss: 143.0374 - val_loss: 287.4075\n",
      "Epoch 37/50\n",
      "865/865 - 3s - loss: 142.9437 - val_loss: 285.9449\n",
      "Epoch 38/50\n",
      "865/865 - 3s - loss: 142.9578 - val_loss: 285.2313\n",
      "Epoch 39/50\n",
      "865/865 - 3s - loss: 142.4651 - val_loss: 285.5447\n",
      "Epoch 40/50\n",
      "865/865 - 3s - loss: 142.3776 - val_loss: 285.1874\n",
      "Epoch 41/50\n",
      "865/865 - 3s - loss: 142.1444 - val_loss: 284.7565\n",
      "Epoch 42/50\n",
      "865/865 - 3s - loss: 141.9538 - val_loss: 280.8084\n",
      "Epoch 43/50\n",
      "865/865 - 3s - loss: 141.8945 - val_loss: 280.9412\n",
      "Epoch 44/50\n",
      "865/865 - 3s - loss: 141.7540 - val_loss: 280.6182\n",
      "Epoch 45/50\n",
      "865/865 - 3s - loss: 141.6056 - val_loss: 279.4909\n",
      "Epoch 46/50\n",
      "865/865 - 3s - loss: 141.5210 - val_loss: 280.5826\n",
      "Epoch 47/50\n",
      "865/865 - 3s - loss: 141.3935 - val_loss: 279.7346\n",
      "Epoch 48/50\n",
      "865/865 - 3s - loss: 141.3981 - val_loss: 279.2684\n",
      "Epoch 49/50\n",
      "865/865 - 3s - loss: 141.4204 - val_loss: 278.4548\n",
      "Epoch 50/50\n",
      "865/865 - 3s - loss: 141.2709 - val_loss: 280.9663\n",
      "Training time for iteration 1: 174.07132029533386\n",
      "\n",
      "\n",
      "Iteration  2\n",
      "Layer size = 200, activation = sigmoid, batch size = 1024, dropout = 0.2\n",
      "Epoch 1/50\n",
      "3460/3460 - 12s - loss: 371.7603 - val_loss: 344.8805\n",
      "Epoch 2/50\n",
      "3460/3460 - 12s - loss: 198.6655 - val_loss: 324.6979\n",
      "Epoch 3/50\n",
      "3460/3460 - 12s - loss: 157.9846 - val_loss: 325.8992\n",
      "Epoch 4/50\n",
      "3460/3460 - 12s - loss: 143.8618 - val_loss: 323.8504\n",
      "Epoch 5/50\n",
      "3460/3460 - 12s - loss: 138.1362 - val_loss: 318.3623\n",
      "Epoch 6/50\n",
      "3460/3460 - 12s - loss: 134.3226 - val_loss: 312.0404\n",
      "Epoch 7/50\n",
      "3460/3460 - 12s - loss: 131.5339 - val_loss: 306.8610\n",
      "Epoch 8/50\n",
      "3460/3460 - 12s - loss: 130.0202 - val_loss: 301.9949\n",
      "Epoch 9/50\n",
      "3460/3460 - 12s - loss: 128.8640 - val_loss: 292.1192\n",
      "Epoch 10/50\n",
      "3460/3460 - 12s - loss: 127.9285 - val_loss: 290.0038\n",
      "Epoch 11/50\n",
      "3460/3460 - 12s - loss: 127.0907 - val_loss: 284.5588\n",
      "Epoch 12/50\n",
      "3460/3460 - 12s - loss: 126.1596 - val_loss: 285.8660\n",
      "Epoch 13/50\n",
      "3460/3460 - 12s - loss: 125.4474 - val_loss: 281.9030\n",
      "Epoch 14/50\n",
      "3460/3460 - 12s - loss: 125.0241 - val_loss: 275.1526\n",
      "Epoch 15/50\n",
      "3460/3460 - 12s - loss: 124.5184 - val_loss: 271.0645\n",
      "Epoch 16/50\n",
      "3460/3460 - 12s - loss: 124.0709 - val_loss: 263.9264\n",
      "Epoch 17/50\n",
      "3460/3460 - 12s - loss: 123.6367 - val_loss: 266.0587\n",
      "Epoch 18/50\n",
      "3460/3460 - 12s - loss: 123.2697 - val_loss: 262.9447\n",
      "Epoch 19/50\n",
      "3460/3460 - 12s - loss: 122.8071 - val_loss: 261.4724\n",
      "Epoch 20/50\n",
      "3460/3460 - 12s - loss: 122.4492 - val_loss: 259.1934\n",
      "Epoch 21/50\n",
      "3460/3460 - 12s - loss: 122.3861 - val_loss: 258.1469\n",
      "Epoch 22/50\n",
      "3460/3460 - 12s - loss: 122.0561 - val_loss: 258.4165\n",
      "Epoch 23/50\n",
      "3460/3460 - 12s - loss: 121.9889 - val_loss: 257.1918\n",
      "Epoch 24/50\n",
      "3460/3460 - 12s - loss: 121.6646 - val_loss: 256.4093\n",
      "Epoch 25/50\n",
      "3460/3460 - 12s - loss: 121.4999 - val_loss: 254.0593\n",
      "Epoch 26/50\n",
      "3460/3460 - 12s - loss: 121.1576 - val_loss: 250.3074\n",
      "Epoch 27/50\n",
      "3460/3460 - 12s - loss: 121.0931 - val_loss: 245.3760\n",
      "Epoch 28/50\n",
      "3460/3460 - 12s - loss: 120.9620 - val_loss: 248.4130\n",
      "Epoch 29/50\n",
      "3460/3460 - 12s - loss: 120.7604 - val_loss: 243.8104\n",
      "Epoch 30/50\n",
      "3460/3460 - 12s - loss: 120.6008 - val_loss: 242.5295\n",
      "Epoch 31/50\n",
      "3460/3460 - 12s - loss: 120.4573 - val_loss: 240.6859\n",
      "Epoch 32/50\n",
      "3460/3460 - 12s - loss: 120.3569 - val_loss: 244.0128\n",
      "Epoch 33/50\n",
      "3460/3460 - 12s - loss: 120.1585 - val_loss: 242.0955\n",
      "Epoch 34/50\n",
      "3460/3460 - 12s - loss: 119.9919 - val_loss: 240.3887\n",
      "Epoch 35/50\n",
      "3460/3460 - 12s - loss: 119.8708 - val_loss: 239.0500\n",
      "Epoch 36/50\n",
      "3460/3460 - 12s - loss: 119.6456 - val_loss: 238.0717\n",
      "Epoch 37/50\n",
      "3460/3460 - 12s - loss: 119.6324 - val_loss: 235.2204\n",
      "Epoch 38/50\n",
      "3460/3460 - 12s - loss: 119.5898 - val_loss: 237.8064\n",
      "Epoch 39/50\n",
      "3460/3460 - 12s - loss: 119.4113 - val_loss: 234.7408\n",
      "Epoch 40/50\n",
      "3460/3460 - 12s - loss: 119.2807 - val_loss: 232.8733\n",
      "Epoch 41/50\n",
      "3460/3460 - 12s - loss: 119.2315 - val_loss: 234.5658\n",
      "Epoch 42/50\n",
      "3460/3460 - 12s - loss: 119.0865 - val_loss: 231.2600\n",
      "Epoch 43/50\n",
      "3460/3460 - 12s - loss: 118.8545 - val_loss: 232.1819\n",
      "Epoch 44/50\n",
      "3460/3460 - 12s - loss: 118.8258 - val_loss: 233.2885\n",
      "Epoch 45/50\n",
      "3460/3460 - 12s - loss: 118.6608 - val_loss: 233.9886\n",
      "Epoch 46/50\n",
      "3460/3460 - 12s - loss: 118.6080 - val_loss: 230.9677\n",
      "Epoch 47/50\n",
      "3460/3460 - 12s - loss: 118.4832 - val_loss: 231.3021\n",
      "Epoch 48/50\n",
      "3460/3460 - 12s - loss: 118.4775 - val_loss: 231.1466\n",
      "Epoch 49/50\n",
      "3460/3460 - 12s - loss: 118.4170 - val_loss: 228.6890\n",
      "Epoch 50/50\n",
      "3460/3460 - 11s - loss: 118.3538 - val_loss: 228.7561\n",
      "Training time for iteration 2: 590.0474910736084\n",
      "\n",
      "\n",
      "Iteration  3\n",
      "Layer size = 50, activation = relu, batch size = 512, dropout = 0.2\n",
      "Epoch 1/50\n",
      "6920/6920 - 21s - loss: 394.7911 - val_loss: 351.7370\n",
      "Epoch 2/50\n",
      "6920/6920 - 21s - loss: 230.6328 - val_loss: 331.5946\n",
      "Epoch 3/50\n",
      "6920/6920 - 20s - loss: 188.9296 - val_loss: 327.1504\n",
      "Epoch 4/50\n",
      "6920/6920 - 22s - loss: 171.5697 - val_loss: 323.3914\n",
      "Epoch 5/50\n",
      "6920/6920 - 22s - loss: 163.1945 - val_loss: 313.3961\n",
      "Epoch 6/50\n",
      "6920/6920 - 22s - loss: 157.3928 - val_loss: 306.4860\n",
      "Epoch 7/50\n",
      "6920/6920 - 21s - loss: 153.4307 - val_loss: 301.2585\n",
      "Epoch 8/50\n",
      "6920/6920 - 21s - loss: 150.8325 - val_loss: 298.0676\n",
      "Epoch 9/50\n",
      "6920/6920 - 21s - loss: 149.6809 - val_loss: 290.7577\n",
      "Epoch 10/50\n",
      "6920/6920 - 21s - loss: 148.6284 - val_loss: 291.4875\n",
      "Epoch 11/50\n",
      "6920/6920 - 20s - loss: 147.4186 - val_loss: 284.3949\n",
      "Epoch 12/50\n",
      "6920/6920 - 20s - loss: 146.5563 - val_loss: 287.6052\n",
      "Epoch 13/50\n",
      "6920/6920 - 21s - loss: 145.1013 - val_loss: 282.8093\n",
      "Epoch 14/50\n",
      "6920/6920 - 21s - loss: 144.5196 - val_loss: 281.7222\n",
      "Epoch 15/50\n",
      "6920/6920 - 21s - loss: 143.7636 - val_loss: 276.3344\n",
      "Epoch 16/50\n",
      "6920/6920 - 21s - loss: 143.7393 - val_loss: 274.9159\n",
      "Epoch 17/50\n",
      "6920/6920 - 21s - loss: 143.4083 - val_loss: 276.9264\n",
      "Epoch 18/50\n",
      "6920/6920 - 21s - loss: 143.0441 - val_loss: 272.1664\n",
      "Epoch 19/50\n",
      "6920/6920 - 21s - loss: 142.9609 - val_loss: 271.6384\n",
      "Epoch 20/50\n",
      "6920/6920 - 21s - loss: 142.7938 - val_loss: 271.3089\n",
      "Epoch 21/50\n",
      "6920/6920 - 21s - loss: 142.6919 - val_loss: 270.9001\n",
      "Epoch 22/50\n",
      "6920/6920 - 21s - loss: 142.6722 - val_loss: 271.3216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "6920/6920 - 21s - loss: 142.5112 - val_loss: 270.8837\n",
      "Epoch 24/50\n",
      "6920/6920 - 21s - loss: 142.3551 - val_loss: 271.3700\n",
      "Epoch 25/50\n",
      "6920/6920 - 20s - loss: 142.3488 - val_loss: 269.5215\n",
      "Epoch 26/50\n",
      "6920/6920 - 19s - loss: 142.2084 - val_loss: 268.6650\n",
      "Epoch 27/50\n",
      "6920/6920 - 20s - loss: 142.1132 - val_loss: 265.4964\n",
      "Epoch 28/50\n",
      "6920/6920 - 19s - loss: 142.0426 - val_loss: 266.9849\n",
      "Epoch 29/50\n",
      "6920/6920 - 19s - loss: 142.0344 - val_loss: 262.3175\n",
      "Epoch 30/50\n",
      "6920/6920 - 19s - loss: 142.0628 - val_loss: 263.9802\n",
      "Epoch 31/50\n",
      "6920/6920 - 19s - loss: 141.6036 - val_loss: 268.6519\n",
      "Epoch 32/50\n",
      "6920/6920 - 19s - loss: 141.1811 - val_loss: 265.0557\n",
      "Epoch 33/50\n",
      "6920/6920 - 19s - loss: 141.1789 - val_loss: 265.6983\n",
      "Epoch 34/50\n",
      "6920/6920 - 19s - loss: 140.9750 - val_loss: 259.6786\n",
      "Epoch 35/50\n",
      "6920/6920 - 19s - loss: 140.7414 - val_loss: 261.3293\n",
      "Epoch 36/50\n",
      "6920/6920 - 19s - loss: 140.6194 - val_loss: 260.0631\n",
      "Epoch 37/50\n",
      "6920/6920 - 20s - loss: 140.6537 - val_loss: 258.3746\n",
      "Epoch 38/50\n",
      "6920/6920 - 19s - loss: 140.4101 - val_loss: 262.2738\n",
      "Epoch 39/50\n",
      "6920/6920 - 19s - loss: 140.4337 - val_loss: 258.5705\n",
      "Epoch 40/50\n",
      "6920/6920 - 19s - loss: 140.4489 - val_loss: 258.0167\n",
      "Epoch 41/50\n",
      "6920/6920 - 20s - loss: 140.2573 - val_loss: 258.0620\n",
      "Epoch 42/50\n",
      "6920/6920 - 19s - loss: 140.3067 - val_loss: 257.5055\n",
      "Epoch 43/50\n",
      "6920/6920 - 19s - loss: 140.2931 - val_loss: 257.9350\n",
      "Epoch 44/50\n",
      "6920/6920 - 19s - loss: 140.2556 - val_loss: 261.0562\n",
      "Epoch 45/50\n",
      "6920/6920 - 19s - loss: 140.1940 - val_loss: 261.1329\n",
      "Epoch 46/50\n",
      "6920/6920 - 19s - loss: 140.2408 - val_loss: 258.3830\n",
      "Epoch 47/50\n",
      "6920/6920 - 19s - loss: 140.1435 - val_loss: 258.0266\n",
      "Epoch 48/50\n",
      "6920/6920 - 20s - loss: 140.2417 - val_loss: 257.7896\n",
      "Epoch 49/50\n",
      "6920/6920 - 19s - loss: 140.0662 - val_loss: 256.5016\n",
      "Epoch 50/50\n",
      "6920/6920 - 19s - loss: 140.2208 - val_loss: 259.0833\n",
      "Training time for iteration 3: 1009.99090051651\n",
      "\n",
      "\n",
      "Iteration  4\n",
      "Layer size = 300, activation = tanh, batch size = 512, dropout = 0.1\n",
      "Epoch 1/50\n",
      "6920/6920 - 20s - loss: 264.0514 - val_loss: 327.1253\n",
      "Epoch 2/50\n",
      "6920/6920 - 20s - loss: 141.3849 - val_loss: 328.8450\n",
      "Epoch 3/50\n",
      "6920/6920 - 20s - loss: 126.0953 - val_loss: 309.8139\n",
      "Epoch 4/50\n",
      "6920/6920 - 20s - loss: 120.7703 - val_loss: 304.1865\n",
      "Epoch 5/50\n",
      "6920/6920 - 20s - loss: 117.8333 - val_loss: 296.8659\n",
      "Epoch 6/50\n",
      "6920/6920 - 20s - loss: 115.9026 - val_loss: 289.3551\n",
      "Epoch 7/50\n",
      "6920/6920 - 20s - loss: 114.4092 - val_loss: 284.2660\n",
      "Epoch 8/50\n",
      "6920/6920 - 20s - loss: 113.3524 - val_loss: 273.9349\n",
      "Epoch 9/50\n",
      "6920/6920 - 20s - loss: 112.5773 - val_loss: 267.8076\n",
      "Epoch 10/50\n",
      "6920/6920 - 20s - loss: 111.9280 - val_loss: 264.9849\n",
      "Epoch 11/50\n",
      "6920/6920 - 20s - loss: 111.2478 - val_loss: 256.2030\n",
      "Epoch 12/50\n",
      "6920/6920 - 20s - loss: 110.8251 - val_loss: 256.7543\n",
      "Epoch 13/50\n",
      "6920/6920 - 20s - loss: 110.3335 - val_loss: 251.2133\n",
      "Epoch 14/50\n",
      "6920/6920 - 20s - loss: 109.9299 - val_loss: 250.1479\n",
      "Epoch 15/50\n",
      "6920/6920 - 20s - loss: 109.6283 - val_loss: 242.3806\n",
      "Epoch 16/50\n",
      "6920/6920 - 20s - loss: 109.1081 - val_loss: 242.3799\n",
      "Epoch 17/50\n",
      "6920/6920 - 20s - loss: 108.8469 - val_loss: 238.6958\n",
      "Epoch 18/50\n",
      "6920/6920 - 20s - loss: 108.4561 - val_loss: 239.0723\n",
      "Epoch 19/50\n",
      "6920/6920 - 20s - loss: 108.1188 - val_loss: 235.1948\n",
      "Epoch 20/50\n",
      "6920/6920 - 20s - loss: 107.8407 - val_loss: 232.2031\n",
      "Epoch 21/50\n",
      "6920/6920 - 20s - loss: 107.4847 - val_loss: 230.1019\n",
      "Epoch 22/50\n",
      "6920/6920 - 20s - loss: 107.1615 - val_loss: 228.0324\n",
      "Epoch 23/50\n",
      "6920/6920 - 20s - loss: 106.9132 - val_loss: 224.0254\n",
      "Epoch 24/50\n",
      "6920/6920 - 20s - loss: 106.7905 - val_loss: 223.7080\n",
      "Epoch 25/50\n",
      "6920/6920 - 20s - loss: 106.6243 - val_loss: 221.7412\n",
      "Epoch 26/50\n",
      "6920/6920 - 20s - loss: 106.3607 - val_loss: 219.4516\n",
      "Epoch 27/50\n",
      "6920/6920 - 20s - loss: 106.3422 - val_loss: 217.4360\n",
      "Epoch 28/50\n",
      "6920/6920 - 21s - loss: 106.0601 - val_loss: 216.8790\n",
      "Epoch 29/50\n",
      "6920/6920 - 20s - loss: 105.9629 - val_loss: 212.9280\n",
      "Epoch 30/50\n",
      "6920/6920 - 20s - loss: 105.8042 - val_loss: 215.3917\n",
      "Epoch 31/50\n",
      "6920/6920 - 21s - loss: 105.6625 - val_loss: 208.4936\n",
      "Epoch 32/50\n",
      "6920/6920 - 21s - loss: 105.5962 - val_loss: 210.6519\n",
      "Epoch 33/50\n",
      "6920/6920 - 20s - loss: 105.4115 - val_loss: 212.1944\n",
      "Epoch 34/50\n",
      "6920/6920 - 20s - loss: 105.1387 - val_loss: 206.0445\n",
      "Epoch 35/50\n",
      "6920/6920 - 20s - loss: 104.9748 - val_loss: 210.0214\n",
      "Epoch 36/50\n",
      "6920/6920 - 20s - loss: 104.7106 - val_loss: 206.8443\n",
      "Epoch 37/50\n",
      "6920/6920 - 20s - loss: 104.6962 - val_loss: 207.2280\n",
      "Epoch 38/50\n",
      "6920/6920 - 20s - loss: 104.6493 - val_loss: 206.0446\n",
      "Epoch 39/50\n",
      "6920/6920 - 20s - loss: 104.3773 - val_loss: 208.0495\n",
      "Epoch 40/50\n",
      "6920/6920 - 20s - loss: 104.2012 - val_loss: 202.2014\n",
      "Epoch 41/50\n",
      "6920/6920 - 20s - loss: 103.9398 - val_loss: 204.1530\n",
      "Epoch 42/50\n",
      "6920/6920 - 20s - loss: 103.7355 - val_loss: 203.8855\n",
      "Epoch 43/50\n",
      "6920/6920 - 20s - loss: 103.3993 - val_loss: 204.7037\n",
      "Epoch 44/50\n",
      "6920/6920 - 20s - loss: 103.1057 - val_loss: 204.9819\n",
      "Epoch 45/50\n",
      "6920/6920 - 20s - loss: 102.8970 - val_loss: 200.8375\n",
      "Epoch 46/50\n",
      "6920/6920 - 20s - loss: 102.6857 - val_loss: 204.7586\n",
      "Epoch 47/50\n",
      "6920/6920 - 20s - loss: 102.4699 - val_loss: 199.8484\n",
      "Epoch 48/50\n",
      "6920/6920 - 21s - loss: 102.3464 - val_loss: 199.9680\n",
      "Epoch 49/50\n",
      "6920/6920 - 20s - loss: 102.2330 - val_loss: 197.6374\n",
      "Epoch 50/50\n",
      "6920/6920 - 20s - loss: 101.8913 - val_loss: 202.3510\n",
      "Training time for iteration 4: 1004.6424021720886\n",
      "\n",
      "\n",
      "Iteration  5\n",
      "Layer size = 500, activation = sigmoid, batch size = 2048, dropout = 0.1\n",
      "Epoch 1/50\n",
      "1730/1730 - 9s - loss: 396.4367 - val_loss: 350.5074\n",
      "Epoch 2/50\n",
      "1730/1730 - 9s - loss: 201.0642 - val_loss: 324.3938\n",
      "Epoch 3/50\n",
      "1730/1730 - 9s - loss: 159.2955 - val_loss: 314.2950\n",
      "Epoch 4/50\n",
      "1730/1730 - 9s - loss: 139.1422 - val_loss: 308.3775\n",
      "Epoch 5/50\n",
      "1730/1730 - 9s - loss: 128.8985 - val_loss: 306.6811\n",
      "Epoch 6/50\n",
      "1730/1730 - 9s - loss: 122.5036 - val_loss: 299.6983\n",
      "Epoch 7/50\n",
      "1730/1730 - 9s - loss: 118.2610 - val_loss: 299.8538\n",
      "Epoch 8/50\n",
      "1730/1730 - 9s - loss: 115.4405 - val_loss: 294.2670\n",
      "Epoch 9/50\n",
      "1730/1730 - 9s - loss: 113.3613 - val_loss: 283.0493\n",
      "Epoch 10/50\n",
      "1730/1730 - 9s - loss: 111.4870 - val_loss: 278.1231\n",
      "Epoch 11/50\n",
      "1730/1730 - 9s - loss: 109.9660 - val_loss: 274.7164\n",
      "Epoch 12/50\n",
      "1730/1730 - 9s - loss: 108.5386 - val_loss: 272.4204\n",
      "Epoch 13/50\n",
      "1730/1730 - 9s - loss: 107.4325 - val_loss: 265.7998\n",
      "Epoch 14/50\n",
      "1730/1730 - 9s - loss: 106.5622 - val_loss: 260.7282\n",
      "Epoch 15/50\n",
      "1730/1730 - 9s - loss: 105.7405 - val_loss: 257.1280\n",
      "Epoch 16/50\n",
      "1730/1730 - 9s - loss: 105.1098 - val_loss: 247.8709\n",
      "Epoch 17/50\n",
      "1730/1730 - 9s - loss: 104.5190 - val_loss: 249.9512\n",
      "Epoch 18/50\n",
      "1730/1730 - 9s - loss: 103.9917 - val_loss: 244.5092\n",
      "Epoch 19/50\n",
      "1730/1730 - 9s - loss: 103.5134 - val_loss: 241.9588\n",
      "Epoch 20/50\n",
      "1730/1730 - 9s - loss: 102.9888 - val_loss: 241.3843\n",
      "Epoch 21/50\n",
      "1730/1730 - 9s - loss: 102.6493 - val_loss: 236.8890\n",
      "Epoch 22/50\n",
      "1730/1730 - 9s - loss: 102.0032 - val_loss: 232.4438\n",
      "Epoch 23/50\n",
      "1730/1730 - 9s - loss: 101.7324 - val_loss: 235.3776\n",
      "Epoch 24/50\n",
      "1730/1730 - 9s - loss: 101.2470 - val_loss: 230.7560\n",
      "Epoch 25/50\n",
      "1730/1730 - 9s - loss: 101.0478 - val_loss: 224.1157\n",
      "Epoch 26/50\n",
      "1730/1730 - 9s - loss: 100.5734 - val_loss: 224.0014\n",
      "Epoch 27/50\n",
      "1730/1730 - 9s - loss: 100.1830 - val_loss: 220.0627\n",
      "Epoch 28/50\n",
      "1730/1730 - 9s - loss: 99.8925 - val_loss: 221.7841\n",
      "Epoch 29/50\n",
      "1730/1730 - 9s - loss: 99.5980 - val_loss: 218.5766\n",
      "Epoch 30/50\n",
      "1730/1730 - 9s - loss: 99.2965 - val_loss: 217.9802\n",
      "Epoch 31/50\n",
      "1730/1730 - 9s - loss: 99.0244 - val_loss: 214.5769\n",
      "Epoch 32/50\n",
      "1730/1730 - 9s - loss: 98.8255 - val_loss: 216.8003\n",
      "Epoch 33/50\n",
      "1730/1730 - 9s - loss: 98.4867 - val_loss: 210.5851\n",
      "Epoch 34/50\n",
      "1730/1730 - 9s - loss: 98.2643 - val_loss: 210.5730\n",
      "Epoch 35/50\n",
      "1730/1730 - 9s - loss: 97.9505 - val_loss: 210.1029\n",
      "Epoch 36/50\n",
      "1730/1730 - 9s - loss: 97.7446 - val_loss: 209.7529\n",
      "Epoch 37/50\n",
      "1730/1730 - 9s - loss: 97.5126 - val_loss: 204.1647\n",
      "Epoch 38/50\n",
      "1730/1730 - 9s - loss: 97.3041 - val_loss: 206.7292\n",
      "Epoch 39/50\n",
      "1730/1730 - 9s - loss: 97.2368 - val_loss: 202.0084\n",
      "Epoch 40/50\n",
      "1730/1730 - 9s - loss: 97.0568 - val_loss: 200.7496\n",
      "Epoch 41/50\n",
      "1730/1730 - 9s - loss: 96.9109 - val_loss: 200.9609\n",
      "Epoch 42/50\n",
      "1730/1730 - 9s - loss: 96.6582 - val_loss: 198.3472\n",
      "Epoch 43/50\n",
      "1730/1730 - 9s - loss: 96.5570 - val_loss: 198.1552\n",
      "Epoch 44/50\n",
      "1730/1730 - 9s - loss: 96.4674 - val_loss: 198.6753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "1730/1730 - 9s - loss: 96.2601 - val_loss: 197.6980\n",
      "Epoch 46/50\n",
      "1730/1730 - 9s - loss: 96.1894 - val_loss: 198.2617\n",
      "Epoch 47/50\n",
      "1730/1730 - 9s - loss: 95.9604 - val_loss: 196.9119\n",
      "Epoch 48/50\n",
      "1730/1730 - 9s - loss: 95.8419 - val_loss: 194.6739\n",
      "Epoch 49/50\n",
      "1730/1730 - 9s - loss: 95.7247 - val_loss: 193.9533\n",
      "Epoch 50/50\n",
      "1730/1730 - 9s - loss: 95.4777 - val_loss: 193.6474\n",
      "Training time for iteration 5: 454.2009446620941\n",
      "\n",
      "\n",
      "Iteration  6\n",
      "Layer size = 300, activation = tanh, batch size = 4096, dropout = 0.1\n",
      "Epoch 1/50\n",
      "865/865 - 5s - loss: 620.5825 - val_loss: 372.4251\n",
      "Epoch 2/50\n",
      "865/865 - 5s - loss: 262.7896 - val_loss: 353.0065\n",
      "Epoch 3/50\n",
      "865/865 - 5s - loss: 229.9757 - val_loss: 341.2409\n",
      "Epoch 4/50\n",
      "865/865 - 5s - loss: 201.6724 - val_loss: 327.2952\n",
      "Epoch 5/50\n",
      "865/865 - 5s - loss: 177.4448 - val_loss: 319.3315\n",
      "Epoch 6/50\n",
      "865/865 - 5s - loss: 160.0800 - val_loss: 311.0482\n",
      "Epoch 7/50\n",
      "865/865 - 5s - loss: 148.8618 - val_loss: 307.8676\n",
      "Epoch 8/50\n",
      "865/865 - 5s - loss: 141.3578 - val_loss: 306.1148\n",
      "Epoch 9/50\n",
      "865/865 - 5s - loss: 135.8205 - val_loss: 304.2140\n",
      "Epoch 10/50\n",
      "865/865 - 5s - loss: 131.8906 - val_loss: 303.3495\n",
      "Epoch 11/50\n",
      "865/865 - 5s - loss: 128.5599 - val_loss: 300.1119\n",
      "Epoch 12/50\n",
      "865/865 - 5s - loss: 125.6586 - val_loss: 296.7129\n",
      "Epoch 13/50\n",
      "865/865 - 5s - loss: 122.9997 - val_loss: 296.0007\n",
      "Epoch 14/50\n",
      "865/865 - 5s - loss: 120.6745 - val_loss: 294.5979\n",
      "Epoch 15/50\n",
      "865/865 - 5s - loss: 118.8612 - val_loss: 284.5189\n",
      "Epoch 16/50\n",
      "865/865 - 5s - loss: 117.1509 - val_loss: 280.8098\n",
      "Epoch 17/50\n",
      "865/865 - 5s - loss: 115.6985 - val_loss: 277.3776\n",
      "Epoch 18/50\n",
      "865/865 - 5s - loss: 114.3967 - val_loss: 274.7885\n",
      "Epoch 19/50\n",
      "865/865 - 5s - loss: 113.3866 - val_loss: 270.2742\n",
      "Epoch 20/50\n",
      "865/865 - 5s - loss: 112.4804 - val_loss: 269.1538\n",
      "Epoch 21/50\n",
      "865/865 - 5s - loss: 111.7881 - val_loss: 266.2482\n",
      "Epoch 22/50\n",
      "865/865 - 5s - loss: 111.2415 - val_loss: 262.6561\n",
      "Epoch 23/50\n",
      "865/865 - 5s - loss: 110.7115 - val_loss: 260.5879\n",
      "Epoch 24/50\n",
      "865/865 - 5s - loss: 110.1780 - val_loss: 260.1357\n",
      "Epoch 25/50\n",
      "865/865 - 5s - loss: 109.6729 - val_loss: 256.7641\n",
      "Epoch 26/50\n",
      "865/865 - 5s - loss: 109.3361 - val_loss: 254.9491\n",
      "Epoch 27/50\n",
      "865/865 - 5s - loss: 108.9968 - val_loss: 251.9517\n",
      "Epoch 28/50\n",
      "865/865 - 5s - loss: 108.6631 - val_loss: 251.0550\n",
      "Epoch 29/50\n",
      "865/865 - 5s - loss: 108.3540 - val_loss: 245.9587\n",
      "Epoch 30/50\n",
      "865/865 - 5s - loss: 107.9560 - val_loss: 247.5125\n",
      "Epoch 31/50\n",
      "865/865 - 5s - loss: 107.6892 - val_loss: 245.4690\n",
      "Epoch 32/50\n",
      "865/865 - 5s - loss: 107.3615 - val_loss: 244.4729\n",
      "Epoch 33/50\n",
      "865/865 - 5s - loss: 107.1606 - val_loss: 242.2499\n",
      "Epoch 34/50\n",
      "865/865 - 5s - loss: 106.9339 - val_loss: 241.1609\n",
      "Epoch 35/50\n",
      "865/865 - 5s - loss: 106.7970 - val_loss: 238.3055\n",
      "Epoch 36/50\n",
      "865/865 - 5s - loss: 106.4951 - val_loss: 238.7081\n",
      "Epoch 37/50\n",
      "865/865 - 5s - loss: 106.2552 - val_loss: 236.0097\n",
      "Epoch 38/50\n",
      "865/865 - 5s - loss: 106.0008 - val_loss: 235.7297\n",
      "Epoch 39/50\n",
      "865/865 - 5s - loss: 105.8581 - val_loss: 232.9678\n",
      "Epoch 40/50\n",
      "865/865 - 5s - loss: 105.7246 - val_loss: 232.3970\n",
      "Epoch 41/50\n",
      "865/865 - 5s - loss: 105.4571 - val_loss: 232.4337\n",
      "Epoch 42/50\n",
      "865/865 - 5s - loss: 105.3258 - val_loss: 228.3037\n",
      "Epoch 43/50\n",
      "865/865 - 5s - loss: 105.0561 - val_loss: 232.5434\n",
      "Epoch 44/50\n",
      "865/865 - 5s - loss: 104.9941 - val_loss: 228.2921\n",
      "Epoch 45/50\n",
      "865/865 - 5s - loss: 104.7555 - val_loss: 229.0215\n",
      "Epoch 46/50\n",
      "865/865 - 5s - loss: 104.6077 - val_loss: 225.9545\n",
      "Epoch 47/50\n",
      "865/865 - 5s - loss: 104.4795 - val_loss: 227.6928\n",
      "Epoch 48/50\n",
      "865/865 - 5s - loss: 104.3226 - val_loss: 224.7950\n",
      "Epoch 49/50\n",
      "865/865 - 5s - loss: 104.2022 - val_loss: 224.8155\n",
      "Epoch 50/50\n",
      "865/865 - 5s - loss: 104.0750 - val_loss: 221.3105\n",
      "Training time for iteration 6: 255.0132701396942\n",
      "\n",
      "\n",
      "Iteration  7\n",
      "Layer size = 200, activation = relu, batch size = 1024, dropout = 0.4\n",
      "Epoch 1/50\n",
      "3460/3460 - 11s - loss: 386.9777 - val_loss: 349.3063\n",
      "Epoch 2/50\n",
      "3460/3460 - 11s - loss: 219.2161 - val_loss: 327.2654\n",
      "Epoch 3/50\n",
      "3460/3460 - 10s - loss: 176.7556 - val_loss: 325.0682\n",
      "Epoch 4/50\n",
      "3460/3460 - 11s - loss: 160.3467 - val_loss: 324.9072\n",
      "Epoch 5/50\n",
      "3460/3460 - 11s - loss: 153.9010 - val_loss: 317.4145\n",
      "Epoch 6/50\n",
      "3460/3460 - 11s - loss: 149.9578 - val_loss: 310.4958\n",
      "Epoch 7/50\n",
      "3460/3460 - 11s - loss: 147.3181 - val_loss: 307.5398\n",
      "Epoch 8/50\n",
      "3460/3460 - 10s - loss: 144.8381 - val_loss: 304.7980\n",
      "Epoch 9/50\n",
      "3460/3460 - 10s - loss: 143.0857 - val_loss: 296.9644\n",
      "Epoch 10/50\n",
      "3460/3460 - 11s - loss: 141.5386 - val_loss: 294.8641\n",
      "Epoch 11/50\n",
      "3460/3460 - 10s - loss: 140.4758 - val_loss: 289.1938\n",
      "Epoch 12/50\n",
      "3460/3460 - 11s - loss: 139.4031 - val_loss: 292.4952\n",
      "Epoch 13/50\n",
      "3460/3460 - 10s - loss: 138.6172 - val_loss: 290.5744\n",
      "Epoch 14/50\n",
      "3460/3460 - 11s - loss: 138.0727 - val_loss: 286.9853\n",
      "Epoch 15/50\n",
      "3460/3460 - 11s - loss: 137.3505 - val_loss: 282.7204\n",
      "Epoch 16/50\n",
      "3460/3460 - 11s - loss: 137.1090 - val_loss: 276.2226\n",
      "Epoch 17/50\n",
      "3460/3460 - 11s - loss: 136.6064 - val_loss: 279.6917\n",
      "Epoch 18/50\n",
      "3460/3460 - 11s - loss: 136.1489 - val_loss: 278.0672\n",
      "Epoch 19/50\n",
      "3460/3460 - 10s - loss: 136.1390 - val_loss: 276.6728\n",
      "Epoch 20/50\n",
      "3460/3460 - 11s - loss: 135.8091 - val_loss: 276.8906\n",
      "Epoch 21/50\n",
      "3460/3460 - 11s - loss: 135.5432 - val_loss: 276.2088\n",
      "Epoch 22/50\n",
      "3460/3460 - 10s - loss: 135.3760 - val_loss: 274.4854\n",
      "Epoch 23/50\n",
      "3460/3460 - 10s - loss: 135.2080 - val_loss: 274.7862\n",
      "Epoch 24/50\n",
      "3460/3460 - 10s - loss: 135.0360 - val_loss: 275.1176\n",
      "Epoch 25/50\n",
      "3460/3460 - 10s - loss: 135.0689 - val_loss: 271.0101\n",
      "Epoch 26/50\n",
      "3460/3460 - 10s - loss: 134.9087 - val_loss: 270.8206\n",
      "Epoch 27/50\n",
      "3460/3460 - 10s - loss: 134.7716 - val_loss: 268.9062\n",
      "Epoch 28/50\n",
      "3460/3460 - 10s - loss: 134.6116 - val_loss: 271.4499\n",
      "Epoch 29/50\n",
      "3460/3460 - 10s - loss: 134.5564 - val_loss: 267.2419\n",
      "Epoch 30/50\n",
      "3460/3460 - 10s - loss: 134.4561 - val_loss: 267.5062\n",
      "Epoch 31/50\n",
      "3460/3460 - 10s - loss: 134.3077 - val_loss: 268.5026\n",
      "Epoch 32/50\n",
      "3460/3460 - 10s - loss: 134.1993 - val_loss: 270.1460\n",
      "Epoch 33/50\n",
      "3460/3460 - 10s - loss: 134.1763 - val_loss: 266.3312\n",
      "Epoch 34/50\n",
      "3460/3460 - 10s - loss: 134.1170 - val_loss: 265.0091\n",
      "Epoch 35/50\n",
      "3460/3460 - 10s - loss: 133.7990 - val_loss: 265.4753\n",
      "Epoch 36/50\n",
      "3460/3460 - 10s - loss: 133.7027 - val_loss: 265.1014\n",
      "Epoch 37/50\n",
      "3460/3460 - 10s - loss: 133.6290 - val_loss: 261.6399\n",
      "Epoch 38/50\n",
      "3460/3460 - 10s - loss: 133.4897 - val_loss: 266.7750\n",
      "Epoch 39/50\n",
      "3460/3460 - 10s - loss: 133.5524 - val_loss: 261.4022\n",
      "Epoch 40/50\n",
      "3460/3460 - 10s - loss: 133.4967 - val_loss: 261.2613\n",
      "Epoch 41/50\n",
      "3460/3460 - 10s - loss: 133.3141 - val_loss: 263.0771\n",
      "Epoch 42/50\n",
      "3460/3460 - 10s - loss: 133.4156 - val_loss: 261.5783\n",
      "Epoch 43/50\n",
      "3460/3460 - 10s - loss: 133.2600 - val_loss: 263.0514\n",
      "Epoch 44/50\n",
      "3460/3460 - 10s - loss: 133.3607 - val_loss: 261.8356\n",
      "Epoch 45/50\n",
      "3460/3460 - 10s - loss: 133.1740 - val_loss: 264.9138\n",
      "Epoch 46/50\n",
      "3460/3460 - 10s - loss: 132.9909 - val_loss: 261.5585\n",
      "Epoch 47/50\n",
      "3460/3460 - 10s - loss: 133.0442 - val_loss: 262.5023\n",
      "Epoch 48/50\n",
      "3460/3460 - 10s - loss: 132.6564 - val_loss: 260.7444\n",
      "Epoch 49/50\n",
      "3460/3460 - 10s - loss: 132.9071 - val_loss: 259.9757\n",
      "Epoch 50/50\n",
      "3460/3460 - 10s - loss: 132.7920 - val_loss: 260.3115\n",
      "Training time for iteration 7: 511.6008970737457\n",
      "\n",
      "\n",
      "Iteration  8\n",
      "Layer size = 500, activation = tanh, batch size = 4096, dropout = 0.4\n",
      "Epoch 1/50\n",
      "865/865 - 7s - loss: 551.5019 - val_loss: 365.6985\n",
      "Epoch 2/50\n",
      "865/865 - 6s - loss: 258.9706 - val_loss: 347.7015\n",
      "Epoch 3/50\n",
      "865/865 - 7s - loss: 224.8018 - val_loss: 336.6382\n",
      "Epoch 4/50\n",
      "865/865 - 6s - loss: 197.4515 - val_loss: 323.6827\n",
      "Epoch 5/50\n",
      "865/865 - 7s - loss: 175.0989 - val_loss: 317.2572\n",
      "Epoch 6/50\n",
      "865/865 - 7s - loss: 160.2427 - val_loss: 310.3225\n",
      "Epoch 7/50\n",
      "865/865 - 7s - loss: 150.4503 - val_loss: 309.1411\n",
      "Epoch 8/50\n",
      "865/865 - 7s - loss: 144.2684 - val_loss: 307.0460\n",
      "Epoch 9/50\n",
      "865/865 - 7s - loss: 139.8161 - val_loss: 303.3992\n",
      "Epoch 10/50\n",
      "865/865 - 7s - loss: 136.8266 - val_loss: 301.9944\n",
      "Epoch 11/50\n",
      "865/865 - 7s - loss: 134.1913 - val_loss: 297.7271\n",
      "Epoch 12/50\n",
      "865/865 - 7s - loss: 132.2924 - val_loss: 292.3970\n",
      "Epoch 13/50\n",
      "865/865 - 7s - loss: 130.9341 - val_loss: 292.5882\n",
      "Epoch 14/50\n",
      "865/865 - 6s - loss: 129.8660 - val_loss: 291.9521\n",
      "Epoch 15/50\n",
      "865/865 - 7s - loss: 128.7358 - val_loss: 283.4438\n",
      "Epoch 16/50\n",
      "865/865 - 7s - loss: 127.9368 - val_loss: 279.4087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "865/865 - 7s - loss: 127.2687 - val_loss: 279.4161\n",
      "Epoch 18/50\n",
      "865/865 - 7s - loss: 126.7146 - val_loss: 278.4604\n",
      "Epoch 19/50\n",
      "865/865 - 7s - loss: 126.2888 - val_loss: 275.6756\n",
      "Epoch 20/50\n",
      "865/865 - 7s - loss: 125.8923 - val_loss: 272.7062\n",
      "Epoch 21/50\n",
      "865/865 - 7s - loss: 125.4150 - val_loss: 271.6313\n",
      "Epoch 22/50\n",
      "865/865 - 7s - loss: 125.1102 - val_loss: 270.9930\n",
      "Epoch 23/50\n",
      "865/865 - 7s - loss: 124.9182 - val_loss: 270.6113\n",
      "Epoch 24/50\n",
      "865/865 - 7s - loss: 124.5282 - val_loss: 269.2213\n",
      "Epoch 25/50\n",
      "865/865 - 7s - loss: 124.1828 - val_loss: 265.0808\n",
      "Epoch 26/50\n",
      "865/865 - 7s - loss: 123.8461 - val_loss: 265.5946\n",
      "Epoch 27/50\n",
      "865/865 - 7s - loss: 123.7095 - val_loss: 262.4717\n",
      "Epoch 28/50\n",
      "865/865 - 7s - loss: 123.3394 - val_loss: 262.6719\n",
      "Epoch 29/50\n",
      "865/865 - 7s - loss: 123.2001 - val_loss: 258.2931\n",
      "Epoch 30/50\n",
      "865/865 - 7s - loss: 122.8858 - val_loss: 260.0453\n",
      "Epoch 31/50\n",
      "865/865 - 7s - loss: 122.7181 - val_loss: 257.4184\n",
      "Epoch 32/50\n",
      "865/865 - 7s - loss: 122.5603 - val_loss: 257.2970\n",
      "Epoch 33/50\n",
      "865/865 - 7s - loss: 122.3128 - val_loss: 255.6839\n",
      "Epoch 34/50\n",
      "865/865 - 7s - loss: 122.2947 - val_loss: 255.1412\n",
      "Epoch 35/50\n",
      "865/865 - 7s - loss: 122.1675 - val_loss: 251.9679\n",
      "Epoch 36/50\n",
      "865/865 - 7s - loss: 121.8882 - val_loss: 252.8180\n",
      "Epoch 37/50\n",
      "865/865 - 7s - loss: 121.7337 - val_loss: 250.5831\n",
      "Epoch 38/50\n",
      "865/865 - 7s - loss: 121.5592 - val_loss: 249.1316\n",
      "Epoch 39/50\n",
      "865/865 - 7s - loss: 121.3739 - val_loss: 246.4161\n",
      "Epoch 40/50\n",
      "865/865 - 7s - loss: 121.2985 - val_loss: 247.7653\n",
      "Epoch 41/50\n",
      "865/865 - 7s - loss: 121.0274 - val_loss: 246.7923\n",
      "Epoch 42/50\n",
      "865/865 - 7s - loss: 121.0593 - val_loss: 244.9101\n",
      "Epoch 43/50\n",
      "865/865 - 6s - loss: 120.8694 - val_loss: 247.8888\n",
      "Epoch 44/50\n",
      "865/865 - 7s - loss: 120.8685 - val_loss: 244.1468\n",
      "Epoch 45/50\n",
      "865/865 - 7s - loss: 120.6435 - val_loss: 246.5139\n",
      "Epoch 46/50\n",
      "865/865 - 7s - loss: 120.4416 - val_loss: 243.3912\n",
      "Epoch 47/50\n",
      "865/865 - 7s - loss: 120.4648 - val_loss: 245.3800\n",
      "Epoch 48/50\n",
      "865/865 - 7s - loss: 120.3528 - val_loss: 241.8856\n",
      "Epoch 49/50\n",
      "865/865 - 7s - loss: 120.3536 - val_loss: 240.2674\n",
      "Epoch 50/50\n",
      "865/865 - 7s - loss: 120.1636 - val_loss: 238.9353\n",
      "Training time for iteration 8: 328.07579135894775\n",
      "\n",
      "\n",
      "Iteration  9\n",
      "Layer size = 300, activation = tanh, batch size = 1024, dropout = 0.1\n",
      "Epoch 1/50\n",
      "3460/3460 - 10s - loss: 336.8546 - val_loss: 338.7199\n",
      "Epoch 2/50\n",
      "3460/3460 - 10s - loss: 172.8477 - val_loss: 320.3813\n",
      "Epoch 3/50\n",
      "3460/3460 - 10s - loss: 139.6334 - val_loss: 322.5223\n",
      "Epoch 4/50\n",
      "3460/3460 - 10s - loss: 128.1857 - val_loss: 316.5503\n",
      "Epoch 5/50\n",
      "3460/3460 - 10s - loss: 122.5539 - val_loss: 306.1793\n",
      "Epoch 6/50\n",
      "3460/3460 - 10s - loss: 119.1013 - val_loss: 300.6528\n",
      "Epoch 7/50\n",
      "3460/3460 - 10s - loss: 116.7209 - val_loss: 293.5476\n",
      "Epoch 8/50\n",
      "3460/3460 - 10s - loss: 114.8301 - val_loss: 287.0671\n",
      "Epoch 9/50\n",
      "3460/3460 - 10s - loss: 113.6525 - val_loss: 277.5250\n",
      "Epoch 10/50\n",
      "3460/3460 - 10s - loss: 112.6808 - val_loss: 275.2774\n",
      "Epoch 11/50\n",
      "3460/3460 - 10s - loss: 111.7413 - val_loss: 267.6008\n",
      "Epoch 12/50\n",
      "3460/3460 - 10s - loss: 111.0178 - val_loss: 267.7070\n",
      "Epoch 13/50\n",
      "3460/3460 - 10s - loss: 110.3396 - val_loss: 265.5702\n",
      "Epoch 14/50\n",
      "3460/3460 - 10s - loss: 109.8487 - val_loss: 259.2639\n",
      "Epoch 15/50\n",
      "3460/3460 - 10s - loss: 109.3275 - val_loss: 254.6730\n",
      "Epoch 16/50\n",
      "3460/3460 - 10s - loss: 108.9437 - val_loss: 248.6427\n",
      "Epoch 17/50\n",
      "3460/3460 - 10s - loss: 108.6895 - val_loss: 247.9980\n",
      "Epoch 18/50\n",
      "3460/3460 - 10s - loss: 108.3578 - val_loss: 247.2679\n",
      "Epoch 19/50\n",
      "3460/3460 - 10s - loss: 108.0924 - val_loss: 243.4600\n",
      "Epoch 20/50\n",
      "3460/3460 - 10s - loss: 107.7304 - val_loss: 240.2336\n",
      "Epoch 21/50\n",
      "3460/3460 - 10s - loss: 107.5187 - val_loss: 238.6079\n",
      "Epoch 22/50\n",
      "3460/3460 - 10s - loss: 107.1804 - val_loss: 238.6303\n",
      "Epoch 23/50\n",
      "3460/3460 - 10s - loss: 106.9712 - val_loss: 235.2510\n",
      "Epoch 24/50\n",
      "3460/3460 - 10s - loss: 106.7867 - val_loss: 236.5208\n",
      "Epoch 25/50\n",
      "3460/3460 - 10s - loss: 106.5137 - val_loss: 232.3671\n",
      "Epoch 26/50\n",
      "3460/3460 - 10s - loss: 106.3765 - val_loss: 233.1996\n",
      "Epoch 27/50\n",
      "3460/3460 - 10s - loss: 106.1575 - val_loss: 226.4642\n",
      "Epoch 28/50\n",
      "3460/3460 - 10s - loss: 106.0477 - val_loss: 226.9122\n",
      "Epoch 29/50\n",
      "3460/3460 - 10s - loss: 105.8555 - val_loss: 221.4356\n",
      "Epoch 30/50\n",
      "3460/3460 - 10s - loss: 105.6131 - val_loss: 221.7092\n",
      "Epoch 31/50\n",
      "3460/3460 - 10s - loss: 105.4331 - val_loss: 222.8122\n",
      "Epoch 32/50\n",
      "3460/3460 - 10s - loss: 105.3147 - val_loss: 224.1877\n",
      "Epoch 33/50\n",
      "3460/3460 - 10s - loss: 105.1733 - val_loss: 224.6463\n",
      "Epoch 34/50\n",
      "3460/3460 - 10s - loss: 105.0499 - val_loss: 216.3407\n",
      "Epoch 35/50\n",
      "3460/3460 - 10s - loss: 104.9095 - val_loss: 217.5272\n",
      "Epoch 36/50\n",
      "3460/3460 - 10s - loss: 104.7975 - val_loss: 216.5510\n",
      "Epoch 37/50\n",
      "3460/3460 - 10s - loss: 104.5886 - val_loss: 214.1148\n",
      "Epoch 38/50\n",
      "3460/3460 - 10s - loss: 104.5302 - val_loss: 217.9635\n",
      "Epoch 39/50\n",
      "3460/3460 - 10s - loss: 104.3512 - val_loss: 213.6175\n",
      "Epoch 40/50\n",
      "3460/3460 - 10s - loss: 104.2964 - val_loss: 210.6058\n",
      "Epoch 41/50\n",
      "3460/3460 - 10s - loss: 104.1787 - val_loss: 214.6813\n",
      "Epoch 42/50\n",
      "3460/3460 - 10s - loss: 104.0720 - val_loss: 212.2101\n",
      "Epoch 43/50\n",
      "3460/3460 - 10s - loss: 104.0449 - val_loss: 208.3911\n",
      "Epoch 44/50\n",
      "3460/3460 - 10s - loss: 103.8239 - val_loss: 213.0877\n",
      "Epoch 45/50\n",
      "3460/3460 - 10s - loss: 103.8295 - val_loss: 214.8087\n",
      "Epoch 46/50\n",
      "3460/3460 - 10s - loss: 103.6786 - val_loss: 209.4164\n",
      "Epoch 47/50\n",
      "3460/3460 - 10s - loss: 103.6465 - val_loss: 207.5941\n",
      "Epoch 48/50\n",
      "3460/3460 - 10s - loss: 103.4020 - val_loss: 207.0041\n",
      "Epoch 49/50\n",
      "3460/3460 - 10s - loss: 103.2338 - val_loss: 203.4091\n",
      "Epoch 50/50\n",
      "3460/3460 - 10s - loss: 103.3525 - val_loss: 208.0650\n",
      "Training time for iteration 9: 515.7965867519379\n",
      "\n",
      "\n",
      "Iteration  10\n",
      "Layer size = 500, activation = tanh, batch size = 512, dropout = 0.0\n",
      "Epoch 1/50\n",
      "6920/6920 - 20s - loss: 231.7693 - val_loss: 321.1210\n",
      "Epoch 2/50\n",
      "6920/6920 - 20s - loss: 121.9369 - val_loss: 315.1052\n",
      "Epoch 3/50\n",
      "6920/6920 - 20s - loss: 107.3206 - val_loss: 287.0778\n",
      "Epoch 4/50\n",
      "6920/6920 - 20s - loss: 101.1867 - val_loss: 276.6325\n",
      "Epoch 5/50\n",
      "6920/6920 - 20s - loss: 97.1836 - val_loss: 265.6150\n",
      "Epoch 6/50\n",
      "6920/6920 - 20s - loss: 93.8378 - val_loss: 257.9619\n",
      "Epoch 7/50\n",
      "6920/6920 - 20s - loss: 91.0317 - val_loss: 247.9272\n",
      "Epoch 8/50\n",
      "6920/6920 - 20s - loss: 88.4942 - val_loss: 236.3872\n",
      "Epoch 9/50\n",
      "6920/6920 - 20s - loss: 86.2602 - val_loss: 230.2923\n",
      "Epoch 10/50\n",
      "6920/6920 - 20s - loss: 84.2513 - val_loss: 223.5249\n",
      "Epoch 11/50\n",
      "6920/6920 - 20s - loss: 82.4569 - val_loss: 215.4796\n",
      "Epoch 12/50\n",
      "6920/6920 - 20s - loss: 80.8382 - val_loss: 209.5522\n",
      "Epoch 13/50\n",
      "6920/6920 - 21s - loss: 79.4124 - val_loss: 197.8459\n",
      "Epoch 14/50\n",
      "6920/6920 - 21s - loss: 78.1189 - val_loss: 194.4813\n",
      "Epoch 15/50\n",
      "6920/6920 - 20s - loss: 76.8833 - val_loss: 190.3917\n",
      "Epoch 16/50\n",
      "6920/6920 - 20s - loss: 75.7468 - val_loss: 182.9006\n",
      "Epoch 17/50\n",
      "6920/6920 - 20s - loss: 74.7190 - val_loss: 182.3411\n",
      "Epoch 18/50\n",
      "6920/6920 - 20s - loss: 73.8960 - val_loss: 179.2898\n",
      "Epoch 19/50\n",
      "6920/6920 - 21s - loss: 73.1436 - val_loss: 175.8287\n",
      "Epoch 20/50\n",
      "6920/6920 - 21s - loss: 72.4696 - val_loss: 171.9844\n",
      "Epoch 21/50\n",
      "6920/6920 - 20s - loss: 71.7416 - val_loss: 167.2489\n",
      "Epoch 22/50\n",
      "6920/6920 - 21s - loss: 71.1634 - val_loss: 164.2866\n",
      "Epoch 23/50\n",
      "6920/6920 - 20s - loss: 70.5879 - val_loss: 171.4557\n",
      "Epoch 24/50\n",
      "6920/6920 - 21s - loss: 70.0762 - val_loss: 159.1940\n",
      "Epoch 25/50\n",
      "6920/6920 - 21s - loss: 69.5610 - val_loss: 163.6175\n",
      "Epoch 26/50\n",
      "6920/6920 - 20s - loss: 69.1096 - val_loss: 159.6256\n",
      "Epoch 27/50\n",
      "6920/6920 - 20s - loss: 68.6703 - val_loss: 155.0308\n",
      "Epoch 28/50\n",
      "6920/6920 - 20s - loss: 68.2640 - val_loss: 156.2464\n",
      "Epoch 29/50\n",
      "6920/6920 - 20s - loss: 67.8764 - val_loss: 156.1675\n",
      "Epoch 30/50\n",
      "6920/6920 - 20s - loss: 67.5479 - val_loss: 153.2474\n",
      "Epoch 31/50\n",
      "6920/6920 - 21s - loss: 67.1219 - val_loss: 150.2237\n",
      "Epoch 32/50\n",
      "6920/6920 - 20s - loss: 66.7976 - val_loss: 146.7830\n",
      "Epoch 33/50\n",
      "6920/6920 - 20s - loss: 66.5205 - val_loss: 150.3122\n",
      "Epoch 34/50\n",
      "6920/6920 - 20s - loss: 66.2759 - val_loss: 147.9642\n",
      "Epoch 35/50\n",
      "6920/6920 - 20s - loss: 66.0254 - val_loss: 148.0363\n",
      "Epoch 36/50\n",
      "6920/6920 - 21s - loss: 65.6780 - val_loss: 145.3388\n",
      "Epoch 37/50\n",
      "6920/6920 - 21s - loss: 65.4403 - val_loss: 148.7387\n",
      "Epoch 38/50\n",
      "6920/6920 - 20s - loss: 65.2268 - val_loss: 148.1288\n",
      "Epoch 39/50\n",
      "6920/6920 - 20s - loss: 64.9569 - val_loss: 145.9017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "6920/6920 - 21s - loss: 64.7588 - val_loss: 142.4423\n",
      "Epoch 41/50\n",
      "6920/6920 - 20s - loss: 64.5322 - val_loss: 145.5239\n",
      "Epoch 42/50\n",
      "6920/6920 - 20s - loss: 64.3905 - val_loss: 143.3393\n",
      "Epoch 43/50\n",
      "6920/6920 - 20s - loss: 64.1726 - val_loss: 142.6071\n",
      "Epoch 44/50\n",
      "6920/6920 - 20s - loss: 64.0193 - val_loss: 140.5734\n",
      "Epoch 45/50\n",
      "6920/6920 - 20s - loss: 63.8755 - val_loss: 140.9116\n",
      "Epoch 46/50\n",
      "6920/6920 - 21s - loss: 63.5885 - val_loss: 138.8769\n",
      "Epoch 47/50\n",
      "6920/6920 - 20s - loss: 63.4284 - val_loss: 143.9260\n",
      "Epoch 48/50\n",
      "6920/6920 - 21s - loss: 63.2656 - val_loss: 138.9468\n",
      "Epoch 49/50\n",
      "6920/6920 - 21s - loss: 63.0978 - val_loss: 139.1892\n",
      "Epoch 50/50\n",
      "6920/6920 - 20s - loss: 62.9506 - val_loss: 141.3914\n",
      "Training time for iteration 10: 1021.9127204418182\n",
      "\n",
      "\n",
      "Iteration  11\n",
      "Layer size = 50, activation = relu, batch size = 1024, dropout = 0.3\n",
      "Epoch 1/50\n",
      "3460/3460 - 11s - loss: 511.3810 - val_loss: 363.2689\n",
      "Epoch 2/50\n",
      "3460/3460 - 10s - loss: 279.4815 - val_loss: 349.5229\n",
      "Epoch 3/50\n",
      "3460/3460 - 10s - loss: 240.6433 - val_loss: 337.6610\n",
      "Epoch 4/50\n",
      "3460/3460 - 10s - loss: 210.7245 - val_loss: 330.7135\n",
      "Epoch 5/50\n",
      "3460/3460 - 10s - loss: 191.6793 - val_loss: 323.1603\n",
      "Epoch 6/50\n",
      "3460/3460 - 10s - loss: 181.3530 - val_loss: 316.6496\n",
      "Epoch 7/50\n",
      "3460/3460 - 10s - loss: 174.4282 - val_loss: 312.9298\n",
      "Epoch 8/50\n",
      "3460/3460 - 10s - loss: 170.3604 - val_loss: 309.9034\n",
      "Epoch 9/50\n",
      "3460/3460 - 10s - loss: 167.4623 - val_loss: 304.9862\n",
      "Epoch 10/50\n",
      "3460/3460 - 10s - loss: 164.5919 - val_loss: 304.4627\n",
      "Epoch 11/50\n",
      "3460/3460 - 10s - loss: 162.6569 - val_loss: 298.7661\n",
      "Epoch 12/50\n",
      "3460/3460 - 10s - loss: 161.1994 - val_loss: 297.2417\n",
      "Epoch 13/50\n",
      "3460/3460 - 10s - loss: 160.2789 - val_loss: 297.1258\n",
      "Epoch 14/50\n",
      "3460/3460 - 10s - loss: 159.8351 - val_loss: 296.6345\n",
      "Epoch 15/50\n",
      "3460/3460 - 10s - loss: 159.2088 - val_loss: 292.2307\n",
      "Epoch 16/50\n",
      "3460/3460 - 10s - loss: 159.2790 - val_loss: 288.4529\n",
      "Epoch 17/50\n",
      "3460/3460 - 10s - loss: 158.5154 - val_loss: 290.5841\n",
      "Epoch 18/50\n",
      "3460/3460 - 10s - loss: 158.1888 - val_loss: 290.8094\n",
      "Epoch 19/50\n",
      "3460/3460 - 10s - loss: 157.4489 - val_loss: 288.3405\n",
      "Epoch 20/50\n",
      "3460/3460 - 10s - loss: 157.2104 - val_loss: 287.6494\n",
      "Epoch 21/50\n",
      "3460/3460 - 10s - loss: 156.5210 - val_loss: 285.4271\n",
      "Epoch 22/50\n",
      "3460/3460 - 10s - loss: 156.2901 - val_loss: 283.3883\n",
      "Epoch 23/50\n",
      "3460/3460 - 10s - loss: 155.9201 - val_loss: 284.3693\n",
      "Epoch 24/50\n",
      "3460/3460 - 10s - loss: 155.6846 - val_loss: 284.7710\n",
      "Epoch 25/50\n",
      "3460/3460 - 10s - loss: 155.3668 - val_loss: 280.4808\n",
      "Epoch 26/50\n",
      "3460/3460 - 10s - loss: 154.9922 - val_loss: 282.7000\n",
      "Epoch 27/50\n",
      "3460/3460 - 10s - loss: 154.7652 - val_loss: 279.1244\n",
      "Epoch 28/50\n",
      "3460/3460 - 10s - loss: 154.6633 - val_loss: 279.3831\n",
      "Epoch 29/50\n",
      "3460/3460 - 10s - loss: 154.5934 - val_loss: 278.5700\n",
      "Epoch 30/50\n",
      "3460/3460 - 10s - loss: 154.3357 - val_loss: 278.4446\n",
      "Epoch 31/50\n",
      "3460/3460 - 10s - loss: 154.2030 - val_loss: 277.9053\n",
      "Epoch 32/50\n",
      "3460/3460 - 10s - loss: 154.1164 - val_loss: 278.6098\n",
      "Epoch 33/50\n",
      "3460/3460 - 10s - loss: 154.0688 - val_loss: 276.5413\n",
      "Epoch 34/50\n",
      "3460/3460 - 10s - loss: 153.7776 - val_loss: 275.7701\n",
      "Epoch 35/50\n",
      "3460/3460 - 10s - loss: 153.9122 - val_loss: 274.5126\n",
      "Epoch 36/50\n",
      "3460/3460 - 10s - loss: 153.8124 - val_loss: 273.4430\n",
      "Epoch 37/50\n",
      "3460/3460 - 10s - loss: 153.7883 - val_loss: 272.9683\n",
      "Epoch 38/50\n",
      "3460/3460 - 10s - loss: 153.6129 - val_loss: 274.5002\n",
      "Epoch 39/50\n",
      "3460/3460 - 10s - loss: 153.7084 - val_loss: 270.1672\n",
      "Epoch 40/50\n",
      "3460/3460 - 10s - loss: 153.4860 - val_loss: 271.6964\n",
      "Epoch 41/50\n",
      "3460/3460 - 10s - loss: 153.5154 - val_loss: 270.2743\n",
      "Epoch 42/50\n",
      "3460/3460 - 10s - loss: 153.3548 - val_loss: 269.9102\n",
      "Epoch 43/50\n",
      "3460/3460 - 10s - loss: 153.2251 - val_loss: 270.6925\n",
      "Epoch 44/50\n",
      "3460/3460 - 10s - loss: 153.2584 - val_loss: 271.2842\n",
      "Epoch 45/50\n",
      "3460/3460 - 10s - loss: 153.2804 - val_loss: 272.8581\n",
      "Epoch 46/50\n",
      "3460/3460 - 10s - loss: 153.1731 - val_loss: 273.3129\n",
      "Epoch 47/50\n",
      "3460/3460 - 10s - loss: 153.2615 - val_loss: 270.6365\n",
      "Epoch 48/50\n",
      "3460/3460 - 10s - loss: 153.2990 - val_loss: 268.1163\n",
      "Epoch 49/50\n",
      "3460/3460 - 10s - loss: 153.2436 - val_loss: 268.8770\n",
      "Epoch 50/50\n",
      "3460/3460 - 10s - loss: 153.1524 - val_loss: 269.5283\n",
      "Training time for iteration 11: 497.770681142807\n",
      "\n",
      "\n",
      "Iteration  12\n",
      "Layer size = 500, activation = relu, batch size = 4096, dropout = 0.3\n",
      "Epoch 1/50\n",
      "865/865 - 7s - loss: 547.9774 - val_loss: 365.2718\n",
      "Epoch 2/50\n",
      "865/865 - 7s - loss: 253.1807 - val_loss: 347.9585\n",
      "Epoch 3/50\n",
      "865/865 - 7s - loss: 218.4106 - val_loss: 337.2543\n",
      "Epoch 4/50\n",
      "865/865 - 7s - loss: 190.6077 - val_loss: 324.5139\n",
      "Epoch 5/50\n",
      "865/865 - 7s - loss: 168.7657 - val_loss: 318.6198\n",
      "Epoch 6/50\n",
      "865/865 - 7s - loss: 154.8184 - val_loss: 312.4328\n",
      "Epoch 7/50\n",
      "865/865 - 7s - loss: 145.7574 - val_loss: 310.1704\n",
      "Epoch 8/50\n",
      "865/865 - 7s - loss: 139.8436 - val_loss: 307.5211\n",
      "Epoch 9/50\n",
      "865/865 - 7s - loss: 135.4878 - val_loss: 304.6747\n",
      "Epoch 10/50\n",
      "865/865 - 7s - loss: 132.1493 - val_loss: 303.1159\n",
      "Epoch 11/50\n",
      "865/865 - 7s - loss: 129.2038 - val_loss: 297.4615\n",
      "Epoch 12/50\n",
      "865/865 - 7s - loss: 126.8511 - val_loss: 290.6051\n",
      "Epoch 13/50\n",
      "865/865 - 7s - loss: 124.9887 - val_loss: 291.2055\n",
      "Epoch 14/50\n",
      "865/865 - 7s - loss: 123.5074 - val_loss: 286.8955\n",
      "Epoch 15/50\n",
      "865/865 - 7s - loss: 122.4107 - val_loss: 277.2349\n",
      "Epoch 16/50\n",
      "865/865 - 7s - loss: 121.5260 - val_loss: 274.6339\n",
      "Epoch 17/50\n",
      "865/865 - 7s - loss: 120.8096 - val_loss: 273.2000\n",
      "Epoch 18/50\n",
      "865/865 - 7s - loss: 120.2924 - val_loss: 271.7165\n",
      "Epoch 19/50\n",
      "865/865 - 7s - loss: 119.7456 - val_loss: 268.6320\n",
      "Epoch 20/50\n",
      "865/865 - 7s - loss: 119.3007 - val_loss: 267.5957\n",
      "Epoch 21/50\n",
      "865/865 - 7s - loss: 118.7232 - val_loss: 265.4379\n",
      "Epoch 22/50\n",
      "865/865 - 7s - loss: 118.3496 - val_loss: 263.9247\n",
      "Epoch 23/50\n",
      "865/865 - 7s - loss: 118.2122 - val_loss: 262.2351\n",
      "Epoch 24/50\n",
      "865/865 - 7s - loss: 117.8172 - val_loss: 261.8430\n",
      "Epoch 25/50\n",
      "865/865 - 7s - loss: 117.3861 - val_loss: 259.1633\n",
      "Epoch 26/50\n",
      "865/865 - 7s - loss: 117.1990 - val_loss: 258.1815\n",
      "Epoch 27/50\n",
      "865/865 - 7s - loss: 116.8725 - val_loss: 256.0658\n",
      "Epoch 28/50\n",
      "865/865 - 7s - loss: 116.4622 - val_loss: 255.4586\n",
      "Epoch 29/50\n",
      "865/865 - 7s - loss: 116.2983 - val_loss: 252.6183\n",
      "Epoch 30/50\n",
      "865/865 - 7s - loss: 116.0200 - val_loss: 252.8414\n",
      "Epoch 31/50\n",
      "865/865 - 7s - loss: 115.8298 - val_loss: 249.9980\n",
      "Epoch 32/50\n",
      "865/865 - 7s - loss: 115.5745 - val_loss: 250.5259\n",
      "Epoch 33/50\n",
      "865/865 - 7s - loss: 115.4765 - val_loss: 248.5064\n",
      "Epoch 34/50\n",
      "865/865 - 7s - loss: 115.1884 - val_loss: 246.8592\n",
      "Epoch 35/50\n",
      "865/865 - 7s - loss: 115.1797 - val_loss: 244.4720\n",
      "Epoch 36/50\n",
      "865/865 - 7s - loss: 114.8714 - val_loss: 246.6068\n",
      "Epoch 37/50\n",
      "865/865 - 7s - loss: 114.7689 - val_loss: 245.2485\n",
      "Epoch 38/50\n",
      "865/865 - 7s - loss: 114.6884 - val_loss: 242.2353\n",
      "Epoch 39/50\n",
      "865/865 - 7s - loss: 114.3846 - val_loss: 239.1600\n",
      "Epoch 40/50\n",
      "865/865 - 7s - loss: 114.1885 - val_loss: 240.0344\n",
      "Epoch 41/50\n",
      "865/865 - 7s - loss: 113.9000 - val_loss: 240.2073\n",
      "Epoch 42/50\n",
      "865/865 - 7s - loss: 113.8529 - val_loss: 236.7519\n",
      "Epoch 43/50\n",
      "865/865 - 7s - loss: 113.6920 - val_loss: 240.1419\n",
      "Epoch 44/50\n",
      "865/865 - 7s - loss: 113.5997 - val_loss: 237.7466\n",
      "Epoch 45/50\n",
      "865/865 - 7s - loss: 113.4594 - val_loss: 237.8752\n",
      "Epoch 46/50\n",
      "865/865 - 7s - loss: 113.2591 - val_loss: 236.0193\n",
      "Epoch 47/50\n",
      "865/865 - 7s - loss: 112.9859 - val_loss: 234.8380\n",
      "Epoch 48/50\n",
      "865/865 - 7s - loss: 112.9785 - val_loss: 233.0374\n",
      "Epoch 49/50\n",
      "865/865 - 7s - loss: 112.7183 - val_loss: 232.2841\n",
      "Epoch 50/50\n",
      "865/865 - 7s - loss: 112.7295 - val_loss: 231.8862\n",
      "Training time for iteration 12: 331.6824667453766\n",
      "\n",
      "\n",
      "Iteration  13\n",
      "Layer size = 500, activation = tanh, batch size = 2048, dropout = 0.4\n",
      "Epoch 1/50\n",
      "1730/1730 - 9s - loss: 407.5378 - val_loss: 351.0636\n",
      "Epoch 2/50\n",
      "1730/1730 - 9s - loss: 218.4239 - val_loss: 330.8994\n",
      "Epoch 3/50\n",
      "1730/1730 - 9s - loss: 176.3120 - val_loss: 321.1332\n",
      "Epoch 4/50\n",
      "1730/1730 - 9s - loss: 153.0520 - val_loss: 319.1274\n",
      "Epoch 5/50\n",
      "1730/1730 - 9s - loss: 142.9829 - val_loss: 315.3826\n",
      "Epoch 6/50\n",
      "1730/1730 - 9s - loss: 137.5278 - val_loss: 308.1958\n",
      "Epoch 7/50\n",
      "1730/1730 - 9s - loss: 133.8898 - val_loss: 305.6458\n",
      "Epoch 8/50\n",
      "1730/1730 - 9s - loss: 131.3601 - val_loss: 303.6567\n",
      "Epoch 9/50\n",
      "1730/1730 - 9s - loss: 129.8244 - val_loss: 294.2265\n",
      "Epoch 10/50\n",
      "1730/1730 - 9s - loss: 128.5246 - val_loss: 291.8338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "1730/1730 - 9s - loss: 127.7338 - val_loss: 289.9198\n",
      "Epoch 12/50\n",
      "1730/1730 - 9s - loss: 127.0535 - val_loss: 287.9686\n",
      "Epoch 13/50\n",
      "1730/1730 - 9s - loss: 126.5704 - val_loss: 285.2843\n",
      "Epoch 14/50\n",
      "1730/1730 - 9s - loss: 126.0587 - val_loss: 283.3799\n",
      "Epoch 15/50\n",
      "1730/1730 - 9s - loss: 125.5129 - val_loss: 278.1486\n",
      "Epoch 16/50\n",
      "1730/1730 - 9s - loss: 125.0910 - val_loss: 274.7921\n",
      "Epoch 17/50\n",
      "1730/1730 - 9s - loss: 124.8204 - val_loss: 274.9618\n",
      "Epoch 18/50\n",
      "1730/1730 - 9s - loss: 124.4067 - val_loss: 275.6575\n",
      "Epoch 19/50\n",
      "1730/1730 - 9s - loss: 124.0489 - val_loss: 272.2033\n",
      "Epoch 20/50\n",
      "1730/1730 - 9s - loss: 123.7300 - val_loss: 270.8949\n",
      "Epoch 21/50\n",
      "1730/1730 - 9s - loss: 123.4734 - val_loss: 267.9414\n",
      "Epoch 22/50\n",
      "1730/1730 - 9s - loss: 123.1490 - val_loss: 264.7600\n",
      "Epoch 23/50\n",
      "1730/1730 - 9s - loss: 122.8695 - val_loss: 266.6800\n",
      "Epoch 24/50\n",
      "1730/1730 - 9s - loss: 122.4853 - val_loss: 262.4298\n",
      "Epoch 25/50\n",
      "1730/1730 - 9s - loss: 122.3373 - val_loss: 258.6313\n",
      "Epoch 26/50\n",
      "1730/1730 - 9s - loss: 121.9945 - val_loss: 261.2613\n",
      "Epoch 27/50\n",
      "1730/1730 - 9s - loss: 121.9523 - val_loss: 256.9707\n",
      "Epoch 28/50\n",
      "1730/1730 - 9s - loss: 121.6409 - val_loss: 258.1805\n",
      "Epoch 29/50\n",
      "1730/1730 - 9s - loss: 121.5798 - val_loss: 253.3353\n",
      "Epoch 30/50\n",
      "1730/1730 - 9s - loss: 121.3142 - val_loss: 252.4791\n",
      "Epoch 31/50\n",
      "1730/1730 - 9s - loss: 121.1705 - val_loss: 252.2542\n",
      "Epoch 32/50\n",
      "1730/1730 - 9s - loss: 121.0798 - val_loss: 256.2100\n",
      "Epoch 33/50\n",
      "1730/1730 - 9s - loss: 120.8261 - val_loss: 253.4742\n",
      "Epoch 34/50\n",
      "1730/1730 - 9s - loss: 120.7872 - val_loss: 251.8376\n",
      "Epoch 35/50\n",
      "1730/1730 - 9s - loss: 120.4759 - val_loss: 247.4733\n",
      "Epoch 36/50\n",
      "1730/1730 - 9s - loss: 120.4524 - val_loss: 248.3804\n",
      "Epoch 37/50\n",
      "1730/1730 - 9s - loss: 120.4702 - val_loss: 244.6836\n",
      "Epoch 38/50\n",
      "1730/1730 - 9s - loss: 120.2458 - val_loss: 248.2972\n",
      "Epoch 39/50\n",
      "1730/1730 - 9s - loss: 120.1353 - val_loss: 243.0200\n",
      "Epoch 40/50\n",
      "1730/1730 - 9s - loss: 120.0412 - val_loss: 244.4975\n",
      "Epoch 41/50\n",
      "1730/1730 - 9s - loss: 119.8427 - val_loss: 241.5242\n",
      "Epoch 42/50\n",
      "1730/1730 - 9s - loss: 119.8866 - val_loss: 240.4413\n",
      "Epoch 43/50\n",
      "1730/1730 - 9s - loss: 119.7661 - val_loss: 241.0710\n",
      "Epoch 44/50\n",
      "1730/1730 - 9s - loss: 119.5397 - val_loss: 243.3284\n",
      "Epoch 45/50\n",
      "1730/1730 - 9s - loss: 119.4114 - val_loss: 243.3888\n",
      "Epoch 46/50\n",
      "1730/1730 - 9s - loss: 119.3159 - val_loss: 240.8909\n",
      "Epoch 47/50\n",
      "1730/1730 - 9s - loss: 119.2467 - val_loss: 240.3255\n",
      "Epoch 48/50\n",
      "1730/1730 - 9s - loss: 119.0589 - val_loss: 235.7470\n",
      "Epoch 49/50\n",
      "1730/1730 - 9s - loss: 119.0026 - val_loss: 234.4289\n",
      "Epoch 50/50\n",
      "1730/1730 - 9s - loss: 118.6687 - val_loss: 238.8139\n",
      "Training time for iteration 13: 456.88113474845886\n",
      "\n",
      "\n",
      "Iteration  14\n",
      "Layer size = 50, activation = sigmoid, batch size = 512, dropout = 0.4\n",
      "Epoch 1/50\n",
      "6920/6920 - 20s - loss: 412.5623 - val_loss: 357.7043\n",
      "Epoch 2/50\n",
      "6920/6920 - 19s - loss: 253.0104 - val_loss: 337.2849\n",
      "Epoch 3/50\n",
      "6920/6920 - 19s - loss: 210.1885 - val_loss: 324.4722\n",
      "Epoch 4/50\n",
      "6920/6920 - 19s - loss: 191.8870 - val_loss: 317.8315\n",
      "Epoch 5/50\n",
      "6920/6920 - 19s - loss: 183.4662 - val_loss: 309.3506\n",
      "Epoch 6/50\n",
      "6920/6920 - 20s - loss: 178.5862 - val_loss: 306.9555\n",
      "Epoch 7/50\n",
      "6920/6920 - 19s - loss: 176.5471 - val_loss: 304.4039\n",
      "Epoch 8/50\n",
      "6920/6920 - 19s - loss: 175.2958 - val_loss: 306.8774\n",
      "Epoch 9/50\n",
      "6920/6920 - 19s - loss: 174.4251 - val_loss: 299.7878\n",
      "Epoch 10/50\n",
      "6920/6920 - 19s - loss: 173.6832 - val_loss: 301.0804\n",
      "Epoch 11/50\n",
      "6920/6920 - 19s - loss: 172.9810 - val_loss: 295.6566\n",
      "Epoch 12/50\n",
      "6920/6920 - 19s - loss: 172.3471 - val_loss: 300.4328\n",
      "Epoch 13/50\n",
      "6920/6920 - 19s - loss: 172.3105 - val_loss: 298.4355\n",
      "Epoch 14/50\n",
      "6920/6920 - 19s - loss: 172.2025 - val_loss: 299.3065\n",
      "Epoch 15/50\n",
      "6920/6920 - 19s - loss: 171.9991 - val_loss: 294.7397\n",
      "Epoch 16/50\n",
      "6920/6920 - 19s - loss: 171.8466 - val_loss: 292.4326\n",
      "Epoch 17/50\n",
      "6920/6920 - 19s - loss: 171.8077 - val_loss: 294.9115\n",
      "Epoch 18/50\n",
      "6920/6920 - 19s - loss: 171.5747 - val_loss: 293.4709\n",
      "Epoch 19/50\n",
      "6920/6920 - 19s - loss: 171.5838 - val_loss: 293.0580\n",
      "Epoch 20/50\n",
      "6920/6920 - 19s - loss: 171.3745 - val_loss: 293.7235\n",
      "Epoch 21/50\n",
      "6920/6920 - 19s - loss: 171.3192 - val_loss: 291.9803\n",
      "Epoch 22/50\n",
      "6920/6920 - 19s - loss: 171.2465 - val_loss: 291.6490\n",
      "Epoch 23/50\n",
      "6920/6920 - 19s - loss: 170.9779 - val_loss: 291.5294\n",
      "Epoch 24/50\n",
      "6920/6920 - 19s - loss: 170.9714 - val_loss: 290.3369\n",
      "Epoch 25/50\n",
      "6920/6920 - 19s - loss: 170.9962 - val_loss: 286.8195\n",
      "Epoch 26/50\n",
      "6920/6920 - 19s - loss: 170.7862 - val_loss: 287.9917\n",
      "Epoch 27/50\n",
      "6920/6920 - 19s - loss: 170.9364 - val_loss: 285.7024\n",
      "Epoch 28/50\n",
      "6920/6920 - 19s - loss: 170.7081 - val_loss: 285.8853\n",
      "Epoch 29/50\n",
      "6920/6920 - 19s - loss: 170.5583 - val_loss: 282.9616\n",
      "Epoch 30/50\n",
      "6920/6920 - 20s - loss: 170.4053 - val_loss: 282.7891\n",
      "Epoch 31/50\n",
      "6920/6920 - 19s - loss: 170.4469 - val_loss: 285.7059\n",
      "Epoch 32/50\n",
      "6920/6920 - 19s - loss: 170.3318 - val_loss: 285.2676\n",
      "Epoch 33/50\n",
      "6920/6920 - 19s - loss: 170.1575 - val_loss: 285.4075\n",
      "Epoch 34/50\n",
      "6920/6920 - 19s - loss: 170.3558 - val_loss: 281.5969\n",
      "Epoch 35/50\n",
      "6920/6920 - 19s - loss: 170.3847 - val_loss: 283.1142\n",
      "Epoch 36/50\n",
      "6920/6920 - 19s - loss: 170.0328 - val_loss: 280.8574\n",
      "Epoch 37/50\n",
      "6920/6920 - 19s - loss: 170.2711 - val_loss: 279.9547\n",
      "Epoch 38/50\n",
      "6920/6920 - 19s - loss: 170.0972 - val_loss: 282.4664\n",
      "Epoch 39/50\n",
      "6920/6920 - 19s - loss: 170.0927 - val_loss: 279.1515\n",
      "Epoch 40/50\n",
      "6920/6920 - 19s - loss: 170.2260 - val_loss: 282.1426\n",
      "Epoch 41/50\n",
      "6920/6920 - 19s - loss: 170.3393 - val_loss: 280.7303\n",
      "Epoch 42/50\n",
      "6920/6920 - 19s - loss: 169.9933 - val_loss: 279.5642\n",
      "Epoch 43/50\n",
      "6920/6920 - 19s - loss: 169.9591 - val_loss: 280.3102\n",
      "Epoch 44/50\n",
      "6920/6920 - 19s - loss: 170.1808 - val_loss: 280.0483\n",
      "Epoch 45/50\n",
      "6920/6920 - 19s - loss: 170.0007 - val_loss: 281.2399\n",
      "Epoch 46/50\n",
      "6920/6920 - 19s - loss: 170.2394 - val_loss: 281.7444\n",
      "Epoch 47/50\n",
      "6920/6920 - 19s - loss: 170.1138 - val_loss: 279.6846\n",
      "Epoch 48/50\n",
      "6920/6920 - 19s - loss: 170.0734 - val_loss: 279.2899\n",
      "Epoch 49/50\n",
      "6920/6920 - 19s - loss: 170.0156 - val_loss: 280.3875\n",
      "Epoch 50/50\n",
      "6920/6920 - 19s - loss: 170.0789 - val_loss: 281.4167\n",
      "Training time for iteration 14: 970.7434074878693\n",
      "\n",
      "\n",
      "Iteration  15\n",
      "Layer size = 200, activation = sigmoid, batch size = 2048, dropout = 0.0\n",
      "Epoch 1/50\n",
      "1730/1730 - 6s - loss: 476.3412 - val_loss: 361.4553\n",
      "Epoch 2/50\n",
      "1730/1730 - 6s - loss: 233.6664 - val_loss: 339.7858\n",
      "Epoch 3/50\n",
      "1730/1730 - 5s - loss: 181.8484 - val_loss: 323.9070\n",
      "Epoch 4/50\n",
      "1730/1730 - 6s - loss: 150.5767 - val_loss: 315.3221\n",
      "Epoch 5/50\n",
      "1730/1730 - 6s - loss: 135.8951 - val_loss: 310.1748\n",
      "Epoch 6/50\n",
      "1730/1730 - 6s - loss: 125.9024 - val_loss: 303.0709\n",
      "Epoch 7/50\n",
      "1730/1730 - 6s - loss: 119.0711 - val_loss: 299.9616\n",
      "Epoch 8/50\n",
      "1730/1730 - 6s - loss: 114.5555 - val_loss: 299.5171\n",
      "Epoch 9/50\n",
      "1730/1730 - 6s - loss: 110.8367 - val_loss: 292.2578\n",
      "Epoch 10/50\n",
      "1730/1730 - 6s - loss: 107.9438 - val_loss: 289.7113\n",
      "Epoch 11/50\n",
      "1730/1730 - 6s - loss: 105.7209 - val_loss: 286.4823\n",
      "Epoch 12/50\n",
      "1730/1730 - 6s - loss: 103.7884 - val_loss: 284.7175\n",
      "Epoch 13/50\n",
      "1730/1730 - 6s - loss: 102.1488 - val_loss: 279.7333\n",
      "Epoch 14/50\n",
      "1730/1730 - 6s - loss: 100.8520 - val_loss: 279.0420\n",
      "Epoch 15/50\n",
      "1730/1730 - 6s - loss: 99.7715 - val_loss: 272.3675\n",
      "Epoch 16/50\n",
      "1730/1730 - 6s - loss: 98.8077 - val_loss: 267.9346\n",
      "Epoch 17/50\n",
      "1730/1730 - 6s - loss: 97.8726 - val_loss: 267.8531\n",
      "Epoch 18/50\n",
      "1730/1730 - 6s - loss: 96.9699 - val_loss: 267.1161\n",
      "Epoch 19/50\n",
      "1730/1730 - 6s - loss: 96.2083 - val_loss: 262.7673\n",
      "Epoch 20/50\n",
      "1730/1730 - 6s - loss: 95.5034 - val_loss: 263.1438\n",
      "Epoch 21/50\n",
      "1730/1730 - 6s - loss: 94.8251 - val_loss: 260.3670\n",
      "Epoch 22/50\n",
      "1730/1730 - 6s - loss: 94.1598 - val_loss: 254.6507\n",
      "Epoch 23/50\n",
      "1730/1730 - 6s - loss: 93.5563 - val_loss: 258.3487\n",
      "Epoch 24/50\n",
      "1730/1730 - 5s - loss: 92.9215 - val_loss: 252.5944\n",
      "Epoch 25/50\n",
      "1730/1730 - 5s - loss: 92.3167 - val_loss: 247.0293\n",
      "Epoch 26/50\n",
      "1730/1730 - 6s - loss: 91.7933 - val_loss: 248.3439\n",
      "Epoch 27/50\n",
      "1730/1730 - 6s - loss: 91.2792 - val_loss: 245.8680\n",
      "Epoch 28/50\n",
      "1730/1730 - 6s - loss: 90.8032 - val_loss: 246.9430\n",
      "Epoch 29/50\n",
      "1730/1730 - 6s - loss: 90.3571 - val_loss: 242.7667\n",
      "Epoch 30/50\n",
      "1730/1730 - 6s - loss: 89.9365 - val_loss: 241.1149\n",
      "Epoch 31/50\n",
      "1730/1730 - 5s - loss: 89.5127 - val_loss: 240.1772\n",
      "Epoch 32/50\n",
      "1730/1730 - 6s - loss: 89.0806 - val_loss: 240.8934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "1730/1730 - 6s - loss: 88.5673 - val_loss: 237.2279\n",
      "Epoch 34/50\n",
      "1730/1730 - 6s - loss: 88.1909 - val_loss: 235.7367\n",
      "Epoch 35/50\n",
      "1730/1730 - 6s - loss: 87.7842 - val_loss: 233.2042\n",
      "Epoch 36/50\n",
      "1730/1730 - 6s - loss: 87.4117 - val_loss: 233.2148\n",
      "Epoch 37/50\n",
      "1730/1730 - 6s - loss: 87.0302 - val_loss: 229.0452\n",
      "Epoch 38/50\n",
      "1730/1730 - 6s - loss: 86.6606 - val_loss: 230.3887\n",
      "Epoch 39/50\n",
      "1730/1730 - 6s - loss: 86.3006 - val_loss: 224.5490\n",
      "Epoch 40/50\n",
      "1730/1730 - 6s - loss: 85.9295 - val_loss: 225.4094\n",
      "Epoch 41/50\n",
      "1730/1730 - 6s - loss: 85.6278 - val_loss: 222.7458\n",
      "Epoch 42/50\n",
      "1730/1730 - 6s - loss: 85.2894 - val_loss: 221.7805\n",
      "Epoch 43/50\n",
      "1730/1730 - 6s - loss: 84.9918 - val_loss: 221.9808\n",
      "Epoch 44/50\n",
      "1730/1730 - 6s - loss: 84.7212 - val_loss: 220.6304\n",
      "Epoch 45/50\n",
      "1730/1730 - 6s - loss: 84.4289 - val_loss: 221.3266\n",
      "Epoch 46/50\n",
      "1730/1730 - 6s - loss: 84.0913 - val_loss: 218.5683\n",
      "Epoch 47/50\n",
      "1730/1730 - 6s - loss: 83.8348 - val_loss: 216.2608\n",
      "Epoch 48/50\n",
      "1730/1730 - 6s - loss: 83.5562 - val_loss: 215.7536\n",
      "Epoch 49/50\n",
      "1730/1730 - 6s - loss: 83.2959 - val_loss: 213.4786\n",
      "Epoch 50/50\n",
      "1730/1730 - 6s - loss: 83.0300 - val_loss: 212.1415\n",
      "Training time for iteration 15: 281.97455978393555\n",
      "\n",
      "\n",
      "Iteration  16\n",
      "Layer size = 200, activation = tanh, batch size = 1024, dropout = 0.3\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0156s). Check your callbacks.\n",
      "3460/3460 - 11s - loss: 373.1942 - val_loss: 347.8951\n",
      "Epoch 2/50\n",
      "3460/3460 - 10s - loss: 207.0214 - val_loss: 329.1004\n",
      "Epoch 3/50\n",
      "3460/3460 - 10s - loss: 165.6927 - val_loss: 330.4921\n",
      "Epoch 4/50\n",
      "3460/3460 - 10s - loss: 151.1006 - val_loss: 326.1879\n",
      "Epoch 5/50\n",
      "3460/3460 - 10s - loss: 145.7971 - val_loss: 322.2362\n",
      "Epoch 6/50\n",
      "3460/3460 - 10s - loss: 142.7731 - val_loss: 316.3895\n",
      "Epoch 7/50\n",
      "3460/3460 - 10s - loss: 140.2929 - val_loss: 312.9848\n",
      "Epoch 8/50\n",
      "3460/3460 - 10s - loss: 138.5427 - val_loss: 310.8605\n",
      "Epoch 9/50\n",
      "3460/3460 - 10s - loss: 137.4568 - val_loss: 301.2616\n",
      "Epoch 10/50\n",
      "3460/3460 - 10s - loss: 136.4262 - val_loss: 301.4933\n",
      "Epoch 11/50\n",
      "3460/3460 - 10s - loss: 135.6699 - val_loss: 295.5053\n",
      "Epoch 12/50\n",
      "3460/3460 - 10s - loss: 135.2695 - val_loss: 298.3452\n",
      "Epoch 13/50\n",
      "3460/3460 - 10s - loss: 134.5617 - val_loss: 297.6164\n",
      "Epoch 14/50\n",
      "3460/3460 - 10s - loss: 134.2774 - val_loss: 294.3041\n",
      "Epoch 15/50\n",
      "3460/3460 - 10s - loss: 133.5540 - val_loss: 291.7584\n",
      "Epoch 16/50\n",
      "3460/3460 - 10s - loss: 133.2115 - val_loss: 284.7267\n",
      "Epoch 17/50\n",
      "3460/3460 - 10s - loss: 132.8666 - val_loss: 287.4329\n",
      "Epoch 18/50\n",
      "3460/3460 - 10s - loss: 132.3517 - val_loss: 286.3693\n",
      "Epoch 19/50\n",
      "3460/3460 - 10s - loss: 131.9633 - val_loss: 284.7267\n",
      "Epoch 20/50\n",
      "3460/3460 - 10s - loss: 131.5842 - val_loss: 283.8504\n",
      "Epoch 21/50\n",
      "3460/3460 - 10s - loss: 131.4659 - val_loss: 281.8336\n",
      "Epoch 22/50\n",
      "3460/3460 - 10s - loss: 131.3607 - val_loss: 282.2425\n",
      "Epoch 23/50\n",
      "3460/3460 - 10s - loss: 131.1448 - val_loss: 281.2090\n",
      "Epoch 24/50\n",
      "3460/3460 - 10s - loss: 131.0479 - val_loss: 280.1786\n",
      "Epoch 25/50\n",
      "3460/3460 - 10s - loss: 130.9590 - val_loss: 276.6368\n",
      "Epoch 26/50\n",
      "3460/3460 - 10s - loss: 130.7929 - val_loss: 275.7979\n",
      "Epoch 27/50\n",
      "3460/3460 - 10s - loss: 130.5811 - val_loss: 274.1058\n",
      "Epoch 28/50\n",
      "3460/3460 - 10s - loss: 130.5401 - val_loss: 276.2804\n",
      "Epoch 29/50\n",
      "3460/3460 - 10s - loss: 130.4416 - val_loss: 271.2843\n",
      "Epoch 30/50\n",
      "3460/3460 - 10s - loss: 130.2252 - val_loss: 270.7867\n",
      "Epoch 31/50\n",
      "3460/3460 - 10s - loss: 130.3429 - val_loss: 272.3263\n",
      "Epoch 32/50\n",
      "3460/3460 - 10s - loss: 129.9338 - val_loss: 273.2647\n",
      "Epoch 33/50\n",
      "3460/3460 - 10s - loss: 129.8115 - val_loss: 269.1807\n",
      "Epoch 34/50\n",
      "3460/3460 - 10s - loss: 129.5610 - val_loss: 267.7915\n",
      "Epoch 35/50\n",
      "3460/3460 - 10s - loss: 129.4546 - val_loss: 269.1064\n",
      "Epoch 36/50\n",
      "3460/3460 - 10s - loss: 129.3329 - val_loss: 266.7449\n",
      "Epoch 37/50\n",
      "3460/3460 - 10s - loss: 129.2226 - val_loss: 261.8565\n",
      "Epoch 38/50\n",
      "3460/3460 - 10s - loss: 129.0175 - val_loss: 266.6171\n",
      "Epoch 39/50\n",
      "3460/3460 - 10s - loss: 128.9593 - val_loss: 262.3107\n",
      "Epoch 40/50\n",
      "3460/3460 - 10s - loss: 128.9239 - val_loss: 260.8729\n",
      "Epoch 41/50\n",
      "3460/3460 - 10s - loss: 128.7581 - val_loss: 263.2978\n",
      "Epoch 42/50\n",
      "3460/3460 - 10s - loss: 128.6886 - val_loss: 259.7832\n",
      "Epoch 43/50\n",
      "3460/3460 - 10s - loss: 128.5025 - val_loss: 261.6740\n",
      "Epoch 44/50\n",
      "3460/3460 - 10s - loss: 128.4376 - val_loss: 259.3011\n",
      "Epoch 45/50\n",
      "3460/3460 - 10s - loss: 128.0632 - val_loss: 261.0820\n",
      "Epoch 46/50\n",
      "3460/3460 - 10s - loss: 127.9561 - val_loss: 259.1988\n",
      "Epoch 47/50\n",
      "3460/3460 - 10s - loss: 127.7364 - val_loss: 260.2241\n",
      "Epoch 48/50\n",
      "3460/3460 - 10s - loss: 127.5480 - val_loss: 256.3613\n",
      "Epoch 49/50\n",
      "3460/3460 - 10s - loss: 127.5672 - val_loss: 254.0310\n",
      "Epoch 50/50\n",
      "3460/3460 - 10s - loss: 127.4792 - val_loss: 256.8578\n",
      "Training time for iteration 16: 509.45870637893677\n",
      "\n",
      "\n",
      "Iteration  17\n",
      "Layer size = 50, activation = sigmoid, batch size = 2048, dropout = 0.4\n",
      "Epoch 1/50\n",
      "1730/1730 - 5s - loss: 677.0392 - val_loss: 378.0150\n",
      "Epoch 2/50\n",
      "1730/1730 - 5s - loss: 322.0358 - val_loss: 361.6584\n",
      "Epoch 3/50\n",
      "1730/1730 - 5s - loss: 291.5756 - val_loss: 354.7749\n",
      "Epoch 4/50\n",
      "1730/1730 - 5s - loss: 263.3406 - val_loss: 345.5485\n",
      "Epoch 5/50\n",
      "1730/1730 - 5s - loss: 238.8950 - val_loss: 337.6491\n",
      "Epoch 6/50\n",
      "1730/1730 - 5s - loss: 217.9902 - val_loss: 329.7777\n",
      "Epoch 7/50\n",
      "1730/1730 - 5s - loss: 204.0037 - val_loss: 326.4159\n",
      "Epoch 8/50\n",
      "1730/1730 - 5s - loss: 195.6678 - val_loss: 323.7575\n",
      "Epoch 9/50\n",
      "1730/1730 - 5s - loss: 190.2953 - val_loss: 319.7297\n",
      "Epoch 10/50\n",
      "1730/1730 - 5s - loss: 186.7243 - val_loss: 319.8629\n",
      "Epoch 11/50\n",
      "1730/1730 - 5s - loss: 183.2044 - val_loss: 314.4371\n",
      "Epoch 12/50\n",
      "1730/1730 - 5s - loss: 180.0235 - val_loss: 310.9327\n",
      "Epoch 13/50\n",
      "1730/1730 - 5s - loss: 177.9834 - val_loss: 311.9755\n",
      "Epoch 14/50\n",
      "1730/1730 - 5s - loss: 176.1323 - val_loss: 310.3776\n",
      "Epoch 15/50\n",
      "1730/1730 - 5s - loss: 175.0428 - val_loss: 306.2097\n",
      "Epoch 16/50\n",
      "1730/1730 - 5s - loss: 173.9066 - val_loss: 303.6369\n",
      "Epoch 17/50\n",
      "1730/1730 - 5s - loss: 173.0271 - val_loss: 304.2033\n",
      "Epoch 18/50\n",
      "1730/1730 - 5s - loss: 172.1900 - val_loss: 304.4271\n",
      "Epoch 19/50\n",
      "1730/1730 - 5s - loss: 171.5368 - val_loss: 301.4727\n",
      "Epoch 20/50\n",
      "1730/1730 - 5s - loss: 171.3696 - val_loss: 300.9174\n",
      "Epoch 21/50\n",
      "1730/1730 - 5s - loss: 171.0363 - val_loss: 301.7873\n",
      "Epoch 22/50\n",
      "1730/1730 - 5s - loss: 170.8694 - val_loss: 300.0758\n",
      "Epoch 23/50\n",
      "1730/1730 - 5s - loss: 170.5465 - val_loss: 299.0690\n",
      "Epoch 24/50\n",
      "1730/1730 - 5s - loss: 170.4041 - val_loss: 299.3667\n",
      "Epoch 25/50\n",
      "1730/1730 - 5s - loss: 170.3893 - val_loss: 297.1057\n",
      "Epoch 26/50\n",
      "1730/1730 - 5s - loss: 170.2103 - val_loss: 299.5929\n",
      "Epoch 27/50\n",
      "1730/1730 - 5s - loss: 169.9041 - val_loss: 296.1037\n",
      "Epoch 28/50\n",
      "1730/1730 - 5s - loss: 170.0122 - val_loss: 297.4408\n",
      "Epoch 29/50\n",
      "1730/1730 - 5s - loss: 169.6061 - val_loss: 296.4574\n",
      "Epoch 30/50\n",
      "1730/1730 - 5s - loss: 169.4892 - val_loss: 293.6212\n",
      "Epoch 31/50\n",
      "1730/1730 - 5s - loss: 169.4084 - val_loss: 294.7925\n",
      "Epoch 32/50\n",
      "1730/1730 - 5s - loss: 169.2949 - val_loss: 293.8980\n",
      "Epoch 33/50\n",
      "1730/1730 - 5s - loss: 169.1677 - val_loss: 293.5621\n",
      "Epoch 34/50\n",
      "1730/1730 - 5s - loss: 169.0201 - val_loss: 291.2554\n",
      "Epoch 35/50\n",
      "1730/1730 - 5s - loss: 169.1316 - val_loss: 289.8120\n",
      "Epoch 36/50\n",
      "1730/1730 - 5s - loss: 168.7393 - val_loss: 291.7370\n",
      "Epoch 37/50\n",
      "1730/1730 - 5s - loss: 168.7824 - val_loss: 293.2223\n",
      "Epoch 38/50\n",
      "1730/1730 - 5s - loss: 168.8119 - val_loss: 290.7510\n",
      "Epoch 39/50\n",
      "1730/1730 - 5s - loss: 168.5925 - val_loss: 289.4258\n",
      "Epoch 40/50\n",
      "1730/1730 - 5s - loss: 168.7177 - val_loss: 289.8734\n",
      "Epoch 41/50\n",
      "1730/1730 - 5s - loss: 168.6119 - val_loss: 289.3881\n",
      "Epoch 42/50\n",
      "1730/1730 - 5s - loss: 168.4636 - val_loss: 287.0944\n",
      "Epoch 43/50\n",
      "1730/1730 - 5s - loss: 168.1051 - val_loss: 290.1358\n",
      "Epoch 44/50\n",
      "1730/1730 - 5s - loss: 168.5043 - val_loss: 286.8387\n",
      "Epoch 45/50\n",
      "1730/1730 - 5s - loss: 168.3049 - val_loss: 287.3878\n",
      "Epoch 46/50\n",
      "1730/1730 - 5s - loss: 168.3736 - val_loss: 287.3481\n",
      "Epoch 47/50\n",
      "1730/1730 - 5s - loss: 168.1522 - val_loss: 287.6715\n",
      "Epoch 48/50\n",
      "1730/1730 - 5s - loss: 168.2784 - val_loss: 285.8718\n",
      "Epoch 49/50\n",
      "1730/1730 - 5s - loss: 168.1763 - val_loss: 286.8566\n",
      "Epoch 50/50\n",
      "1730/1730 - 5s - loss: 168.1491 - val_loss: 286.0950\n",
      "Training time for iteration 17: 251.13424158096313\n",
      "\n",
      "\n",
      "Iteration  18\n",
      "Layer size = 100, activation = sigmoid, batch size = 512, dropout = 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6920/6920 - 20s - loss: 337.5848 - val_loss: 340.4140\n",
      "Epoch 2/50\n",
      "6920/6920 - 20s - loss: 188.7397 - val_loss: 329.3845\n",
      "Epoch 3/50\n",
      "6920/6920 - 20s - loss: 157.2552 - val_loss: 332.2244\n",
      "Epoch 4/50\n",
      "6920/6920 - 20s - loss: 148.9326 - val_loss: 325.9721\n",
      "Epoch 5/50\n",
      "6920/6920 - 19s - loss: 144.5429 - val_loss: 316.5474\n",
      "Epoch 6/50\n",
      "6920/6920 - 20s - loss: 141.6776 - val_loss: 309.9805\n",
      "Epoch 7/50\n",
      "6920/6920 - 19s - loss: 139.3140 - val_loss: 307.7002\n",
      "Epoch 8/50\n",
      "6920/6920 - 19s - loss: 137.3121 - val_loss: 300.4829\n",
      "Epoch 9/50\n",
      "6920/6920 - 20s - loss: 135.9661 - val_loss: 290.6904\n",
      "Epoch 10/50\n",
      "6920/6920 - 19s - loss: 134.7773 - val_loss: 291.0313\n",
      "Epoch 11/50\n",
      "6920/6920 - 20s - loss: 134.0382 - val_loss: 282.1867\n",
      "Epoch 12/50\n",
      "6920/6920 - 20s - loss: 133.1622 - val_loss: 283.4385\n",
      "Epoch 13/50\n",
      "6920/6920 - 20s - loss: 132.4239 - val_loss: 278.4883\n",
      "Epoch 14/50\n",
      "6920/6920 - 19s - loss: 131.9063 - val_loss: 279.8432\n",
      "Epoch 15/50\n",
      "6920/6920 - 20s - loss: 131.5407 - val_loss: 273.4707\n",
      "Epoch 16/50\n",
      "6920/6920 - 19s - loss: 131.2054 - val_loss: 269.4083\n",
      "Epoch 17/50\n",
      "6920/6920 - 20s - loss: 130.8334 - val_loss: 269.7053\n",
      "Epoch 18/50\n",
      "6920/6920 - 20s - loss: 130.6548 - val_loss: 266.8225\n",
      "Epoch 19/50\n",
      "6920/6920 - 20s - loss: 130.2838 - val_loss: 265.1211\n",
      "Epoch 20/50\n",
      "6920/6920 - 20s - loss: 129.9921 - val_loss: 261.8974\n",
      "Epoch 21/50\n",
      "6920/6920 - 19s - loss: 129.8951 - val_loss: 261.7061\n",
      "Epoch 22/50\n",
      "6920/6920 - 20s - loss: 129.4805 - val_loss: 260.5305\n",
      "Epoch 23/50\n",
      "6920/6920 - 20s - loss: 129.0179 - val_loss: 259.6170\n",
      "Epoch 24/50\n",
      "6920/6920 - 20s - loss: 128.7603 - val_loss: 257.9948\n",
      "Epoch 25/50\n",
      "6920/6920 - 20s - loss: 128.7118 - val_loss: 254.5772\n",
      "Epoch 26/50\n",
      "6920/6920 - 20s - loss: 128.3805 - val_loss: 252.6858\n",
      "Epoch 27/50\n",
      "6920/6920 - 20s - loss: 128.1302 - val_loss: 250.3414\n",
      "Epoch 28/50\n",
      "6920/6920 - 20s - loss: 128.0779 - val_loss: 249.7739\n",
      "Epoch 29/50\n",
      "6920/6920 - 20s - loss: 127.9457 - val_loss: 245.3436\n",
      "Epoch 30/50\n",
      "6920/6920 - 20s - loss: 127.8782 - val_loss: 247.5691\n",
      "Epoch 31/50\n",
      "6920/6920 - 19s - loss: 127.8015 - val_loss: 246.5672\n",
      "Epoch 32/50\n",
      "6920/6920 - 20s - loss: 127.7928 - val_loss: 247.7760\n",
      "Epoch 33/50\n",
      "6920/6920 - 20s - loss: 127.6502 - val_loss: 249.5100\n",
      "Epoch 34/50\n",
      "6920/6920 - 20s - loss: 127.6301 - val_loss: 241.4457\n",
      "Epoch 35/50\n",
      "6920/6920 - 19s - loss: 127.6610 - val_loss: 245.9597\n",
      "Epoch 36/50\n",
      "6920/6920 - 19s - loss: 127.4053 - val_loss: 243.0699\n",
      "Epoch 37/50\n",
      "6920/6920 - 20s - loss: 127.5279 - val_loss: 242.0591\n",
      "Epoch 38/50\n",
      "6920/6920 - 20s - loss: 127.3136 - val_loss: 245.4765\n",
      "Epoch 39/50\n",
      "6920/6920 - 19s - loss: 127.1026 - val_loss: 240.7092\n",
      "Epoch 40/50\n",
      "6920/6920 - 20s - loss: 127.1549 - val_loss: 242.1996\n",
      "Epoch 41/50\n",
      "6920/6920 - 20s - loss: 127.1061 - val_loss: 241.9631\n",
      "Epoch 42/50\n",
      "6920/6920 - 19s - loss: 127.1239 - val_loss: 240.0308\n",
      "Epoch 43/50\n",
      "6920/6920 - 20s - loss: 127.0565 - val_loss: 242.7899\n",
      "Epoch 44/50\n",
      "6920/6920 - 20s - loss: 126.8432 - val_loss: 243.3031\n",
      "Epoch 45/50\n",
      "6920/6920 - 19s - loss: 126.9580 - val_loss: 243.1753\n",
      "Epoch 46/50\n",
      "6920/6920 - 19s - loss: 126.9341 - val_loss: 241.2853\n",
      "Epoch 47/50\n",
      "6920/6920 - 19s - loss: 126.9185 - val_loss: 240.0609\n",
      "Epoch 48/50\n",
      "6920/6920 - 20s - loss: 126.8479 - val_loss: 240.1007\n",
      "Epoch 49/50\n",
      "6920/6920 - 19s - loss: 126.7509 - val_loss: 238.6006\n",
      "Epoch 50/50\n",
      "6920/6920 - 20s - loss: 126.6159 - val_loss: 241.0754\n",
      "Training time for iteration 18: 977.6909549236298\n",
      "\n",
      "\n",
      "Iteration  19\n",
      "Layer size = 500, activation = sigmoid, batch size = 1024, dropout = 0.1\n",
      "Epoch 1/50\n",
      "3460/3460 - 13s - loss: 304.9723 - val_loss: 332.0417\n",
      "Epoch 2/50\n",
      "3460/3460 - 13s - loss: 155.4620 - val_loss: 315.6858\n",
      "Epoch 3/50\n",
      "3460/3460 - 13s - loss: 130.1423 - val_loss: 315.3159\n",
      "Epoch 4/50\n",
      "3460/3460 - 13s - loss: 120.5811 - val_loss: 305.5629\n",
      "Epoch 5/50\n",
      "3460/3460 - 13s - loss: 115.9598 - val_loss: 296.7834\n",
      "Epoch 6/50\n",
      "3460/3460 - 13s - loss: 113.0371 - val_loss: 288.2041\n",
      "Epoch 7/50\n",
      "3460/3460 - 13s - loss: 110.6611 - val_loss: 284.2482\n",
      "Epoch 8/50\n",
      "3460/3460 - 13s - loss: 109.0425 - val_loss: 272.9002\n",
      "Epoch 9/50\n",
      "3460/3460 - 13s - loss: 107.8562 - val_loss: 261.1068\n",
      "Epoch 10/50\n",
      "3460/3460 - 13s - loss: 106.7152 - val_loss: 257.3167\n",
      "Epoch 11/50\n",
      "3460/3460 - 13s - loss: 105.7072 - val_loss: 249.4238\n",
      "Epoch 12/50\n",
      "3460/3460 - 13s - loss: 104.7844 - val_loss: 246.8982\n",
      "Epoch 13/50\n",
      "3460/3460 - 12s - loss: 103.9950 - val_loss: 244.6646\n",
      "Epoch 14/50\n",
      "3460/3460 - 12s - loss: 103.4626 - val_loss: 240.5244\n",
      "Epoch 15/50\n",
      "3460/3460 - 13s - loss: 102.8533 - val_loss: 234.2592\n",
      "Epoch 16/50\n",
      "3460/3460 - 13s - loss: 102.3741 - val_loss: 230.1436\n",
      "Epoch 17/50\n",
      "3460/3460 - 13s - loss: 101.8548 - val_loss: 229.5579\n",
      "Epoch 18/50\n",
      "3460/3460 - 13s - loss: 101.4575 - val_loss: 223.0857\n",
      "Epoch 19/50\n",
      "3460/3460 - 13s - loss: 101.1562 - val_loss: 219.8323\n",
      "Epoch 20/50\n",
      "3460/3460 - 13s - loss: 100.8285 - val_loss: 215.6586\n",
      "Epoch 21/50\n",
      "3460/3460 - 13s - loss: 100.5385 - val_loss: 214.2035\n",
      "Epoch 22/50\n",
      "3460/3460 - 13s - loss: 100.1988 - val_loss: 210.7988\n",
      "Epoch 23/50\n",
      "3460/3460 - 13s - loss: 99.9153 - val_loss: 208.2998\n",
      "Epoch 24/50\n",
      "3460/3460 - 13s - loss: 99.7198 - val_loss: 206.9099\n",
      "Epoch 25/50\n",
      "3460/3460 - 13s - loss: 99.3597 - val_loss: 205.1104\n",
      "Epoch 26/50\n",
      "3460/3460 - 13s - loss: 99.0626 - val_loss: 204.1776\n",
      "Epoch 27/50\n",
      "3460/3460 - 13s - loss: 98.8987 - val_loss: 198.6980\n",
      "Epoch 28/50\n",
      "3460/3460 - 13s - loss: 98.7212 - val_loss: 201.4171\n",
      "Epoch 29/50\n",
      "3460/3460 - 13s - loss: 98.4792 - val_loss: 194.0948\n",
      "Epoch 30/50\n",
      "3460/3460 - 13s - loss: 98.2825 - val_loss: 195.5719\n",
      "Epoch 31/50\n",
      "3460/3460 - 13s - loss: 98.1608 - val_loss: 192.2552\n",
      "Epoch 32/50\n",
      "3460/3460 - 13s - loss: 97.9867 - val_loss: 192.2858\n",
      "Epoch 33/50\n",
      "3460/3460 - 13s - loss: 97.9316 - val_loss: 195.6313\n",
      "Epoch 34/50\n",
      "3460/3460 - 13s - loss: 97.6715 - val_loss: 187.9549\n",
      "Epoch 35/50\n",
      "3460/3460 - 13s - loss: 97.6626 - val_loss: 189.8673\n",
      "Epoch 36/50\n",
      "3460/3460 - 13s - loss: 97.3923 - val_loss: 190.8033\n",
      "Epoch 37/50\n",
      "3460/3460 - 13s - loss: 97.0809 - val_loss: 186.4907\n",
      "Epoch 38/50\n",
      "3460/3460 - 13s - loss: 96.8338 - val_loss: 191.3809\n",
      "Epoch 39/50\n",
      "3460/3460 - 13s - loss: 96.7265 - val_loss: 184.2078\n",
      "Epoch 40/50\n",
      "3460/3460 - 13s - loss: 96.3986 - val_loss: 184.5405\n",
      "Epoch 41/50\n",
      "3460/3460 - 13s - loss: 96.2956 - val_loss: 186.8709\n",
      "Epoch 42/50\n",
      "3460/3460 - 13s - loss: 96.1118 - val_loss: 183.2415\n",
      "Epoch 43/50\n",
      "3460/3460 - 13s - loss: 96.0239 - val_loss: 184.9037\n",
      "Epoch 44/50\n",
      "3460/3460 - 13s - loss: 95.6653 - val_loss: 184.9151\n",
      "Epoch 45/50\n",
      "3460/3460 - 13s - loss: 95.6280 - val_loss: 185.0974\n",
      "Epoch 46/50\n",
      "3460/3460 - 13s - loss: 95.5353 - val_loss: 184.3188\n",
      "Epoch 47/50\n",
      "3460/3460 - 13s - loss: 95.4370 - val_loss: 178.9119\n",
      "Epoch 48/50\n",
      "3460/3460 - 13s - loss: 95.2462 - val_loss: 181.9781\n",
      "Epoch 49/50\n",
      "3460/3460 - 13s - loss: 95.0618 - val_loss: 178.0746\n",
      "Epoch 50/50\n",
      "3460/3460 - 13s - loss: 94.9537 - val_loss: 182.1349\n",
      "Training time for iteration 19: 646.8042454719543\n",
      "\n",
      "\n",
      "Iteration  20\n",
      "Layer size = 200, activation = relu, batch size = 512, dropout = 0.4\n",
      "Epoch 1/50\n",
      "6920/6920 - 20s - loss: 307.8011 - val_loss: 333.5769\n",
      "Epoch 2/50\n",
      "6920/6920 - 20s - loss: 178.0144 - val_loss: 329.1413\n",
      "Epoch 3/50\n",
      "6920/6920 - 20s - loss: 156.7936 - val_loss: 322.6997\n",
      "Epoch 4/50\n",
      "6920/6920 - 20s - loss: 149.7224 - val_loss: 316.0740\n",
      "Epoch 5/50\n",
      "6920/6920 - 20s - loss: 145.9888 - val_loss: 308.6793\n",
      "Epoch 6/50\n",
      "6920/6920 - 20s - loss: 143.3002 - val_loss: 304.2473\n",
      "Epoch 7/50\n",
      "6920/6920 - 20s - loss: 140.9420 - val_loss: 301.3188\n",
      "Epoch 8/50\n",
      "6920/6920 - 20s - loss: 139.3596 - val_loss: 293.7349\n",
      "Epoch 9/50\n",
      "6920/6920 - 20s - loss: 138.2683 - val_loss: 286.0487\n",
      "Epoch 10/50\n",
      "6920/6920 - 20s - loss: 137.2338 - val_loss: 287.5967\n",
      "Epoch 11/50\n",
      "6920/6920 - 20s - loss: 136.6552 - val_loss: 279.8519\n",
      "Epoch 12/50\n",
      "6920/6920 - 20s - loss: 136.1515 - val_loss: 282.8106\n",
      "Epoch 13/50\n",
      "6920/6920 - 20s - loss: 135.6314 - val_loss: 279.1854\n",
      "Epoch 14/50\n",
      "6920/6920 - 20s - loss: 135.4401 - val_loss: 281.1568\n",
      "Epoch 15/50\n",
      "6920/6920 - 20s - loss: 134.8877 - val_loss: 275.1438\n",
      "Epoch 16/50\n",
      "6920/6920 - 20s - loss: 134.6495 - val_loss: 273.7233\n",
      "Epoch 17/50\n",
      "6920/6920 - 20s - loss: 134.2661 - val_loss: 274.1787\n",
      "Epoch 18/50\n",
      "6920/6920 - 20s - loss: 134.0839 - val_loss: 271.2215\n",
      "Epoch 19/50\n",
      "6920/6920 - 20s - loss: 133.8838 - val_loss: 272.3376\n",
      "Epoch 20/50\n",
      "6920/6920 - 20s - loss: 133.9107 - val_loss: 268.8176\n",
      "Epoch 21/50\n",
      "6920/6920 - 20s - loss: 133.7971 - val_loss: 270.0601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "6920/6920 - 20s - loss: 133.6613 - val_loss: 268.1625\n",
      "Epoch 23/50\n",
      "6920/6920 - 20s - loss: 133.3333 - val_loss: 268.6742\n",
      "Epoch 24/50\n",
      "6920/6920 - 20s - loss: 133.3416 - val_loss: 267.9915\n",
      "Epoch 25/50\n",
      "6920/6920 - 20s - loss: 133.3517 - val_loss: 268.3556\n",
      "Epoch 26/50\n",
      "6920/6920 - 20s - loss: 133.2561 - val_loss: 264.2823\n",
      "Epoch 27/50\n",
      "6920/6920 - 20s - loss: 133.0532 - val_loss: 263.9502\n",
      "Epoch 28/50\n",
      "6920/6920 - 20s - loss: 133.0493 - val_loss: 262.7930\n",
      "Epoch 29/50\n",
      "6920/6920 - 20s - loss: 132.9836 - val_loss: 261.5589\n",
      "Epoch 30/50\n",
      "6920/6920 - 20s - loss: 132.8248 - val_loss: 261.4804\n",
      "Epoch 31/50\n",
      "6920/6920 - 20s - loss: 132.9078 - val_loss: 264.0240\n",
      "Epoch 32/50\n",
      "6920/6920 - 20s - loss: 132.7671 - val_loss: 262.0619\n",
      "Epoch 33/50\n",
      "6920/6920 - 20s - loss: 132.7908 - val_loss: 264.3912\n",
      "Epoch 34/50\n",
      "6920/6920 - 20s - loss: 132.7171 - val_loss: 257.1230\n",
      "Epoch 35/50\n",
      "6920/6920 - 20s - loss: 132.7558 - val_loss: 262.9280\n",
      "Epoch 36/50\n",
      "6920/6920 - 20s - loss: 132.4505 - val_loss: 260.4357\n",
      "Epoch 37/50\n",
      "6920/6920 - 20s - loss: 132.5141 - val_loss: 257.8159\n",
      "Epoch 38/50\n",
      "6920/6920 - 20s - loss: 132.4652 - val_loss: 265.0768\n",
      "Epoch 39/50\n",
      "6920/6920 - 20s - loss: 132.4700 - val_loss: 260.2991\n",
      "Epoch 40/50\n",
      "6920/6920 - 20s - loss: 132.3124 - val_loss: 260.3322\n",
      "Epoch 41/50\n",
      "6920/6920 - 20s - loss: 132.2546 - val_loss: 259.7912\n",
      "Epoch 42/50\n",
      "6920/6920 - 20s - loss: 132.0787 - val_loss: 259.3365\n",
      "Epoch 43/50\n",
      "6920/6920 - 20s - loss: 131.9163 - val_loss: 259.9279\n",
      "Epoch 44/50\n",
      "6920/6920 - 20s - loss: 132.0039 - val_loss: 261.6844\n",
      "Epoch 45/50\n",
      "6920/6920 - 20s - loss: 131.9474 - val_loss: 260.2824\n",
      "Epoch 46/50\n",
      "6920/6920 - 20s - loss: 131.8980 - val_loss: 264.0738\n",
      "Epoch 47/50\n",
      "6920/6920 - 20s - loss: 131.6929 - val_loss: 258.2312\n",
      "Epoch 48/50\n",
      "6920/6920 - 20s - loss: 131.5933 - val_loss: 260.4590\n",
      "Epoch 49/50\n",
      "6920/6920 - 21s - loss: 131.5453 - val_loss: 256.1165\n",
      "Epoch 50/50\n",
      "6920/6920 - 20s - loss: 131.4622 - val_loss: 259.0486\n",
      "Training time for iteration 20: 1008.0849995613098\n",
      "\n",
      "\n",
      "Iteration  21\n",
      "Layer size = 200, activation = tanh, batch size = 512, dropout = 0.0\n",
      "Epoch 1/50\n",
      "6920/6920 - 21s - loss: 277.5512 - val_loss: 332.6638\n",
      "Epoch 2/50\n",
      "6920/6920 - 20s - loss: 138.3495 - val_loss: 328.3339\n",
      "Epoch 3/50\n",
      "6920/6920 - 20s - loss: 118.4073 - val_loss: 309.3465\n",
      "Epoch 4/50\n",
      "6920/6920 - 20s - loss: 110.8284 - val_loss: 302.8380\n",
      "Epoch 5/50\n",
      "6920/6920 - 20s - loss: 105.4479 - val_loss: 293.5341\n",
      "Epoch 6/50\n",
      "6920/6920 - 20s - loss: 101.3386 - val_loss: 281.7049\n",
      "Epoch 7/50\n",
      "6920/6920 - 20s - loss: 98.3902 - val_loss: 274.7433\n",
      "Epoch 8/50\n",
      "6920/6920 - 20s - loss: 95.8634 - val_loss: 264.1022\n",
      "Epoch 9/50\n",
      "6920/6920 - 20s - loss: 93.5894 - val_loss: 255.4957\n",
      "Epoch 10/50\n",
      "6920/6920 - 20s - loss: 91.6765 - val_loss: 248.1116\n",
      "Epoch 11/50\n",
      "6920/6920 - 20s - loss: 89.9584 - val_loss: 240.5311\n",
      "Epoch 12/50\n",
      "6920/6920 - 20s - loss: 88.4265 - val_loss: 235.0474\n",
      "Epoch 13/50\n",
      "6920/6920 - 20s - loss: 86.9414 - val_loss: 225.3080\n",
      "Epoch 14/50\n",
      "6920/6920 - 20s - loss: 85.5948 - val_loss: 223.0403\n",
      "Epoch 15/50\n",
      "6920/6920 - 20s - loss: 84.3838 - val_loss: 215.7476\n",
      "Epoch 16/50\n",
      "6920/6920 - 20s - loss: 83.3192 - val_loss: 213.4773\n",
      "Epoch 17/50\n",
      "6920/6920 - 20s - loss: 82.3634 - val_loss: 210.1324\n",
      "Epoch 18/50\n",
      "6920/6920 - 20s - loss: 81.4611 - val_loss: 208.1671\n",
      "Epoch 19/50\n",
      "6920/6920 - 20s - loss: 80.6338 - val_loss: 200.1625\n",
      "Epoch 20/50\n",
      "6920/6920 - 20s - loss: 79.8194 - val_loss: 198.2474\n",
      "Epoch 21/50\n",
      "6920/6920 - 20s - loss: 79.1206 - val_loss: 194.0578\n",
      "Epoch 22/50\n",
      "6920/6920 - 20s - loss: 78.4993 - val_loss: 189.7211\n",
      "Epoch 23/50\n",
      "6920/6920 - 20s - loss: 77.8960 - val_loss: 189.2512\n",
      "Epoch 24/50\n",
      "6920/6920 - 20s - loss: 77.2854 - val_loss: 182.8054\n",
      "Epoch 25/50\n",
      "6920/6920 - 20s - loss: 76.7038 - val_loss: 181.4149\n",
      "Epoch 26/50\n",
      "6920/6920 - 20s - loss: 76.0754 - val_loss: 178.0125\n",
      "Epoch 27/50\n",
      "6920/6920 - 20s - loss: 75.4357 - val_loss: 175.0974\n",
      "Epoch 28/50\n",
      "6920/6920 - 20s - loss: 74.8837 - val_loss: 171.7376\n",
      "Epoch 29/50\n",
      "6920/6920 - 20s - loss: 74.3824 - val_loss: 169.6844\n",
      "Epoch 30/50\n",
      "6920/6920 - 20s - loss: 73.9432 - val_loss: 168.5195\n",
      "Epoch 31/50\n",
      "6920/6920 - 20s - loss: 73.5304 - val_loss: 163.0346\n",
      "Epoch 32/50\n",
      "6920/6920 - 20s - loss: 73.1275 - val_loss: 161.8364\n",
      "Epoch 33/50\n",
      "6920/6920 - 20s - loss: 72.7566 - val_loss: 163.2652\n",
      "Epoch 34/50\n",
      "6920/6920 - 20s - loss: 72.4470 - val_loss: 158.8385\n",
      "Epoch 35/50\n",
      "6920/6920 - 20s - loss: 72.2132 - val_loss: 158.1536\n",
      "Epoch 36/50\n",
      "6920/6920 - 20s - loss: 71.9512 - val_loss: 158.5713\n",
      "Epoch 37/50\n",
      "6920/6920 - 20s - loss: 71.7083 - val_loss: 158.3958\n",
      "Epoch 38/50\n",
      "6920/6920 - 20s - loss: 71.5306 - val_loss: 157.4640\n",
      "Epoch 39/50\n",
      "6920/6920 - 20s - loss: 71.3569 - val_loss: 157.6295\n",
      "Epoch 40/50\n",
      "6920/6920 - 20s - loss: 71.1887 - val_loss: 153.5106\n",
      "Epoch 41/50\n",
      "6920/6920 - 20s - loss: 71.0115 - val_loss: 151.8940\n",
      "Epoch 42/50\n",
      "6920/6920 - 20s - loss: 70.8806 - val_loss: 157.9048\n",
      "Epoch 43/50\n",
      "6920/6920 - 20s - loss: 70.7060 - val_loss: 152.4752\n",
      "Epoch 44/50\n",
      "6920/6920 - 20s - loss: 70.5396 - val_loss: 150.4347\n",
      "Epoch 45/50\n",
      "6920/6920 - 20s - loss: 70.3983 - val_loss: 151.4778\n",
      "Epoch 46/50\n",
      "6920/6920 - 20s - loss: 70.2107 - val_loss: 149.0831\n",
      "Epoch 47/50\n",
      "6920/6920 - 20s - loss: 70.0254 - val_loss: 150.2962\n",
      "Epoch 48/50\n",
      "6920/6920 - 20s - loss: 69.8836 - val_loss: 148.6341\n",
      "Epoch 49/50\n",
      "6920/6920 - 20s - loss: 69.7102 - val_loss: 152.6136\n",
      "Epoch 50/50\n",
      "6920/6920 - 20s - loss: 69.5330 - val_loss: 153.5412\n",
      "Training time for iteration 21: 1004.871087551117\n",
      "\n",
      "\n",
      "Iteration  22\n",
      "Layer size = 200, activation = tanh, batch size = 1024, dropout = 0.0\n",
      "Epoch 1/50\n",
      "3460/3460 - 10s - loss: 354.6306 - val_loss: 343.6605\n",
      "Epoch 2/50\n",
      "3460/3460 - 10s - loss: 175.8644 - val_loss: 324.6196\n",
      "Epoch 3/50\n",
      "3460/3460 - 10s - loss: 139.5589 - val_loss: 321.2885\n",
      "Epoch 4/50\n",
      "3460/3460 - 10s - loss: 123.9124 - val_loss: 314.5952\n",
      "Epoch 5/50\n",
      "3460/3460 - 10s - loss: 115.5135 - val_loss: 307.2705\n",
      "Epoch 6/50\n",
      "3460/3460 - 10s - loss: 110.1972 - val_loss: 301.1152\n",
      "Epoch 7/50\n",
      "3460/3460 - 10s - loss: 106.5811 - val_loss: 296.4329\n",
      "Epoch 8/50\n",
      "3460/3460 - 10s - loss: 103.9266 - val_loss: 289.2635\n",
      "Epoch 9/50\n",
      "3460/3460 - 10s - loss: 101.6805 - val_loss: 280.8170\n",
      "Epoch 10/50\n",
      "3460/3460 - 10s - loss: 99.4696 - val_loss: 279.0552\n",
      "Epoch 11/50\n",
      "3460/3460 - 10s - loss: 97.5349 - val_loss: 270.6386\n",
      "Epoch 12/50\n",
      "3460/3460 - 10s - loss: 95.9293 - val_loss: 271.2970\n",
      "Epoch 13/50\n",
      "3460/3460 - 10s - loss: 94.5412 - val_loss: 267.2210\n",
      "Epoch 14/50\n",
      "3460/3460 - 10s - loss: 93.2450 - val_loss: 260.6214\n",
      "Epoch 15/50\n",
      "3460/3460 - 10s - loss: 91.9302 - val_loss: 255.6083\n",
      "Epoch 16/50\n",
      "3460/3460 - 10s - loss: 90.7116 - val_loss: 249.0589\n",
      "Epoch 17/50\n",
      "3460/3460 - 10s - loss: 89.5781 - val_loss: 248.6877\n",
      "Epoch 18/50\n",
      "3460/3460 - 10s - loss: 88.4857 - val_loss: 242.6595\n",
      "Epoch 19/50\n",
      "3460/3460 - 10s - loss: 87.4400 - val_loss: 240.2959\n",
      "Epoch 20/50\n",
      "3460/3460 - 10s - loss: 86.4819 - val_loss: 234.8568\n",
      "Epoch 21/50\n",
      "3460/3460 - 10s - loss: 85.6612 - val_loss: 232.5023\n",
      "Epoch 22/50\n",
      "3460/3460 - 10s - loss: 84.9081 - val_loss: 228.3352\n",
      "Epoch 23/50\n",
      "3460/3460 - 10s - loss: 84.2239 - val_loss: 226.5037\n",
      "Epoch 24/50\n",
      "3460/3460 - 10s - loss: 83.6216 - val_loss: 224.8988\n",
      "Epoch 25/50\n",
      "3460/3460 - 10s - loss: 83.0617 - val_loss: 221.1420\n",
      "Epoch 26/50\n",
      "3460/3460 - 10s - loss: 82.5401 - val_loss: 218.3376\n",
      "Epoch 27/50\n",
      "3460/3460 - 10s - loss: 82.0513 - val_loss: 213.3767\n",
      "Epoch 28/50\n",
      "3460/3460 - 11s - loss: 81.5983 - val_loss: 213.4520\n",
      "Epoch 29/50\n",
      "3460/3460 - 10s - loss: 81.1791 - val_loss: 208.1808\n",
      "Epoch 30/50\n",
      "3460/3460 - 10s - loss: 80.7727 - val_loss: 207.1543\n",
      "Epoch 31/50\n",
      "3460/3460 - 10s - loss: 80.3563 - val_loss: 205.9007\n",
      "Epoch 32/50\n",
      "3460/3460 - 10s - loss: 79.9062 - val_loss: 203.6928\n",
      "Epoch 33/50\n",
      "3460/3460 - 10s - loss: 79.4759 - val_loss: 203.7933\n",
      "Epoch 34/50\n",
      "3460/3460 - 11s - loss: 79.0278 - val_loss: 196.7915\n",
      "Epoch 35/50\n",
      "3460/3460 - 10s - loss: 78.6470 - val_loss: 196.6100\n",
      "Epoch 36/50\n",
      "3460/3460 - 10s - loss: 78.2730 - val_loss: 194.6727\n",
      "Epoch 37/50\n",
      "3460/3460 - 10s - loss: 77.9463 - val_loss: 193.0758\n",
      "Epoch 38/50\n",
      "3460/3460 - 10s - loss: 77.6451 - val_loss: 195.9127\n",
      "Epoch 39/50\n",
      "3460/3460 - 10s - loss: 77.3827 - val_loss: 188.6326\n",
      "Epoch 40/50\n",
      "3460/3460 - 10s - loss: 77.0791 - val_loss: 187.4435\n",
      "Epoch 41/50\n",
      "3460/3460 - 10s - loss: 76.8293 - val_loss: 186.7699\n",
      "Epoch 42/50\n",
      "3460/3460 - 10s - loss: 76.5350 - val_loss: 184.6183\n",
      "Epoch 43/50\n",
      "3460/3460 - 10s - loss: 76.2416 - val_loss: 184.5309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50\n",
      "3460/3460 - 10s - loss: 75.9759 - val_loss: 184.6777\n",
      "Epoch 45/50\n",
      "3460/3460 - 10s - loss: 75.7090 - val_loss: 182.1012\n",
      "Epoch 46/50\n",
      "3460/3460 - 10s - loss: 75.4569 - val_loss: 183.1448\n",
      "Epoch 47/50\n",
      "3460/3460 - 10s - loss: 75.1087 - val_loss: 179.9207\n",
      "Epoch 48/50\n",
      "3460/3460 - 10s - loss: 74.8485 - val_loss: 178.3254\n",
      "Epoch 49/50\n",
      "3460/3460 - 10s - loss: 74.6414 - val_loss: 177.5517\n",
      "Epoch 50/50\n",
      "3460/3460 - 10s - loss: 74.4018 - val_loss: 177.4938\n",
      "Training time for iteration 22: 508.1955189704895\n",
      "\n",
      "\n",
      "Iteration  23\n",
      "Layer size = 100, activation = sigmoid, batch size = 512, dropout = 0.0\n",
      "Epoch 1/50\n",
      "6920/6920 - 20s - loss: 317.6396 - val_loss: 338.9895\n",
      "Epoch 2/50\n",
      "6920/6920 - 19s - loss: 158.6900 - val_loss: 326.1947\n",
      "Epoch 3/50\n",
      "6920/6920 - 19s - loss: 129.0345 - val_loss: 317.4272\n",
      "Epoch 4/50\n",
      "6920/6920 - 19s - loss: 119.0809 - val_loss: 311.9202\n",
      "Epoch 5/50\n",
      "6920/6920 - 19s - loss: 113.2277 - val_loss: 305.9840\n",
      "Epoch 6/50\n",
      "6920/6920 - 19s - loss: 109.4832 - val_loss: 299.3370\n",
      "Epoch 7/50\n",
      "6920/6920 - 20s - loss: 106.8978 - val_loss: 298.9872\n",
      "Epoch 8/50\n",
      "6920/6920 - 19s - loss: 104.2846 - val_loss: 287.4937\n",
      "Epoch 9/50\n",
      "6920/6920 - 19s - loss: 102.1221 - val_loss: 277.5124\n",
      "Epoch 10/50\n",
      "6920/6920 - 20s - loss: 100.3202 - val_loss: 275.0404\n",
      "Epoch 11/50\n",
      "6920/6920 - 20s - loss: 98.7867 - val_loss: 267.0605\n",
      "Epoch 12/50\n",
      "6920/6920 - 19s - loss: 97.2115 - val_loss: 266.8380\n",
      "Epoch 13/50\n",
      "6920/6920 - 20s - loss: 95.9577 - val_loss: 260.6523\n",
      "Epoch 14/50\n",
      "6920/6920 - 19s - loss: 94.8382 - val_loss: 259.6168\n",
      "Epoch 15/50\n",
      "6920/6920 - 20s - loss: 93.7690 - val_loss: 249.6486\n",
      "Epoch 16/50\n",
      "6920/6920 - 20s - loss: 92.8365 - val_loss: 248.0115\n",
      "Epoch 17/50\n",
      "6920/6920 - 19s - loss: 92.0320 - val_loss: 246.4319\n",
      "Epoch 18/50\n",
      "6920/6920 - 19s - loss: 91.3218 - val_loss: 241.7467\n",
      "Epoch 19/50\n",
      "6920/6920 - 19s - loss: 90.6547 - val_loss: 240.1254\n",
      "Epoch 20/50\n",
      "6920/6920 - 19s - loss: 90.0078 - val_loss: 236.5293\n",
      "Epoch 21/50\n",
      "6920/6920 - 19s - loss: 89.4671 - val_loss: 235.9825\n",
      "Epoch 22/50\n",
      "6920/6920 - 19s - loss: 89.0076 - val_loss: 231.0362\n",
      "Epoch 23/50\n",
      "6920/6920 - 19s - loss: 88.5015 - val_loss: 229.8761\n",
      "Epoch 24/50\n",
      "6920/6920 - 19s - loss: 88.0321 - val_loss: 228.3542\n",
      "Epoch 25/50\n",
      "6920/6920 - 19s - loss: 87.6094 - val_loss: 224.9612\n",
      "Epoch 26/50\n",
      "6920/6920 - 19s - loss: 87.1400 - val_loss: 221.9954\n",
      "Epoch 27/50\n",
      "6920/6920 - 19s - loss: 86.6047 - val_loss: 217.8989\n",
      "Epoch 28/50\n",
      "6920/6920 - 19s - loss: 86.2097 - val_loss: 217.5194\n",
      "Epoch 29/50\n",
      "6920/6920 - 19s - loss: 85.8485 - val_loss: 214.7001\n",
      "Epoch 30/50\n",
      "6920/6920 - 19s - loss: 85.5296 - val_loss: 215.0332\n",
      "Epoch 31/50\n",
      "6920/6920 - 19s - loss: 85.1869 - val_loss: 212.0473\n",
      "Epoch 32/50\n",
      "6920/6920 - 19s - loss: 84.8830 - val_loss: 210.9534\n",
      "Epoch 33/50\n",
      "6920/6920 - 19s - loss: 84.5605 - val_loss: 211.8261\n",
      "Epoch 34/50\n",
      "6920/6920 - 19s - loss: 84.2629 - val_loss: 205.7127\n",
      "Epoch 35/50\n",
      "6920/6920 - 19s - loss: 83.9915 - val_loss: 207.1138\n",
      "Epoch 36/50\n",
      "6920/6920 - 19s - loss: 83.6812 - val_loss: 206.1760\n",
      "Epoch 37/50\n",
      "6920/6920 - 19s - loss: 83.3329 - val_loss: 205.0089\n",
      "Epoch 38/50\n",
      "6920/6920 - 19s - loss: 83.0427 - val_loss: 207.8901\n",
      "Epoch 39/50\n",
      "6920/6920 - 19s - loss: 82.7536 - val_loss: 206.0741\n",
      "Epoch 40/50\n",
      "6920/6920 - 19s - loss: 82.4520 - val_loss: 202.8241\n",
      "Epoch 41/50\n",
      "6920/6920 - 19s - loss: 82.1816 - val_loss: 199.8259\n",
      "Epoch 42/50\n",
      "6920/6920 - 19s - loss: 81.9442 - val_loss: 201.5319\n",
      "Epoch 43/50\n",
      "6920/6920 - 19s - loss: 81.6609 - val_loss: 201.3275\n",
      "Epoch 44/50\n",
      "6920/6920 - 19s - loss: 81.4198 - val_loss: 197.6609\n",
      "Epoch 45/50\n",
      "6920/6920 - 19s - loss: 81.2058 - val_loss: 196.7124\n",
      "Epoch 46/50\n",
      "6920/6920 - 19s - loss: 80.9909 - val_loss: 197.7819\n",
      "Epoch 47/50\n",
      "6920/6920 - 19s - loss: 80.7607 - val_loss: 194.9671\n",
      "Epoch 48/50\n",
      "6920/6920 - 19s - loss: 80.5573 - val_loss: 192.9554\n",
      "Epoch 49/50\n",
      "6920/6920 - 19s - loss: 80.3481 - val_loss: 190.6719\n",
      "Epoch 50/50\n",
      "6920/6920 - 19s - loss: 80.1472 - val_loss: 194.8670\n",
      "Training time for iteration 23: 968.8852424621582\n",
      "\n",
      "\n",
      "Iteration  24\n",
      "Layer size = 300, activation = relu, batch size = 1024, dropout = 0.1\n",
      "Epoch 1/50\n",
      "3460/3460 - 10s - loss: 337.3690 - val_loss: 339.8445\n",
      "Epoch 2/50\n",
      "3460/3460 - 10s - loss: 173.8754 - val_loss: 319.3052\n",
      "Epoch 3/50\n",
      "3460/3460 - 10s - loss: 139.3430 - val_loss: 320.3663\n",
      "Epoch 4/50\n",
      "3460/3460 - 10s - loss: 127.8309 - val_loss: 313.3398\n",
      "Epoch 5/50\n",
      "3460/3460 - 11s - loss: 122.2729 - val_loss: 303.5962\n",
      "Epoch 6/50\n",
      "3460/3460 - 11s - loss: 119.1726 - val_loss: 299.2415\n",
      "Epoch 7/50\n",
      "3460/3460 - 10s - loss: 116.8806 - val_loss: 293.0421\n",
      "Epoch 8/50\n",
      "3460/3460 - 10s - loss: 115.0736 - val_loss: 284.8365\n",
      "Epoch 9/50\n",
      "3460/3460 - 11s - loss: 113.8669 - val_loss: 275.6915\n",
      "Epoch 10/50\n",
      "3460/3460 - 11s - loss: 112.8128 - val_loss: 275.4704\n",
      "Epoch 11/50\n",
      "3460/3460 - 11s - loss: 111.8235 - val_loss: 267.9492\n",
      "Epoch 12/50\n",
      "3460/3460 - 10s - loss: 111.0979 - val_loss: 268.2365\n",
      "Epoch 13/50\n",
      "3460/3460 - 10s - loss: 110.4609 - val_loss: 265.3651\n",
      "Epoch 14/50\n",
      "3460/3460 - 10s - loss: 109.9607 - val_loss: 259.0802\n",
      "Epoch 15/50\n",
      "3460/3460 - 10s - loss: 109.5707 - val_loss: 255.7878\n",
      "Epoch 16/50\n",
      "3460/3460 - 11s - loss: 109.1814 - val_loss: 250.8371\n",
      "Epoch 17/50\n",
      "3460/3460 - 11s - loss: 108.8725 - val_loss: 250.7782\n",
      "Epoch 18/50\n",
      "3460/3460 - 11s - loss: 108.5460 - val_loss: 249.0369\n",
      "Epoch 19/50\n",
      "3460/3460 - 10s - loss: 108.3602 - val_loss: 246.5136\n",
      "Epoch 20/50\n",
      "3460/3460 - 10s - loss: 108.0058 - val_loss: 242.0424\n",
      "Epoch 21/50\n",
      "3460/3460 - 10s - loss: 107.7743 - val_loss: 240.8275\n",
      "Epoch 22/50\n",
      "3460/3460 - 10s - loss: 107.4998 - val_loss: 239.5170\n",
      "Epoch 23/50\n",
      "3460/3460 - 10s - loss: 107.2836 - val_loss: 235.9877\n",
      "Epoch 24/50\n",
      "3460/3460 - 11s - loss: 107.1465 - val_loss: 237.6098\n",
      "Epoch 25/50\n",
      "3460/3460 - 10s - loss: 106.9339 - val_loss: 234.0890\n",
      "Epoch 26/50\n",
      "3460/3460 - 10s - loss: 106.6630 - val_loss: 232.6892\n",
      "Epoch 27/50\n",
      "3460/3460 - 10s - loss: 106.5108 - val_loss: 227.4476\n",
      "Epoch 28/50\n",
      "3460/3460 - 10s - loss: 106.4294 - val_loss: 227.8480\n",
      "Epoch 29/50\n",
      "3460/3460 - 11s - loss: 106.2507 - val_loss: 221.5953\n",
      "Epoch 30/50\n",
      "3460/3460 - 11s - loss: 106.0211 - val_loss: 221.7846\n",
      "Epoch 31/50\n",
      "3460/3460 - 10s - loss: 105.9052 - val_loss: 222.9612\n",
      "Epoch 32/50\n",
      "3460/3460 - 10s - loss: 105.6699 - val_loss: 222.8650\n",
      "Epoch 33/50\n",
      "3460/3460 - 11s - loss: 105.6087 - val_loss: 225.4101\n",
      "Epoch 34/50\n",
      "3460/3460 - 11s - loss: 105.4622 - val_loss: 219.1285\n",
      "Epoch 35/50\n",
      "3460/3460 - 11s - loss: 105.3116 - val_loss: 221.0553\n",
      "Epoch 36/50\n",
      "3460/3460 - 10s - loss: 105.2061 - val_loss: 219.8575\n",
      "Epoch 37/50\n",
      "3460/3460 - 10s - loss: 105.0618 - val_loss: 217.1873\n",
      "Epoch 38/50\n",
      "3460/3460 - 10s - loss: 104.8838 - val_loss: 222.8829\n",
      "Epoch 39/50\n",
      "3460/3460 - 10s - loss: 104.8056 - val_loss: 216.4398\n",
      "Epoch 40/50\n",
      "3460/3460 - 10s - loss: 104.7182 - val_loss: 214.0090\n",
      "Epoch 41/50\n",
      "3460/3460 - 11s - loss: 104.5186 - val_loss: 216.1949\n",
      "Epoch 42/50\n",
      "3460/3460 - 10s - loss: 104.4863 - val_loss: 213.2382\n",
      "Epoch 43/50\n",
      "3460/3460 - 10s - loss: 104.3539 - val_loss: 212.5949\n",
      "Epoch 44/50\n",
      "3460/3460 - 10s - loss: 104.2789 - val_loss: 216.2281\n",
      "Epoch 45/50\n",
      "3460/3460 - 10s - loss: 104.3726 - val_loss: 215.2771\n",
      "Epoch 46/50\n",
      "3460/3460 - 11s - loss: 104.1709 - val_loss: 213.2985\n",
      "Epoch 47/50\n",
      "3460/3460 - 11s - loss: 104.1746 - val_loss: 210.3673\n",
      "Epoch 48/50\n",
      "3460/3460 - 11s - loss: 104.0443 - val_loss: 211.5464\n",
      "Epoch 49/50\n",
      "3460/3460 - 10s - loss: 103.9083 - val_loss: 207.1246\n",
      "Epoch 50/50\n",
      "3460/3460 - 10s - loss: 103.8316 - val_loss: 211.3335\n",
      "Training time for iteration 24: 525.2464535236359\n",
      "\n",
      "\n",
      "Iteration  25\n",
      "Layer size = 300, activation = sigmoid, batch size = 1024, dropout = 0.2\n",
      "Epoch 1/50\n",
      "3460/3460 - 11s - loss: 342.6703 - val_loss: 340.2276\n",
      "Epoch 2/50\n",
      "3460/3460 - 10s - loss: 181.6542 - val_loss: 322.1986\n",
      "Epoch 3/50\n",
      "3460/3460 - 10s - loss: 145.6463 - val_loss: 327.2871\n",
      "Epoch 4/50\n",
      "3460/3460 - 10s - loss: 134.7863 - val_loss: 320.0558\n",
      "Epoch 5/50\n",
      "3460/3460 - 10s - loss: 130.0207 - val_loss: 312.2029\n",
      "Epoch 6/50\n",
      "3460/3460 - 10s - loss: 127.0520 - val_loss: 306.0036\n",
      "Epoch 7/50\n",
      "3460/3460 - 10s - loss: 124.7590 - val_loss: 300.3567\n",
      "Epoch 8/50\n",
      "3460/3460 - 10s - loss: 123.2254 - val_loss: 295.1581\n",
      "Epoch 9/50\n",
      "3460/3460 - 10s - loss: 122.1115 - val_loss: 283.9239\n",
      "Epoch 10/50\n",
      "3460/3460 - 10s - loss: 121.2820 - val_loss: 284.0621\n",
      "Epoch 11/50\n",
      "3460/3460 - 10s - loss: 120.4998 - val_loss: 276.4525\n",
      "Epoch 12/50\n",
      "3460/3460 - 11s - loss: 119.9705 - val_loss: 277.1504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "3460/3460 - 11s - loss: 119.2382 - val_loss: 275.0432\n",
      "Epoch 14/50\n",
      "3460/3460 - 10s - loss: 118.8303 - val_loss: 268.3265\n",
      "Epoch 15/50\n",
      "3460/3460 - 10s - loss: 118.3128 - val_loss: 263.8106\n",
      "Epoch 16/50\n",
      "3460/3460 - 10s - loss: 117.9671 - val_loss: 259.5367\n",
      "Epoch 17/50\n",
      "3460/3460 - 10s - loss: 117.7393 - val_loss: 260.7649\n",
      "Epoch 18/50\n",
      "3460/3460 - 10s - loss: 117.4314 - val_loss: 260.0495\n",
      "Epoch 19/50\n",
      "3460/3460 - 10s - loss: 117.0715 - val_loss: 255.7380\n",
      "Epoch 20/50\n",
      "3460/3460 - 10s - loss: 116.9141 - val_loss: 254.2758\n",
      "Epoch 21/50\n",
      "3460/3460 - 10s - loss: 116.7060 - val_loss: 251.3938\n",
      "Epoch 22/50\n",
      "3460/3460 - 10s - loss: 116.4632 - val_loss: 252.6469\n",
      "Epoch 23/50\n",
      "3460/3460 - 10s - loss: 116.2617 - val_loss: 251.0083\n",
      "Epoch 24/50\n",
      "3460/3460 - 10s - loss: 115.9223 - val_loss: 250.8032\n",
      "Epoch 25/50\n",
      "3460/3460 - 10s - loss: 115.8838 - val_loss: 247.5342\n",
      "Epoch 26/50\n",
      "3460/3460 - 10s - loss: 115.5854 - val_loss: 247.5060\n",
      "Epoch 27/50\n",
      "3460/3460 - 10s - loss: 115.3360 - val_loss: 241.9401\n",
      "Epoch 28/50\n",
      "3460/3460 - 10s - loss: 115.2528 - val_loss: 241.3949\n",
      "Epoch 29/50\n",
      "3460/3460 - 10s - loss: 115.1404 - val_loss: 236.5157\n",
      "Epoch 30/50\n",
      "3460/3460 - 10s - loss: 114.8777 - val_loss: 236.4352\n",
      "Epoch 31/50\n",
      "3460/3460 - 10s - loss: 114.6780 - val_loss: 235.6484\n",
      "Epoch 32/50\n",
      "3460/3460 - 10s - loss: 114.4466 - val_loss: 236.4540\n",
      "Epoch 33/50\n",
      "3460/3460 - 10s - loss: 114.3593 - val_loss: 237.7194\n",
      "Epoch 34/50\n",
      "3460/3460 - 10s - loss: 114.1674 - val_loss: 231.9039\n",
      "Epoch 35/50\n",
      "3460/3460 - 10s - loss: 114.1607 - val_loss: 233.5980\n",
      "Epoch 36/50\n",
      "3460/3460 - 10s - loss: 113.9343 - val_loss: 230.4438\n",
      "Epoch 37/50\n",
      "3460/3460 - 10s - loss: 113.7595 - val_loss: 227.7283\n",
      "Epoch 38/50\n",
      "3460/3460 - 10s - loss: 113.6556 - val_loss: 232.5406\n",
      "Epoch 39/50\n",
      "3460/3460 - 10s - loss: 113.4172 - val_loss: 227.2720\n",
      "Epoch 40/50\n",
      "3460/3460 - 10s - loss: 113.2286 - val_loss: 227.2335\n",
      "Epoch 41/50\n",
      "3460/3460 - 10s - loss: 113.1384 - val_loss: 228.4767\n",
      "Epoch 42/50\n",
      "3460/3460 - 10s - loss: 113.1657 - val_loss: 224.2290\n",
      "Epoch 43/50\n",
      "3460/3460 - 11s - loss: 112.9994 - val_loss: 223.7741\n",
      "Epoch 44/50\n",
      "3460/3460 - 10s - loss: 112.8781 - val_loss: 227.5309\n",
      "Epoch 45/50\n",
      "3460/3460 - 11s - loss: 112.6190 - val_loss: 228.0078\n",
      "Epoch 46/50\n",
      "3460/3460 - 11s - loss: 112.4907 - val_loss: 221.9028\n",
      "Epoch 47/50\n",
      "3460/3460 - 11s - loss: 112.3172 - val_loss: 221.9991\n",
      "Epoch 48/50\n",
      "3460/3460 - 11s - loss: 112.1539 - val_loss: 222.3884\n",
      "Epoch 49/50\n",
      "3460/3460 - 11s - loss: 112.0155 - val_loss: 218.4574\n",
      "Epoch 50/50\n",
      "3460/3460 - 11s - loss: 111.9639 - val_loss: 222.4608\n",
      "Training time for iteration 25: 523.5525553226471\n",
      "\n",
      "\n",
      "Iteration  26\n",
      "Layer size = 500, activation = sigmoid, batch size = 4096, dropout = 0.1\n",
      "Epoch 1/50\n",
      "865/865 - 7s - loss: 547.1341 - val_loss: 366.4914\n",
      "Epoch 2/50\n",
      "865/865 - 7s - loss: 245.5007 - val_loss: 347.5890\n",
      "Epoch 3/50\n",
      "865/865 - 7s - loss: 208.7061 - val_loss: 333.3951\n",
      "Epoch 4/50\n",
      "865/865 - 6s - loss: 178.1831 - val_loss: 322.2369\n",
      "Epoch 5/50\n",
      "865/865 - 7s - loss: 156.8518 - val_loss: 314.2198\n",
      "Epoch 6/50\n",
      "865/865 - 7s - loss: 144.1854 - val_loss: 306.2531\n",
      "Epoch 7/50\n",
      "865/865 - 6s - loss: 136.0562 - val_loss: 301.4362\n",
      "Epoch 8/50\n",
      "865/865 - 6s - loss: 130.5166 - val_loss: 299.2835\n",
      "Epoch 9/50\n",
      "865/865 - 6s - loss: 126.0748 - val_loss: 295.2990\n",
      "Epoch 10/50\n",
      "865/865 - 7s - loss: 122.3826 - val_loss: 296.1387\n",
      "Epoch 11/50\n",
      "865/865 - 7s - loss: 119.4024 - val_loss: 290.4124\n",
      "Epoch 12/50\n",
      "865/865 - 6s - loss: 117.2015 - val_loss: 283.7242\n",
      "Epoch 13/50\n",
      "865/865 - 6s - loss: 115.2974 - val_loss: 284.1331\n",
      "Epoch 14/50\n",
      "865/865 - 7s - loss: 113.7987 - val_loss: 282.0192\n",
      "Epoch 15/50\n",
      "865/865 - 7s - loss: 112.4624 - val_loss: 273.5079\n",
      "Epoch 16/50\n",
      "865/865 - 7s - loss: 111.3766 - val_loss: 269.9147\n",
      "Epoch 17/50\n",
      "865/865 - 7s - loss: 110.4252 - val_loss: 267.3833\n",
      "Epoch 18/50\n",
      "865/865 - 7s - loss: 109.4972 - val_loss: 264.4577\n",
      "Epoch 19/50\n",
      "865/865 - 7s - loss: 108.7594 - val_loss: 262.8319\n",
      "Epoch 20/50\n",
      "865/865 - 6s - loss: 107.9846 - val_loss: 259.7116\n",
      "Epoch 21/50\n",
      "865/865 - 6s - loss: 107.3142 - val_loss: 256.3220\n",
      "Epoch 22/50\n",
      "865/865 - 6s - loss: 106.6563 - val_loss: 254.9635\n",
      "Epoch 23/50\n",
      "865/865 - 6s - loss: 106.0560 - val_loss: 251.8401\n",
      "Epoch 24/50\n",
      "865/865 - 7s - loss: 105.5420 - val_loss: 250.8850\n",
      "Epoch 25/50\n",
      "865/865 - 6s - loss: 105.1185 - val_loss: 245.9765\n",
      "Epoch 26/50\n",
      "865/865 - 7s - loss: 104.6039 - val_loss: 245.7576\n",
      "Epoch 27/50\n",
      "865/865 - 7s - loss: 104.2188 - val_loss: 242.0889\n",
      "Epoch 28/50\n",
      "865/865 - 7s - loss: 103.8040 - val_loss: 242.6622\n",
      "Epoch 29/50\n",
      "865/865 - 7s - loss: 103.3052 - val_loss: 238.3288\n",
      "Epoch 30/50\n",
      "865/865 - 7s - loss: 102.8999 - val_loss: 236.5312\n",
      "Epoch 31/50\n",
      "865/865 - 7s - loss: 102.5475 - val_loss: 233.8558\n",
      "Epoch 32/50\n",
      "865/865 - 7s - loss: 102.2758 - val_loss: 235.0158\n",
      "Epoch 33/50\n",
      "865/865 - 7s - loss: 101.9494 - val_loss: 231.6485\n",
      "Epoch 34/50\n",
      "865/865 - 6s - loss: 101.5026 - val_loss: 229.4530\n",
      "Epoch 35/50\n",
      "865/865 - 7s - loss: 101.3321 - val_loss: 225.5846\n",
      "Epoch 36/50\n",
      "865/865 - 7s - loss: 100.9380 - val_loss: 226.7301\n",
      "Epoch 37/50\n",
      "865/865 - 7s - loss: 100.7254 - val_loss: 225.8686\n",
      "Epoch 38/50\n",
      "865/865 - 7s - loss: 100.3988 - val_loss: 223.8383\n",
      "Epoch 39/50\n",
      "865/865 - 7s - loss: 100.3775 - val_loss: 218.5866\n",
      "Epoch 40/50\n",
      "865/865 - 7s - loss: 99.9914 - val_loss: 218.2261\n",
      "Epoch 41/50\n",
      "865/865 - 6s - loss: 99.8558 - val_loss: 219.4247\n",
      "Epoch 42/50\n",
      "865/865 - 7s - loss: 99.5450 - val_loss: 215.6147\n",
      "Epoch 43/50\n",
      "865/865 - 7s - loss: 99.3019 - val_loss: 217.7237\n",
      "Epoch 44/50\n",
      "865/865 - 7s - loss: 99.2204 - val_loss: 213.7140\n",
      "Epoch 45/50\n",
      "865/865 - 7s - loss: 99.0397 - val_loss: 217.5877\n",
      "Epoch 46/50\n",
      "865/865 - 7s - loss: 98.7942 - val_loss: 213.4824\n",
      "Epoch 47/50\n",
      "865/865 - 7s - loss: 98.5923 - val_loss: 212.5657\n",
      "Epoch 48/50\n",
      "865/865 - 7s - loss: 98.4649 - val_loss: 210.4150\n",
      "Epoch 49/50\n",
      "865/865 - 7s - loss: 98.2671 - val_loss: 210.3958\n",
      "Epoch 50/50\n",
      "865/865 - 7s - loss: 98.1954 - val_loss: 208.3189\n",
      "Training time for iteration 26: 327.77195978164673\n",
      "\n",
      "\n",
      "Iteration  27\n",
      "Layer size = 500, activation = sigmoid, batch size = 1024, dropout = 0.2\n",
      "Epoch 1/50\n",
      "3460/3460 - 13s - loss: 310.8859 - val_loss: 333.7245\n",
      "Epoch 2/50\n",
      "3460/3460 - 13s - loss: 162.8873 - val_loss: 319.0453\n",
      "Epoch 3/50\n",
      "3460/3460 - 13s - loss: 135.9140 - val_loss: 321.1044\n",
      "Epoch 4/50\n",
      "3460/3460 - 13s - loss: 127.0145 - val_loss: 312.3917\n",
      "Epoch 5/50\n",
      "3460/3460 - 13s - loss: 122.5879 - val_loss: 303.1257\n",
      "Epoch 6/50\n",
      "3460/3460 - 13s - loss: 119.9048 - val_loss: 296.9269\n",
      "Epoch 7/50\n",
      "3460/3460 - 13s - loss: 117.9141 - val_loss: 290.2020\n",
      "Epoch 8/50\n",
      "3460/3460 - 13s - loss: 116.5598 - val_loss: 279.5789\n",
      "Epoch 9/50\n",
      "3460/3460 - 13s - loss: 115.4069 - val_loss: 269.0614\n",
      "Epoch 10/50\n",
      "3460/3460 - 13s - loss: 114.5397 - val_loss: 269.4488\n",
      "Epoch 11/50\n",
      "3460/3460 - 12s - loss: 113.7616 - val_loss: 260.6541\n",
      "Epoch 12/50\n",
      "3460/3460 - 13s - loss: 113.2177 - val_loss: 261.6708\n",
      "Epoch 13/50\n",
      "3460/3460 - 13s - loss: 112.6128 - val_loss: 258.8799\n",
      "Epoch 14/50\n",
      "3460/3460 - 13s - loss: 112.3065 - val_loss: 254.1864\n",
      "Epoch 15/50\n",
      "3460/3460 - 13s - loss: 111.8868 - val_loss: 250.9242\n",
      "Epoch 16/50\n",
      "3460/3460 - 13s - loss: 111.4343 - val_loss: 246.1330\n",
      "Epoch 17/50\n",
      "3460/3460 - 13s - loss: 111.1367 - val_loss: 247.1784\n",
      "Epoch 18/50\n",
      "3460/3460 - 13s - loss: 110.8978 - val_loss: 241.8487\n",
      "Epoch 19/50\n",
      "3460/3460 - 13s - loss: 110.5740 - val_loss: 239.1087\n",
      "Epoch 20/50\n",
      "3460/3460 - 13s - loss: 110.0783 - val_loss: 235.7632\n",
      "Epoch 21/50\n",
      "3460/3460 - 12s - loss: 110.0713 - val_loss: 234.6568\n",
      "Epoch 22/50\n",
      "3460/3460 - 13s - loss: 109.7878 - val_loss: 233.5576\n",
      "Epoch 23/50\n",
      "3460/3460 - 13s - loss: 109.5316 - val_loss: 233.6535\n",
      "Epoch 24/50\n",
      "3460/3460 - 13s - loss: 109.3368 - val_loss: 229.7134\n",
      "Epoch 25/50\n",
      "3460/3460 - 13s - loss: 109.2224 - val_loss: 226.6983\n",
      "Epoch 26/50\n",
      "3460/3460 - 13s - loss: 108.9908 - val_loss: 226.4806\n",
      "Epoch 27/50\n",
      "3460/3460 - 13s - loss: 108.7599 - val_loss: 221.0468\n",
      "Epoch 28/50\n",
      "3460/3460 - 13s - loss: 108.5168 - val_loss: 222.6410\n",
      "Epoch 29/50\n",
      "3460/3460 - 13s - loss: 108.2671 - val_loss: 216.6820\n",
      "Epoch 30/50\n",
      "3460/3460 - 13s - loss: 108.1167 - val_loss: 217.3117\n",
      "Epoch 31/50\n",
      "3460/3460 - 13s - loss: 107.9986 - val_loss: 215.1400\n",
      "Epoch 32/50\n",
      "3460/3460 - 13s - loss: 107.6312 - val_loss: 216.3412\n",
      "Epoch 33/50\n",
      "3460/3460 - 13s - loss: 107.4912 - val_loss: 219.1829\n",
      "Epoch 34/50\n",
      "3460/3460 - 13s - loss: 107.1700 - val_loss: 210.9196\n",
      "Epoch 35/50\n",
      "3460/3460 - 13s - loss: 106.8347 - val_loss: 213.8318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50\n",
      "3460/3460 - 13s - loss: 106.6664 - val_loss: 213.0913\n",
      "Epoch 37/50\n",
      "3460/3460 - 13s - loss: 106.4311 - val_loss: 211.2146\n",
      "Epoch 38/50\n",
      "3460/3460 - 13s - loss: 106.1086 - val_loss: 214.8145\n",
      "Epoch 39/50\n",
      "3460/3460 - 12s - loss: 105.8066 - val_loss: 206.7649\n",
      "Epoch 40/50\n",
      "3460/3460 - 14s - loss: 105.6004 - val_loss: 207.8244\n",
      "Epoch 41/50\n",
      "3460/3460 - 13s - loss: 105.3723 - val_loss: 208.2259\n",
      "Epoch 42/50\n",
      "3460/3460 - 12s - loss: 105.0813 - val_loss: 205.2911\n",
      "Epoch 43/50\n",
      "3460/3460 - 13s - loss: 104.9110 - val_loss: 204.9417\n",
      "Epoch 44/50\n",
      "3460/3460 - 13s - loss: 104.5792 - val_loss: 209.2916\n",
      "Epoch 45/50\n",
      "3460/3460 - 12s - loss: 104.4322 - val_loss: 209.7850\n",
      "Epoch 46/50\n",
      "3460/3460 - 13s - loss: 104.3760 - val_loss: 208.8218\n",
      "Epoch 47/50\n",
      "3460/3460 - 13s - loss: 104.1234 - val_loss: 202.2343\n",
      "Epoch 48/50\n",
      "3460/3460 - 13s - loss: 104.0403 - val_loss: 204.7503\n",
      "Epoch 49/50\n",
      "3460/3460 - 13s - loss: 103.8496 - val_loss: 200.9763\n",
      "Epoch 50/50\n",
      "3460/3460 - 13s - loss: 103.5792 - val_loss: 203.9798\n",
      "Training time for iteration 27: 640.4545061588287\n",
      "\n",
      "\n",
      "Iteration  28\n",
      "Layer size = 300, activation = sigmoid, batch size = 2048, dropout = 0.3\n",
      "Epoch 1/50\n",
      "1730/1730 - 8s - loss: 449.6600 - val_loss: 355.4586\n",
      "Epoch 2/50\n",
      "1730/1730 - 7s - loss: 235.1368 - val_loss: 335.3306\n",
      "Epoch 3/50\n",
      "1730/1730 - 7s - loss: 192.1901 - val_loss: 324.1656\n",
      "Epoch 4/50\n",
      "1730/1730 - 7s - loss: 163.9809 - val_loss: 321.5506\n",
      "Epoch 5/50\n",
      "1730/1730 - 7s - loss: 149.4477 - val_loss: 319.9396\n",
      "Epoch 6/50\n",
      "1730/1730 - 7s - loss: 142.4168 - val_loss: 314.0508\n",
      "Epoch 7/50\n",
      "1730/1730 - 7s - loss: 138.3549 - val_loss: 309.9886\n",
      "Epoch 8/50\n",
      "1730/1730 - 7s - loss: 135.4799 - val_loss: 308.7982\n",
      "Epoch 9/50\n",
      "1730/1730 - 7s - loss: 133.2958 - val_loss: 299.4879\n",
      "Epoch 10/50\n",
      "1730/1730 - 7s - loss: 131.7462 - val_loss: 296.4290\n",
      "Epoch 11/50\n",
      "1730/1730 - 7s - loss: 130.4493 - val_loss: 294.9198\n",
      "Epoch 12/50\n",
      "1730/1730 - 7s - loss: 129.5752 - val_loss: 292.0846\n",
      "Epoch 13/50\n",
      "1730/1730 - 7s - loss: 128.9941 - val_loss: 289.5500\n",
      "Epoch 14/50\n",
      "1730/1730 - 7s - loss: 128.4002 - val_loss: 289.2665\n",
      "Epoch 15/50\n",
      "1730/1730 - 7s - loss: 127.6652 - val_loss: 282.0273\n",
      "Epoch 16/50\n",
      "1730/1730 - 7s - loss: 127.1663 - val_loss: 277.3612\n",
      "Epoch 17/50\n",
      "1730/1730 - 7s - loss: 126.8241 - val_loss: 276.0761\n",
      "Epoch 18/50\n",
      "1730/1730 - 7s - loss: 126.3225 - val_loss: 278.2549\n",
      "Epoch 19/50\n",
      "1730/1730 - 7s - loss: 126.0548 - val_loss: 275.1312\n",
      "Epoch 20/50\n",
      "1730/1730 - 7s - loss: 125.7827 - val_loss: 274.0939\n",
      "Epoch 21/50\n",
      "1730/1730 - 7s - loss: 125.5296 - val_loss: 271.4995\n",
      "Epoch 22/50\n",
      "1730/1730 - 7s - loss: 125.1459 - val_loss: 268.8202\n",
      "Epoch 23/50\n",
      "1730/1730 - 7s - loss: 124.8471 - val_loss: 268.1380\n",
      "Epoch 24/50\n",
      "1730/1730 - 7s - loss: 124.6894 - val_loss: 267.0997\n",
      "Epoch 25/50\n",
      "1730/1730 - 7s - loss: 124.3184 - val_loss: 262.2224\n",
      "Epoch 26/50\n",
      "1730/1730 - 7s - loss: 124.2344 - val_loss: 264.0955\n",
      "Epoch 27/50\n",
      "1730/1730 - 7s - loss: 124.0089 - val_loss: 261.6440\n",
      "Epoch 28/50\n",
      "1730/1730 - 7s - loss: 123.7900 - val_loss: 262.4050\n",
      "Epoch 29/50\n",
      "1730/1730 - 7s - loss: 123.6127 - val_loss: 260.0652\n",
      "Epoch 30/50\n",
      "1730/1730 - 7s - loss: 123.4486 - val_loss: 258.3159\n",
      "Epoch 31/50\n",
      "1730/1730 - 7s - loss: 123.3321 - val_loss: 257.5338\n",
      "Epoch 32/50\n",
      "1730/1730 - 7s - loss: 123.1104 - val_loss: 259.0829\n",
      "Epoch 33/50\n",
      "1730/1730 - 7s - loss: 122.9678 - val_loss: 258.3551\n",
      "Epoch 34/50\n",
      "1730/1730 - 8s - loss: 122.8846 - val_loss: 255.2195\n",
      "Epoch 35/50\n",
      "1730/1730 - 8s - loss: 122.6743 - val_loss: 253.5755\n",
      "Epoch 36/50\n",
      "1730/1730 - 8s - loss: 122.5517 - val_loss: 252.2168\n",
      "Epoch 37/50\n",
      "1730/1730 - 8s - loss: 122.3948 - val_loss: 249.0669\n",
      "Epoch 38/50\n",
      "1730/1730 - 7s - loss: 122.2419 - val_loss: 251.3972\n",
      "Epoch 39/50\n",
      "1730/1730 - 8s - loss: 122.2982 - val_loss: 248.8374\n",
      "Epoch 40/50\n",
      "1730/1730 - 7s - loss: 122.1098 - val_loss: 247.9239\n",
      "Epoch 41/50\n",
      "1730/1730 - 8s - loss: 122.0039 - val_loss: 247.9188\n",
      "Epoch 42/50\n",
      "1730/1730 - 8s - loss: 121.9444 - val_loss: 243.2970\n",
      "Epoch 43/50\n",
      "1730/1730 - 8s - loss: 121.7303 - val_loss: 246.4014\n",
      "Epoch 44/50\n",
      "1730/1730 - 8s - loss: 121.6632 - val_loss: 246.7293\n",
      "Epoch 45/50\n",
      "1730/1730 - 7s - loss: 121.4753 - val_loss: 247.0655\n",
      "Epoch 46/50\n",
      "1730/1730 - 7s - loss: 121.2982 - val_loss: 244.1897\n",
      "Epoch 47/50\n",
      "1730/1730 - 7s - loss: 121.3434 - val_loss: 243.4079\n",
      "Epoch 48/50\n",
      "1730/1730 - 7s - loss: 121.1964 - val_loss: 240.9855\n",
      "Epoch 49/50\n",
      "1730/1730 - 7s - loss: 121.1348 - val_loss: 241.0812\n",
      "Epoch 50/50\n",
      "1730/1730 - 7s - loss: 121.0201 - val_loss: 242.8405\n",
      "Training time for iteration 28: 367.73823285102844\n",
      "\n",
      "\n",
      "Iteration  29\n",
      "Layer size = 500, activation = tanh, batch size = 512, dropout = 0.1\n",
      "Epoch 1/50\n",
      "6920/6920 - 22s - loss: 240.6858 - val_loss: 321.9693\n",
      "Epoch 2/50\n",
      "6920/6920 - 22s - loss: 132.7161 - val_loss: 324.0514\n",
      "Epoch 3/50\n",
      "6920/6920 - 21s - loss: 119.7085 - val_loss: 299.1671\n",
      "Epoch 4/50\n",
      "6920/6920 - 21s - loss: 114.6202 - val_loss: 290.2107\n",
      "Epoch 5/50\n",
      "6920/6920 - 21s - loss: 111.7624 - val_loss: 283.7836\n",
      "Epoch 6/50\n",
      "6920/6920 - 21s - loss: 109.8514 - val_loss: 274.4890\n",
      "Epoch 7/50\n",
      "6920/6920 - 21s - loss: 108.5002 - val_loss: 269.1823\n",
      "Epoch 8/50\n",
      "6920/6920 - 21s - loss: 107.0572 - val_loss: 257.7565\n",
      "Epoch 9/50\n",
      "6920/6920 - 21s - loss: 106.1992 - val_loss: 250.3579\n",
      "Epoch 10/50\n",
      "6920/6920 - 21s - loss: 105.3799 - val_loss: 249.1037\n",
      "Epoch 11/50\n",
      "6920/6920 - 21s - loss: 104.5714 - val_loss: 238.4549\n",
      "Epoch 12/50\n",
      "6920/6920 - 21s - loss: 104.0312 - val_loss: 237.3279\n",
      "Epoch 13/50\n",
      "6920/6920 - 21s - loss: 103.4086 - val_loss: 226.4868\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-05af7c640d27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     history = model.fit(x_train_scaled, y_train,\n\u001b[0m\u001b[0;32m     29\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ITERATIONS = 60 \n",
    "\n",
    "results = pd.DataFrame(columns=['loss_train', 'loss_val', 'nodes', 'activation', 'batch_size', 'dropout', 'training_time'])  \n",
    "weights_file = 'mlp_weights.h5'\n",
    "# epochs = 200\n",
    "epochs = 50\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    layer_size = random.sample(hidden_layer_sizes, 1)[0]\n",
    "    activation = random.sample(activation_functions, 1)[0]\n",
    "    batch_size = random.sample(batch_sizes, 1)[0]\n",
    "    dropout = random.sample(dropout_list, 1)[0]\n",
    "    \n",
    "    print('\\n\\nIteration ', i+1)\n",
    "    print('Layer size = {}, activation = {}, batch size = {}, dropout = {}'.format(layer_size,\n",
    "                                                                                  activation,\n",
    "                                                                                  batch_size,\n",
    "                                                                                  dropout))\n",
    "    \n",
    "    input_dim = x_train_scaled.shape[1]\n",
    "    model = create_mlp_model(input_dim, layer_size, dropout, weights_file)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train_scaled, y_train,\n",
    "                        validation_data=(x_val_scaled, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=2)\n",
    "    training_time = time.time() - start_time\n",
    "    print(\"Training time for iteration {}: {}\".format(i + 1, training_time))\n",
    "    \n",
    "    mse_train = history.history['loss'][-1]\n",
    "    mse_val = history.history['val_loss'][-1]\n",
    "    \n",
    "    \n",
    "    # append results\n",
    "    d = {'loss_train': mse_train, \n",
    "         'loss_val': mse_val,\n",
    "         'nodes':layer_size,\n",
    "         'activation':activation, \n",
    "         'batch_size':batch_size, \n",
    "         'dropout': dropout,\n",
    "         'training_time': training_time}\n",
    "    results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results.to_csv(os.path.join(output_path, 'tuning_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>nodes</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141.270920</td>\n",
       "      <td>280.966309</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.3</td>\n",
       "      <td>174.071320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118.353813</td>\n",
       "      <td>228.756119</td>\n",
       "      <td>200</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.2</td>\n",
       "      <td>590.047491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140.220764</td>\n",
       "      <td>259.083313</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>512</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1009.990901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.891281</td>\n",
       "      <td>202.350983</td>\n",
       "      <td>300</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1004.642402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95.477684</td>\n",
       "      <td>193.647354</td>\n",
       "      <td>500</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.1</td>\n",
       "      <td>454.200945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>104.074989</td>\n",
       "      <td>221.310486</td>\n",
       "      <td>300</td>\n",
       "      <td>tanh</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.1</td>\n",
       "      <td>255.013270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>132.791992</td>\n",
       "      <td>260.311462</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.4</td>\n",
       "      <td>511.600897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>120.163582</td>\n",
       "      <td>238.935272</td>\n",
       "      <td>500</td>\n",
       "      <td>tanh</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.4</td>\n",
       "      <td>328.075791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>103.352516</td>\n",
       "      <td>208.065033</td>\n",
       "      <td>300</td>\n",
       "      <td>tanh</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.1</td>\n",
       "      <td>515.796587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>62.950584</td>\n",
       "      <td>141.391449</td>\n",
       "      <td>500</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1021.912720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>153.152374</td>\n",
       "      <td>269.528259</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.3</td>\n",
       "      <td>497.770681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>112.729492</td>\n",
       "      <td>231.886185</td>\n",
       "      <td>500</td>\n",
       "      <td>relu</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.3</td>\n",
       "      <td>331.682467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>118.668739</td>\n",
       "      <td>238.813873</td>\n",
       "      <td>500</td>\n",
       "      <td>tanh</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.4</td>\n",
       "      <td>456.881135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>170.078918</td>\n",
       "      <td>281.416718</td>\n",
       "      <td>50</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>512</td>\n",
       "      <td>0.4</td>\n",
       "      <td>970.743407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>83.029968</td>\n",
       "      <td>212.141541</td>\n",
       "      <td>200</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>281.974560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>127.479202</td>\n",
       "      <td>256.857849</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.3</td>\n",
       "      <td>509.458706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>168.149063</td>\n",
       "      <td>286.095032</td>\n",
       "      <td>50</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.4</td>\n",
       "      <td>251.134242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>126.615852</td>\n",
       "      <td>241.075394</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>512</td>\n",
       "      <td>0.2</td>\n",
       "      <td>977.690955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>94.953720</td>\n",
       "      <td>182.134933</td>\n",
       "      <td>500</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.1</td>\n",
       "      <td>646.804245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>131.462234</td>\n",
       "      <td>259.048645</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>512</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1008.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>69.532990</td>\n",
       "      <td>153.541199</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1004.871088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>74.401802</td>\n",
       "      <td>177.493820</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>508.195519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>80.147232</td>\n",
       "      <td>194.867035</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>968.885242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>103.831635</td>\n",
       "      <td>211.333496</td>\n",
       "      <td>300</td>\n",
       "      <td>relu</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.1</td>\n",
       "      <td>525.246454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>111.963898</td>\n",
       "      <td>222.460754</td>\n",
       "      <td>300</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.2</td>\n",
       "      <td>523.552555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>98.195419</td>\n",
       "      <td>208.318863</td>\n",
       "      <td>500</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.1</td>\n",
       "      <td>327.771960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>103.579170</td>\n",
       "      <td>203.979828</td>\n",
       "      <td>500</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.2</td>\n",
       "      <td>640.454506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>121.020096</td>\n",
       "      <td>242.840530</td>\n",
       "      <td>300</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.3</td>\n",
       "      <td>367.738233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss_train    loss_val nodes activation batch_size  dropout  training_time\n",
       "0   141.270920  280.966309   100       relu       4096      0.3     174.071320\n",
       "1   118.353813  228.756119   200    sigmoid       1024      0.2     590.047491\n",
       "2   140.220764  259.083313    50       relu        512      0.2    1009.990901\n",
       "3   101.891281  202.350983   300       tanh        512      0.1    1004.642402\n",
       "4    95.477684  193.647354   500    sigmoid       2048      0.1     454.200945\n",
       "5   104.074989  221.310486   300       tanh       4096      0.1     255.013270\n",
       "6   132.791992  260.311462   200       relu       1024      0.4     511.600897\n",
       "7   120.163582  238.935272   500       tanh       4096      0.4     328.075791\n",
       "8   103.352516  208.065033   300       tanh       1024      0.1     515.796587\n",
       "9    62.950584  141.391449   500       tanh        512      0.0    1021.912720\n",
       "10  153.152374  269.528259    50       relu       1024      0.3     497.770681\n",
       "11  112.729492  231.886185   500       relu       4096      0.3     331.682467\n",
       "12  118.668739  238.813873   500       tanh       2048      0.4     456.881135\n",
       "13  170.078918  281.416718    50    sigmoid        512      0.4     970.743407\n",
       "14   83.029968  212.141541   200    sigmoid       2048      0.0     281.974560\n",
       "15  127.479202  256.857849   200       tanh       1024      0.3     509.458706\n",
       "16  168.149063  286.095032    50    sigmoid       2048      0.4     251.134242\n",
       "17  126.615852  241.075394   100    sigmoid        512      0.2     977.690955\n",
       "18   94.953720  182.134933   500    sigmoid       1024      0.1     646.804245\n",
       "19  131.462234  259.048645   200       relu        512      0.4    1008.085000\n",
       "20   69.532990  153.541199   200       tanh        512      0.0    1004.871088\n",
       "21   74.401802  177.493820   200       tanh       1024      0.0     508.195519\n",
       "22   80.147232  194.867035   100    sigmoid        512      0.0     968.885242\n",
       "23  103.831635  211.333496   300       relu       1024      0.1     525.246454\n",
       "24  111.963898  222.460754   300    sigmoid       1024      0.2     523.552555\n",
       "25   98.195419  208.318863   500    sigmoid       4096      0.1     327.771960\n",
       "26  103.579170  203.979828   500    sigmoid       1024      0.2     640.454506\n",
       "27  121.020096  242.840530   300    sigmoid       2048      0.3     367.738233"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = results[results['loss_val'] == results['loss_val'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = int(best_params['nodes'])\n",
    "activation = best_params['activation']\n",
    "batch_size = int(best_params['batch_size'])\n",
    "dropout = float(best_params['dropout'])\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(input_dim, hidden_layer_sizes, weights_file, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_sizes[0], \n",
    "                    input_dim=input_dim, \n",
    "                    kernel_initializer='random_normal', \n",
    "                    activation=activation))\n",
    "\n",
    "    for layer_size in hidden_layer_sizes[1:]:\n",
    "        model.add(Dense(layer_size, \n",
    "                        kernel_initializer='random_normal', \n",
    "                        activation=activation))\n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.save_weights(weights_file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 500)               3000      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 3,501\n",
      "Trainable params: 3,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "6920/6920 [==============================] - ETA: 0s - loss: 415.3812\n",
      "Epoch 00001: val_loss improved from inf to 387.10233, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 22s 3ms/step - loss: 415.3812 - val_loss: 387.1023\n",
      "Epoch 2/200\n",
      "6915/6920 [============================>.] - ETA: 0s - loss: 287.6894\n",
      "Epoch 00002: val_loss improved from 387.10233 to 373.24988, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 287.6713 - val_loss: 373.2499\n",
      "Epoch 3/200\n",
      "6899/6920 [============================>.] - ETA: 0s - loss: 247.3378\n",
      "Epoch 00003: val_loss improved from 373.24988 to 353.74442, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 247.2879 - val_loss: 353.7444\n",
      "Epoch 4/200\n",
      "6904/6920 [============================>.] - ETA: 0s - loss: 212.6861\n",
      "Epoch 00004: val_loss improved from 353.74442 to 328.54443, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 212.6378 - val_loss: 328.5444\n",
      "Epoch 5/200\n",
      "6903/6920 [============================>.] - ETA: 0s - loss: 183.1267\n",
      "Epoch 00005: val_loss improved from 328.54443 to 311.00430, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 183.0977 - val_loss: 311.0043\n",
      "Epoch 6/200\n",
      "6900/6920 [============================>.] - ETA: 0s - loss: 161.4768\n",
      "Epoch 00006: val_loss improved from 311.00430 to 310.38654, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 161.4575 - val_loss: 310.3865\n",
      "Epoch 7/200\n",
      "6913/6920 [============================>.] - ETA: 0s - loss: 147.4777\n",
      "Epoch 00007: val_loss did not improve from 310.38654\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 147.4670 - val_loss: 312.3908\n",
      "Epoch 8/200\n",
      "6915/6920 [============================>.] - ETA: 0s - loss: 138.8325\n",
      "Epoch 00008: val_loss improved from 310.38654 to 300.82053, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 138.8295 - val_loss: 300.8205\n",
      "Epoch 9/200\n",
      "6902/6920 [============================>.] - ETA: 0s - loss: 133.1343\n",
      "Epoch 00009: val_loss did not improve from 300.82053\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 133.1267 - val_loss: 301.4733\n",
      "Epoch 10/200\n",
      "6896/6920 [============================>.] - ETA: 0s - loss: 129.0764\n",
      "Epoch 00010: val_loss improved from 300.82053 to 296.88489, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 129.0624 - val_loss: 296.8849\n",
      "Epoch 11/200\n",
      "6911/6920 [============================>.] - ETA: 0s - loss: 125.5661\n",
      "Epoch 00011: val_loss did not improve from 296.88489\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 125.5695 - val_loss: 304.3019\n",
      "Epoch 12/200\n",
      "6913/6920 [============================>.] - ETA: 0s - loss: 122.9011\n",
      "Epoch 00012: val_loss did not improve from 296.88489\n",
      "6920/6920 [==============================] - 22s 3ms/step - loss: 122.8969 - val_loss: 324.0887\n",
      "Epoch 13/200\n",
      "6906/6920 [============================>.] - ETA: 0s - loss: 120.7847\n",
      "Epoch 00013: val_loss did not improve from 296.88489\n",
      "6920/6920 [==============================] - 22s 3ms/step - loss: 120.7883 - val_loss: 300.7075\n",
      "Epoch 14/200\n",
      "6911/6920 [============================>.] - ETA: 0s - loss: 118.7802\n",
      "Epoch 00014: val_loss did not improve from 296.88489\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 118.7760 - val_loss: 299.8221\n",
      "Epoch 15/200\n",
      "6920/6920 [==============================] - ETA: 0s - loss: 117.1158\n",
      "Epoch 00015: val_loss did not improve from 296.88489\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 117.1158 - val_loss: 302.5363\n",
      "Epoch 16/200\n",
      "6913/6920 [============================>.] - ETA: 0s - loss: 115.5673\n",
      "Epoch 00016: val_loss improved from 296.88489 to 291.55722, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 22s 3ms/step - loss: 115.5666 - val_loss: 291.5572\n",
      "Epoch 17/200\n",
      "6904/6920 [============================>.] - ETA: 0s - loss: 114.2866\n",
      "Epoch 00017: val_loss did not improve from 291.55722\n",
      "6920/6920 [==============================] - 23s 3ms/step - loss: 114.2998 - val_loss: 307.2163\n",
      "Epoch 18/200\n",
      "6919/6920 [============================>.] - ETA: 0s - loss: 113.2703\n",
      "Epoch 00018: val_loss improved from 291.55722 to 278.18332, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 22s 3ms/step - loss: 113.2702 - val_loss: 278.1833\n",
      "Epoch 19/200\n",
      "6914/6920 [============================>.] - ETA: 0s - loss: 112.2016\n",
      "Epoch 00019: val_loss did not improve from 278.18332\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 112.2060 - val_loss: 316.1910\n",
      "Epoch 20/200\n",
      "6912/6920 [============================>.] - ETA: 0s - loss: 111.1865\n",
      "Epoch 00020: val_loss did not improve from 278.18332\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 111.1876 - val_loss: 285.1362\n",
      "Epoch 21/200\n",
      "6915/6920 [============================>.] - ETA: 0s - loss: 110.3217\n",
      "Epoch 00021: val_loss did not improve from 278.18332\n",
      "6920/6920 [==============================] - 19s 3ms/step - loss: 110.3199 - val_loss: 301.3127\n",
      "Epoch 22/200\n",
      "6897/6920 [============================>.] - ETA: 0s - loss: 109.4765\n",
      "Epoch 00022: val_loss did not improve from 278.18332\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 109.4752 - val_loss: 279.5232\n",
      "Epoch 23/200\n",
      "6917/6920 [============================>.] - ETA: 0s - loss: 108.7985\n",
      "Epoch 00023: val_loss did not improve from 278.18332\n",
      "6920/6920 [==============================] - 19s 3ms/step - loss: 108.7992 - val_loss: 282.9576\n",
      "Epoch 24/200\n",
      "6913/6920 [============================>.] - ETA: 0s - loss: 108.0029\n",
      "Epoch 00024: val_loss did not improve from 278.18332\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 108.0045 - val_loss: 286.1994\n",
      "Epoch 25/200\n",
      "6919/6920 [============================>.] - ETA: 0s - loss: 107.3670\n",
      "Epoch 00025: val_loss improved from 278.18332 to 273.59335, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 107.3670 - val_loss: 273.5934\n",
      "Epoch 26/200\n",
      "6905/6920 [============================>.] - ETA: 0s - loss: 106.8303\n",
      "Epoch 00026: val_loss improved from 273.59335 to 269.27716, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 106.8299 - val_loss: 269.2772\n",
      "Epoch 27/200\n",
      "6907/6920 [============================>.] - ETA: 0s - loss: 106.3122\n",
      "Epoch 00027: val_loss improved from 269.27716 to 269.16275, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 106.3149 - val_loss: 269.1628\n",
      "Epoch 28/200\n",
      "6903/6920 [============================>.] - ETA: 0s - loss: 105.7473\n",
      "Epoch 00028: val_loss did not improve from 269.16275\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 105.7422 - val_loss: 272.7406\n",
      "Epoch 29/200\n",
      "6910/6920 [============================>.] - ETA: 0s - loss: 105.2455\n",
      "Epoch 00029: val_loss did not improve from 269.16275\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 105.2404 - val_loss: 275.6768\n",
      "Epoch 30/200\n",
      "6902/6920 [============================>.] - ETA: 0s - loss: 104.8198\n",
      "Epoch 00030: val_loss did not improve from 269.16275\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 104.8273 - val_loss: 275.1062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200\n",
      "6917/6920 [============================>.] - ETA: 0s - loss: 104.3065\n",
      "Epoch 00031: val_loss did not improve from 269.16275\n",
      "6920/6920 [==============================] - 22s 3ms/step - loss: 104.3051 - val_loss: 302.7019\n",
      "Epoch 32/200\n",
      "6915/6920 [============================>.] - ETA: 0s - loss: 103.9529\n",
      "Epoch 00032: val_loss did not improve from 269.16275\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 103.9513 - val_loss: 276.0993\n",
      "Epoch 33/200\n",
      "6906/6920 [============================>.] - ETA: 0s - loss: 103.5550\n",
      "Epoch 00033: val_loss did not improve from 269.16275\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 103.5563 - val_loss: 269.9035\n",
      "Epoch 34/200\n",
      "6899/6920 [============================>.] - ETA: 0s - loss: 103.1470\n",
      "Epoch 00034: val_loss improved from 269.16275 to 267.84283, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 103.1423 - val_loss: 267.8428\n",
      "Epoch 35/200\n",
      "6895/6920 [============================>.] - ETA: 0s - loss: 102.7148\n",
      "Epoch 00035: val_loss did not improve from 267.84283\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 102.7270 - val_loss: 280.7878\n",
      "Epoch 36/200\n",
      "6906/6920 [============================>.] - ETA: 0s - loss: 102.3396\n",
      "Epoch 00036: val_loss improved from 267.84283 to 263.44962, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 102.3442 - val_loss: 263.4496\n",
      "Epoch 37/200\n",
      "6908/6920 [============================>.] - ETA: 0s - loss: 102.0008\n",
      "Epoch 00037: val_loss did not improve from 263.44962\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 102.0006 - val_loss: 278.8014\n",
      "Epoch 38/200\n",
      "6911/6920 [============================>.] - ETA: 0s - loss: 101.6318\n",
      "Epoch 00038: val_loss did not improve from 263.44962\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 101.6303 - val_loss: 283.7514\n",
      "Epoch 39/200\n",
      "6903/6920 [============================>.] - ETA: 0s - loss: 101.3450\n",
      "Epoch 00039: val_loss did not improve from 263.44962\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 101.3408 - val_loss: 268.5031\n",
      "Epoch 40/200\n",
      "6917/6920 [============================>.] - ETA: 0s - loss: 100.9497\n",
      "Epoch 00040: val_loss improved from 263.44962 to 262.87595, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 100.9491 - val_loss: 262.8759\n",
      "Epoch 41/200\n",
      "6899/6920 [============================>.] - ETA: 0s - loss: 100.6700\n",
      "Epoch 00041: val_loss did not improve from 262.87595\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 100.6740 - val_loss: 266.9922\n",
      "Epoch 42/200\n",
      "6893/6920 [============================>.] - ETA: 0s - loss: 100.3276\n",
      "Epoch 00042: val_loss improved from 262.87595 to 253.76591, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 100.3361 - val_loss: 253.7659\n",
      "Epoch 43/200\n",
      "6907/6920 [============================>.] - ETA: 0s - loss: 99.9627\n",
      "Epoch 00043: val_loss did not improve from 253.76591\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 99.9649 - val_loss: 272.8618\n",
      "Epoch 44/200\n",
      "6909/6920 [============================>.] - ETA: 0s - loss: 99.7477\n",
      "Epoch 00044: val_loss did not improve from 253.76591\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 99.7453 - val_loss: 275.1418\n",
      "Epoch 45/200\n",
      "6917/6920 [============================>.] - ETA: 0s - loss: 99.4479\n",
      "Epoch 00045: val_loss did not improve from 253.76591\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 99.4464 - val_loss: 264.0890\n",
      "Epoch 46/200\n",
      "6911/6920 [============================>.] - ETA: 0s - loss: 99.1948\n",
      "Epoch 00046: val_loss improved from 253.76591 to 252.68758, saving model to DS02\\mlp_model_baseline.h5\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 99.1955 - val_loss: 252.6876\n",
      "Epoch 47/200\n",
      "6906/6920 [============================>.] - ETA: 0s - loss: 98.9622\n",
      "Epoch 00047: val_loss did not improve from 252.68758\n",
      "6920/6920 [==============================] - 21s 3ms/step - loss: 98.9569 - val_loss: 259.1261\n",
      "Epoch 48/200\n",
      "6916/6920 [============================>.] - ETA: 0s - loss: 98.7049\n",
      "Epoch 00048: val_loss did not improve from 252.68758\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 98.7052 - val_loss: 257.2215\n",
      "Epoch 49/200\n",
      "6919/6920 [============================>.] - ETA: 0s - loss: 98.4885\n",
      "Epoch 00049: val_loss did not improve from 252.68758\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 98.4887 - val_loss: 266.0333\n",
      "Epoch 50/200\n",
      "6905/6920 [============================>.] - ETA: 0s - loss: 98.2712\n",
      "Epoch 00050: val_loss did not improve from 252.68758\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 98.2622 - val_loss: 274.5539\n",
      "Epoch 51/200\n",
      "6899/6920 [============================>.] - ETA: 0s - loss: 98.0581\n",
      "Epoch 00051: val_loss did not improve from 252.68758\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 98.0576 - val_loss: 262.8862\n",
      "Epoch 52/200\n",
      "6907/6920 [============================>.] - ETA: 0s - loss: 97.7611\n",
      "Epoch 00052: val_loss did not improve from 252.68758\n",
      "6920/6920 [==============================] - 20s 3ms/step - loss: 97.7611 - val_loss: 257.0723\n",
      "Epoch 53/200\n",
      "6908/6920 [============================>.] - ETA: 0s - loss: 97.6160"
     ]
    }
   ],
   "source": [
    "input_dim = x_train_scaled.shape[1]\n",
    "weights_file = os.path.join(output_path, 'mlp_weights.h5')\n",
    "\n",
    "layer_sizes = [500]\n",
    "epochs = 200\n",
    "batch_size = 512\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint(os.path.join(output_path, 'mlp_model_baseline.h5'), monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "model = create_mlp_model(input_dim, layer_sizes, weights_file)\n",
    "model.summary()\n",
    "    \n",
    "start_time = time.time()\n",
    "history = model.fit(x_train_scaled, y_train,\n",
    "                    validation_data=(x_val_scaled, y_val),\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=1,\n",
    "                    callbacks=[es, mc])\n",
    "training_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curves(history, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(os.path.join(output_path, 'mlp_model_baseline.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "MSE: 59.24740202282794\n",
      "RMSE: 7.697233400568541\n",
      "CMAPSS score: 7.493600192832575e+20\n",
      "R2: 0.8359611350418542\n"
     ]
    }
   ],
   "source": [
    "predictions_train = model.predict(x_train_scaled).flatten()\n",
    "evaluation(predictions_train, y_train, 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "MSE: 47.65085246603329\n",
      "RMSE: 6.902959688860517\n",
      "CMAPSS score: 8.532521388510335e+18\n",
      "R2: 0.8575293383323994\n"
     ]
    }
   ],
   "source": [
    "predictions_test = model.predict(x_test_scaled).flatten()\n",
    "evaluation(predictions_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set:\n",
      "MSE: 61.1245858578581\n",
      "RMSE: 7.818221399900242\n",
      "CMAPSS score: 7404883423.049065\n",
      "R2: 0.8317508134559316\n"
     ]
    }
   ],
   "source": [
    "predictions_val = model.predict(x_val_scaled).flatten()\n",
    "evaluation(predictions_val, y_val, 'Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wf</th>\n",
       "      <th>Nf</th>\n",
       "      <th>Nc</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T48</th>\n",
       "      <th>T50</th>\n",
       "      <th>P15</th>\n",
       "      <th>P2</th>\n",
       "      <th>P21</th>\n",
       "      <th>P24</th>\n",
       "      <th>Ps30</th>\n",
       "      <th>P40</th>\n",
       "      <th>P50</th>\n",
       "      <th>alt</th>\n",
       "      <th>TRA</th>\n",
       "      <th>T2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "      <td>4.495287e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.246982e+00</td>\n",
       "      <td>2.014315e+03</td>\n",
       "      <td>8.203450e+03</td>\n",
       "      <td>5.589538e+02</td>\n",
       "      <td>1.319009e+03</td>\n",
       "      <td>1.642581e+03</td>\n",
       "      <td>1.105524e+03</td>\n",
       "      <td>1.080502e+01</td>\n",
       "      <td>8.227008e+00</td>\n",
       "      <td>1.096956e+01</td>\n",
       "      <td>1.357597e+01</td>\n",
       "      <td>2.078485e+02</td>\n",
       "      <td>2.113974e+02</td>\n",
       "      <td>8.011887e+00</td>\n",
       "      <td>2.195349e+04</td>\n",
       "      <td>6.860208e+01</td>\n",
       "      <td>4.748620e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.669091e-01</td>\n",
       "      <td>1.384093e+02</td>\n",
       "      <td>1.839979e+02</td>\n",
       "      <td>1.797591e+01</td>\n",
       "      <td>5.547321e+01</td>\n",
       "      <td>9.837773e+01</td>\n",
       "      <td>5.166616e+01</td>\n",
       "      <td>2.145808e+00</td>\n",
       "      <td>1.766716e+00</td>\n",
       "      <td>2.178485e+00</td>\n",
       "      <td>2.617782e+00</td>\n",
       "      <td>4.382677e+01</td>\n",
       "      <td>4.447162e+01</td>\n",
       "      <td>1.900070e+00</td>\n",
       "      <td>6.313032e+03</td>\n",
       "      <td>1.453999e+01</td>\n",
       "      <td>1.665756e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.279842e-01</td>\n",
       "      <td>1.469742e+03</td>\n",
       "      <td>7.366108e+03</td>\n",
       "      <td>4.841972e+02</td>\n",
       "      <td>1.068823e+03</td>\n",
       "      <td>9.445005e+02</td>\n",
       "      <td>6.901948e+02</td>\n",
       "      <td>5.917596e+00</td>\n",
       "      <td>4.373175e+00</td>\n",
       "      <td>6.007711e+00</td>\n",
       "      <td>6.914424e+00</td>\n",
       "      <td>8.033988e+01</td>\n",
       "      <td>8.210175e+01</td>\n",
       "      <td>4.129309e+00</td>\n",
       "      <td>1.000100e+04</td>\n",
       "      <td>2.355452e+01</td>\n",
       "      <td>4.213779e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.864492e+00</td>\n",
       "      <td>1.959656e+03</td>\n",
       "      <td>8.105007e+03</td>\n",
       "      <td>5.466101e+02</td>\n",
       "      <td>1.288390e+03</td>\n",
       "      <td>1.593953e+03</td>\n",
       "      <td>1.072569e+03</td>\n",
       "      <td>8.976690e+00</td>\n",
       "      <td>6.672743e+00</td>\n",
       "      <td>9.113391e+00</td>\n",
       "      <td>1.145709e+01</td>\n",
       "      <td>1.773841e+02</td>\n",
       "      <td>1.804614e+02</td>\n",
       "      <td>6.371265e+00</td>\n",
       "      <td>1.651700e+04</td>\n",
       "      <td>5.941364e+01</td>\n",
       "      <td>4.605065e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.107214e+00</td>\n",
       "      <td>2.058522e+03</td>\n",
       "      <td>8.204305e+03</td>\n",
       "      <td>5.560642e+02</td>\n",
       "      <td>1.317866e+03</td>\n",
       "      <td>1.652645e+03</td>\n",
       "      <td>1.097053e+03</td>\n",
       "      <td>1.035381e+01</td>\n",
       "      <td>7.821310e+00</td>\n",
       "      <td>1.051148e+01</td>\n",
       "      <td>1.304733e+01</td>\n",
       "      <td>1.967872e+02</td>\n",
       "      <td>2.001841e+02</td>\n",
       "      <td>7.530022e+00</td>\n",
       "      <td>2.299900e+04</td>\n",
       "      <td>7.479439e+01</td>\n",
       "      <td>4.729416e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.558934e+00</td>\n",
       "      <td>2.111743e+03</td>\n",
       "      <td>8.316132e+03</td>\n",
       "      <td>5.710761e+02</td>\n",
       "      <td>1.352912e+03</td>\n",
       "      <td>1.700562e+03</td>\n",
       "      <td>1.137237e+03</td>\n",
       "      <td>1.248794e+01</td>\n",
       "      <td>9.670818e+00</td>\n",
       "      <td>1.267811e+01</td>\n",
       "      <td>1.543851e+01</td>\n",
       "      <td>2.338662e+02</td>\n",
       "      <td>2.378476e+02</td>\n",
       "      <td>9.497462e+00</td>\n",
       "      <td>2.799100e+04</td>\n",
       "      <td>7.962834e+01</td>\n",
       "      <td>4.892556e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.539436e+00</td>\n",
       "      <td>2.263477e+03</td>\n",
       "      <td>8.745187e+03</td>\n",
       "      <td>6.167966e+02</td>\n",
       "      <td>1.487275e+03</td>\n",
       "      <td>1.912816e+03</td>\n",
       "      <td>1.276669e+03</td>\n",
       "      <td>1.691886e+01</td>\n",
       "      <td>1.226862e+01</td>\n",
       "      <td>1.717651e+01</td>\n",
       "      <td>2.198497e+01</td>\n",
       "      <td>3.724845e+02</td>\n",
       "      <td>3.781559e+02</td>\n",
       "      <td>1.321004e+01</td>\n",
       "      <td>3.503300e+04</td>\n",
       "      <td>8.745055e+01</td>\n",
       "      <td>5.108146e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Wf            Nf            Nc           T24           T30  \\\n",
       "count  4.495287e+06  4.495287e+06  4.495287e+06  4.495287e+06  4.495287e+06   \n",
       "mean   2.246982e+00  2.014315e+03  8.203450e+03  5.589538e+02  1.319009e+03   \n",
       "std    5.669091e-01  1.384093e+02  1.839979e+02  1.797591e+01  5.547321e+01   \n",
       "min    3.279842e-01  1.469742e+03  7.366108e+03  4.841972e+02  1.068823e+03   \n",
       "25%    1.864492e+00  1.959656e+03  8.105007e+03  5.466101e+02  1.288390e+03   \n",
       "50%    2.107214e+00  2.058522e+03  8.204305e+03  5.560642e+02  1.317866e+03   \n",
       "75%    2.558934e+00  2.111743e+03  8.316132e+03  5.710761e+02  1.352912e+03   \n",
       "max    4.539436e+00  2.263477e+03  8.745187e+03  6.167966e+02  1.487275e+03   \n",
       "\n",
       "                T48           T50           P15            P2           P21  \\\n",
       "count  4.495287e+06  4.495287e+06  4.495287e+06  4.495287e+06  4.495287e+06   \n",
       "mean   1.642581e+03  1.105524e+03  1.080502e+01  8.227008e+00  1.096956e+01   \n",
       "std    9.837773e+01  5.166616e+01  2.145808e+00  1.766716e+00  2.178485e+00   \n",
       "min    9.445005e+02  6.901948e+02  5.917596e+00  4.373175e+00  6.007711e+00   \n",
       "25%    1.593953e+03  1.072569e+03  8.976690e+00  6.672743e+00  9.113391e+00   \n",
       "50%    1.652645e+03  1.097053e+03  1.035381e+01  7.821310e+00  1.051148e+01   \n",
       "75%    1.700562e+03  1.137237e+03  1.248794e+01  9.670818e+00  1.267811e+01   \n",
       "max    1.912816e+03  1.276669e+03  1.691886e+01  1.226862e+01  1.717651e+01   \n",
       "\n",
       "                P24          Ps30           P40           P50           alt  \\\n",
       "count  4.495287e+06  4.495287e+06  4.495287e+06  4.495287e+06  4.495287e+06   \n",
       "mean   1.357597e+01  2.078485e+02  2.113974e+02  8.011887e+00  2.195349e+04   \n",
       "std    2.617782e+00  4.382677e+01  4.447162e+01  1.900070e+00  6.313032e+03   \n",
       "min    6.914424e+00  8.033988e+01  8.210175e+01  4.129309e+00  1.000100e+04   \n",
       "25%    1.145709e+01  1.773841e+02  1.804614e+02  6.371265e+00  1.651700e+04   \n",
       "50%    1.304733e+01  1.967872e+02  2.001841e+02  7.530022e+00  2.299900e+04   \n",
       "75%    1.543851e+01  2.338662e+02  2.378476e+02  9.497462e+00  2.799100e+04   \n",
       "max    2.198497e+01  3.724845e+02  3.781559e+02  1.321004e+01  3.503300e+04   \n",
       "\n",
       "                TRA            T2  \n",
       "count  4.495287e+06  4.495287e+06  \n",
       "mean   6.860208e+01  4.748620e+02  \n",
       "std    1.453999e+01  1.665756e+01  \n",
       "min    2.355452e+01  4.213779e+02  \n",
       "25%    5.941364e+01  4.605065e+02  \n",
       "50%    7.479439e+01  4.729416e+02  \n",
       "75%    7.962834e+01  4.892556e+02  \n",
       "max    8.745055e+01  5.108146e+02  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wf</th>\n",
       "      <th>Nf</th>\n",
       "      <th>Nc</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T48</th>\n",
       "      <th>T50</th>\n",
       "      <th>P15</th>\n",
       "      <th>P2</th>\n",
       "      <th>P21</th>\n",
       "      <th>P24</th>\n",
       "      <th>Ps30</th>\n",
       "      <th>P40</th>\n",
       "      <th>P50</th>\n",
       "      <th>alt</th>\n",
       "      <th>TRA</th>\n",
       "      <th>T2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "      <td>768160.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.270443</td>\n",
       "      <td>2012.831376</td>\n",
       "      <td>8211.551444</td>\n",
       "      <td>559.670017</td>\n",
       "      <td>1321.243360</td>\n",
       "      <td>1642.971485</td>\n",
       "      <td>1107.960838</td>\n",
       "      <td>10.916905</td>\n",
       "      <td>8.320352</td>\n",
       "      <td>11.083152</td>\n",
       "      <td>13.701222</td>\n",
       "      <td>210.133980</td>\n",
       "      <td>213.736046</td>\n",
       "      <td>8.115639</td>\n",
       "      <td>21594.048002</td>\n",
       "      <td>68.227504</td>\n",
       "      <td>475.866390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.563023</td>\n",
       "      <td>140.225364</td>\n",
       "      <td>184.318054</td>\n",
       "      <td>17.705095</td>\n",
       "      <td>55.580225</td>\n",
       "      <td>99.447636</td>\n",
       "      <td>51.398941</td>\n",
       "      <td>2.091749</td>\n",
       "      <td>1.724774</td>\n",
       "      <td>2.123603</td>\n",
       "      <td>2.553201</td>\n",
       "      <td>43.249960</td>\n",
       "      <td>43.877733</td>\n",
       "      <td>1.855681</td>\n",
       "      <td>6133.974885</td>\n",
       "      <td>14.599670</td>\n",
       "      <td>16.136875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.670597</td>\n",
       "      <td>1486.269935</td>\n",
       "      <td>7366.466367</td>\n",
       "      <td>484.202497</td>\n",
       "      <td>1072.895343</td>\n",
       "      <td>1228.537245</td>\n",
       "      <td>884.040653</td>\n",
       "      <td>5.917624</td>\n",
       "      <td>4.373175</td>\n",
       "      <td>6.007740</td>\n",
       "      <td>6.914573</td>\n",
       "      <td>80.330756</td>\n",
       "      <td>82.093095</td>\n",
       "      <td>4.129153</td>\n",
       "      <td>10001.000000</td>\n",
       "      <td>24.609200</td>\n",
       "      <td>421.377893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.903019</td>\n",
       "      <td>1961.780536</td>\n",
       "      <td>8118.164924</td>\n",
       "      <td>547.650012</td>\n",
       "      <td>1292.238331</td>\n",
       "      <td>1596.036827</td>\n",
       "      <td>1075.764203</td>\n",
       "      <td>9.210827</td>\n",
       "      <td>6.871256</td>\n",
       "      <td>9.351093</td>\n",
       "      <td>11.715248</td>\n",
       "      <td>180.087763</td>\n",
       "      <td>183.215149</td>\n",
       "      <td>6.556973</td>\n",
       "      <td>16288.000000</td>\n",
       "      <td>59.677311</td>\n",
       "      <td>462.916438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.141862</td>\n",
       "      <td>2059.082747</td>\n",
       "      <td>8215.290024</td>\n",
       "      <td>557.539005</td>\n",
       "      <td>1321.138339</td>\n",
       "      <td>1654.725219</td>\n",
       "      <td>1100.371041</td>\n",
       "      <td>10.437394</td>\n",
       "      <td>7.897893</td>\n",
       "      <td>10.596339</td>\n",
       "      <td>13.137887</td>\n",
       "      <td>199.967092</td>\n",
       "      <td>203.388578</td>\n",
       "      <td>7.646297</td>\n",
       "      <td>22349.000000</td>\n",
       "      <td>74.354942</td>\n",
       "      <td>473.438795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.561952</td>\n",
       "      <td>2110.680316</td>\n",
       "      <td>8322.841360</td>\n",
       "      <td>571.306659</td>\n",
       "      <td>1354.594284</td>\n",
       "      <td>1699.374315</td>\n",
       "      <td>1138.016505</td>\n",
       "      <td>12.595039</td>\n",
       "      <td>9.787072</td>\n",
       "      <td>12.786842</td>\n",
       "      <td>15.529839</td>\n",
       "      <td>235.389358</td>\n",
       "      <td>239.447049</td>\n",
       "      <td>9.570440</td>\n",
       "      <td>26995.000000</td>\n",
       "      <td>79.628342</td>\n",
       "      <td>490.182431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.305223</td>\n",
       "      <td>2222.218747</td>\n",
       "      <td>8678.287213</td>\n",
       "      <td>610.637816</td>\n",
       "      <td>1468.537464</td>\n",
       "      <td>1898.923768</td>\n",
       "      <td>1284.410062</td>\n",
       "      <td>16.374098</td>\n",
       "      <td>12.216071</td>\n",
       "      <td>16.623450</td>\n",
       "      <td>21.114727</td>\n",
       "      <td>354.267472</td>\n",
       "      <td>359.529337</td>\n",
       "      <td>12.993743</td>\n",
       "      <td>35033.000000</td>\n",
       "      <td>87.626328</td>\n",
       "      <td>510.292714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Wf             Nf             Nc            T24  \\\n",
       "count  768160.000000  768160.000000  768160.000000  768160.000000   \n",
       "mean        2.270443    2012.831376    8211.551444     559.670017   \n",
       "std         0.563023     140.225364     184.318054      17.705095   \n",
       "min         0.670597    1486.269935    7366.466367     484.202497   \n",
       "25%         1.903019    1961.780536    8118.164924     547.650012   \n",
       "50%         2.141862    2059.082747    8215.290024     557.539005   \n",
       "75%         2.561952    2110.680316    8322.841360     571.306659   \n",
       "max         4.305223    2222.218747    8678.287213     610.637816   \n",
       "\n",
       "                 T30            T48            T50            P15  \\\n",
       "count  768160.000000  768160.000000  768160.000000  768160.000000   \n",
       "mean     1321.243360    1642.971485    1107.960838      10.916905   \n",
       "std        55.580225      99.447636      51.398941       2.091749   \n",
       "min      1072.895343    1228.537245     884.040653       5.917624   \n",
       "25%      1292.238331    1596.036827    1075.764203       9.210827   \n",
       "50%      1321.138339    1654.725219    1100.371041      10.437394   \n",
       "75%      1354.594284    1699.374315    1138.016505      12.595039   \n",
       "max      1468.537464    1898.923768    1284.410062      16.374098   \n",
       "\n",
       "                  P2            P21            P24           Ps30  \\\n",
       "count  768160.000000  768160.000000  768160.000000  768160.000000   \n",
       "mean        8.320352      11.083152      13.701222     210.133980   \n",
       "std         1.724774       2.123603       2.553201      43.249960   \n",
       "min         4.373175       6.007740       6.914573      80.330756   \n",
       "25%         6.871256       9.351093      11.715248     180.087763   \n",
       "50%         7.897893      10.596339      13.137887     199.967092   \n",
       "75%         9.787072      12.786842      15.529839     235.389358   \n",
       "max        12.216071      16.623450      21.114727     354.267472   \n",
       "\n",
       "                 P40            P50            alt            TRA  \\\n",
       "count  768160.000000  768160.000000  768160.000000  768160.000000   \n",
       "mean      213.736046       8.115639   21594.048002      68.227504   \n",
       "std        43.877733       1.855681    6133.974885      14.599670   \n",
       "min        82.093095       4.129153   10001.000000      24.609200   \n",
       "25%       183.215149       6.556973   16288.000000      59.677311   \n",
       "50%       203.388578       7.646297   22349.000000      74.354942   \n",
       "75%       239.447049       9.570440   26995.000000      79.628342   \n",
       "max       359.529337      12.993743   35033.000000      87.626328   \n",
       "\n",
       "                  T2  \n",
       "count  768160.000000  \n",
       "mean      475.866390  \n",
       "std        16.136875  \n",
       "min       421.377893  \n",
       "25%       462.916438  \n",
       "50%       473.438795  \n",
       "75%       490.182431  \n",
       "max       510.292714  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wf</th>\n",
       "      <th>Nf</th>\n",
       "      <th>Nc</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T48</th>\n",
       "      <th>T50</th>\n",
       "      <th>P15</th>\n",
       "      <th>P2</th>\n",
       "      <th>P21</th>\n",
       "      <th>P24</th>\n",
       "      <th>Ps30</th>\n",
       "      <th>P40</th>\n",
       "      <th>P50</th>\n",
       "      <th>alt</th>\n",
       "      <th>TRA</th>\n",
       "      <th>T2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "      <td>1.253743e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.403201e+00</td>\n",
       "      <td>1.999809e+03</td>\n",
       "      <td>8.230953e+03</td>\n",
       "      <td>5.649183e+02</td>\n",
       "      <td>1.327844e+03</td>\n",
       "      <td>1.648611e+03</td>\n",
       "      <td>1.118895e+03</td>\n",
       "      <td>1.174699e+01</td>\n",
       "      <td>9.020636e+00</td>\n",
       "      <td>1.192588e+01</td>\n",
       "      <td>1.467278e+01</td>\n",
       "      <td>2.223658e+02</td>\n",
       "      <td>2.262123e+02</td>\n",
       "      <td>8.851139e+00</td>\n",
       "      <td>1.910834e+04</td>\n",
       "      <td>6.555718e+01</td>\n",
       "      <td>4.822338e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.037376e-01</td>\n",
       "      <td>1.468721e+02</td>\n",
       "      <td>1.931254e+02</td>\n",
       "      <td>1.812375e+01</td>\n",
       "      <td>5.811396e+01</td>\n",
       "      <td>1.041488e+02</td>\n",
       "      <td>5.303728e+01</td>\n",
       "      <td>2.176814e+00</td>\n",
       "      <td>1.787953e+00</td>\n",
       "      <td>2.209963e+00</td>\n",
       "      <td>2.666418e+00</td>\n",
       "      <td>4.596197e+01</td>\n",
       "      <td>4.661605e+01</td>\n",
       "      <td>1.937761e+00</td>\n",
       "      <td>6.175012e+03</td>\n",
       "      <td>1.507279e+01</td>\n",
       "      <td>1.618800e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.416258e-01</td>\n",
       "      <td>1.471017e+03</td>\n",
       "      <td>7.396578e+03</td>\n",
       "      <td>5.045053e+02</td>\n",
       "      <td>1.070961e+03</td>\n",
       "      <td>9.540545e+02</td>\n",
       "      <td>6.976268e+02</td>\n",
       "      <td>7.152428e+00</td>\n",
       "      <td>5.507010e+00</td>\n",
       "      <td>7.261349e+00</td>\n",
       "      <td>8.598599e+00</td>\n",
       "      <td>8.225872e+01</td>\n",
       "      <td>8.577351e+01</td>\n",
       "      <td>4.949927e+00</td>\n",
       "      <td>1.000100e+04</td>\n",
       "      <td>2.355452e+01</td>\n",
       "      <td>4.416871e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.972566e+00</td>\n",
       "      <td>1.918627e+03</td>\n",
       "      <td>8.125872e+03</td>\n",
       "      <td>5.518893e+02</td>\n",
       "      <td>1.295088e+03</td>\n",
       "      <td>1.596720e+03</td>\n",
       "      <td>1.083440e+03</td>\n",
       "      <td>9.977838e+00</td>\n",
       "      <td>7.567901e+00</td>\n",
       "      <td>1.012979e+01</td>\n",
       "      <td>1.255894e+01</td>\n",
       "      <td>1.878106e+02</td>\n",
       "      <td>1.911433e+02</td>\n",
       "      <td>7.234730e+00</td>\n",
       "      <td>1.313600e+04</td>\n",
       "      <td>5.484336e+01</td>\n",
       "      <td>4.696360e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.278735e+00</td>\n",
       "      <td>2.033749e+03</td>\n",
       "      <td>8.234230e+03</td>\n",
       "      <td>5.626667e+02</td>\n",
       "      <td>1.327539e+03</td>\n",
       "      <td>1.657730e+03</td>\n",
       "      <td>1.112218e+03</td>\n",
       "      <td>1.150901e+01</td>\n",
       "      <td>8.810251e+00</td>\n",
       "      <td>1.168427e+01</td>\n",
       "      <td>1.432719e+01</td>\n",
       "      <td>2.122920e+02</td>\n",
       "      <td>2.159778e+02</td>\n",
       "      <td>8.589801e+00</td>\n",
       "      <td>1.941300e+04</td>\n",
       "      <td>6.996044e+01</td>\n",
       "      <td>4.817885e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.780043e+00</td>\n",
       "      <td>2.115296e+03</td>\n",
       "      <td>8.356755e+03</td>\n",
       "      <td>5.775744e+02</td>\n",
       "      <td>1.365424e+03</td>\n",
       "      <td>1.714233e+03</td>\n",
       "      <td>1.154108e+03</td>\n",
       "      <td>1.354695e+01</td>\n",
       "      <td>1.067996e+01</td>\n",
       "      <td>1.375325e+01</td>\n",
       "      <td>1.672680e+01</td>\n",
       "      <td>2.532535e+02</td>\n",
       "      <td>2.575437e+02</td>\n",
       "      <td>1.062363e+01</td>\n",
       "      <td>2.399400e+04</td>\n",
       "      <td>7.892522e+01</td>\n",
       "      <td>4.967878e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.164751e+00</td>\n",
       "      <td>2.207594e+03</td>\n",
       "      <td>8.668865e+03</td>\n",
       "      <td>6.115321e+02</td>\n",
       "      <td>1.463065e+03</td>\n",
       "      <td>1.880632e+03</td>\n",
       "      <td>1.269264e+03</td>\n",
       "      <td>1.667285e+01</td>\n",
       "      <td>1.229306e+01</td>\n",
       "      <td>1.692675e+01</td>\n",
       "      <td>2.142393e+01</td>\n",
       "      <td>3.522265e+02</td>\n",
       "      <td>3.578323e+02</td>\n",
       "      <td>1.289760e+01</td>\n",
       "      <td>3.102800e+04</td>\n",
       "      <td>8.648376e+01</td>\n",
       "      <td>5.111071e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Wf            Nf            Nc           T24           T30  \\\n",
       "count  1.253743e+06  1.253743e+06  1.253743e+06  1.253743e+06  1.253743e+06   \n",
       "mean   2.403201e+00  1.999809e+03  8.230953e+03  5.649183e+02  1.327844e+03   \n",
       "std    6.037376e-01  1.468721e+02  1.931254e+02  1.812375e+01  5.811396e+01   \n",
       "min    3.416258e-01  1.471017e+03  7.396578e+03  5.045053e+02  1.070961e+03   \n",
       "25%    1.972566e+00  1.918627e+03  8.125872e+03  5.518893e+02  1.295088e+03   \n",
       "50%    2.278735e+00  2.033749e+03  8.234230e+03  5.626667e+02  1.327539e+03   \n",
       "75%    2.780043e+00  2.115296e+03  8.356755e+03  5.775744e+02  1.365424e+03   \n",
       "max    4.164751e+00  2.207594e+03  8.668865e+03  6.115321e+02  1.463065e+03   \n",
       "\n",
       "                T48           T50           P15            P2           P21  \\\n",
       "count  1.253743e+06  1.253743e+06  1.253743e+06  1.253743e+06  1.253743e+06   \n",
       "mean   1.648611e+03  1.118895e+03  1.174699e+01  9.020636e+00  1.192588e+01   \n",
       "std    1.041488e+02  5.303728e+01  2.176814e+00  1.787953e+00  2.209963e+00   \n",
       "min    9.540545e+02  6.976268e+02  7.152428e+00  5.507010e+00  7.261349e+00   \n",
       "25%    1.596720e+03  1.083440e+03  9.977838e+00  7.567901e+00  1.012979e+01   \n",
       "50%    1.657730e+03  1.112218e+03  1.150901e+01  8.810251e+00  1.168427e+01   \n",
       "75%    1.714233e+03  1.154108e+03  1.354695e+01  1.067996e+01  1.375325e+01   \n",
       "max    1.880632e+03  1.269264e+03  1.667285e+01  1.229306e+01  1.692675e+01   \n",
       "\n",
       "                P24          Ps30           P40           P50           alt  \\\n",
       "count  1.253743e+06  1.253743e+06  1.253743e+06  1.253743e+06  1.253743e+06   \n",
       "mean   1.467278e+01  2.223658e+02  2.262123e+02  8.851139e+00  1.910834e+04   \n",
       "std    2.666418e+00  4.596197e+01  4.661605e+01  1.937761e+00  6.175012e+03   \n",
       "min    8.598599e+00  8.225872e+01  8.577351e+01  4.949927e+00  1.000100e+04   \n",
       "25%    1.255894e+01  1.878106e+02  1.911433e+02  7.234730e+00  1.313600e+04   \n",
       "50%    1.432719e+01  2.122920e+02  2.159778e+02  8.589801e+00  1.941300e+04   \n",
       "75%    1.672680e+01  2.532535e+02  2.575437e+02  1.062363e+01  2.399400e+04   \n",
       "max    2.142393e+01  3.522265e+02  3.578323e+02  1.289760e+01  3.102800e+04   \n",
       "\n",
       "                TRA            T2  \n",
       "count  1.253743e+06  1.253743e+06  \n",
       "mean   6.555718e+01  4.822338e+02  \n",
       "std    1.507279e+01  1.618800e+01  \n",
       "min    2.355452e+01  4.416871e+02  \n",
       "25%    5.484336e+01  4.696360e+02  \n",
       "50%    6.996044e+01  4.817885e+02  \n",
       "75%    7.892522e+01  4.967878e+02  \n",
       "max    8.648376e+01  5.111071e+02  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-keras-gpu",
   "language": "python",
   "name": "tf-keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
