{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "seed = 0\n",
    "os.environ['PYTHONHASSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/turbofan_dataset/N-CMAPSS_DS02-006.h5'\n",
    "output_path = 'DS02/experiment_set_8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename, load_test_set=True):\n",
    "    \"\"\" Reads a dataset from a given .h5 file and compose (in memory) the train and test data. \n",
    "    Args:\n",
    "        filename(str): path to the .h5 file\n",
    "    Returns:\n",
    "        train_set(pd.DataFrame), test_set(pd.DataFrame)\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'r') as hdf:\n",
    "        # Development set\n",
    "        W_dev = np.array(hdf.get('W_dev'))             # W\n",
    "        X_s_dev = np.array(hdf.get('X_s_dev'))         # X_s\n",
    "        X_v_dev = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "        T_dev = np.array(hdf.get('T_dev'))             # T\n",
    "        Y_dev = np.array(hdf.get('Y_dev'))             # RUL  \n",
    "        A_dev = np.array(hdf.get('A_dev'))             # Auxiliary\n",
    "\n",
    "        # Test set\n",
    "        if load_test_set:\n",
    "            W_test = np.array(hdf.get('W_test'))           # W\n",
    "            X_s_test = np.array(hdf.get('X_s_test'))       # X_s\n",
    "            X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "            T_test = np.array(hdf.get('T_test'))           # T\n",
    "            Y_test = np.array(hdf.get('Y_test'))           # RUL  \n",
    "            A_test = np.array(hdf.get('A_test'))           # Auxiliary\n",
    "        \n",
    "        # Column names\n",
    "        W_var = np.array(hdf.get('W_var'))\n",
    "        X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "        X_v_var = np.array(hdf.get('X_v_var')) \n",
    "        T_var = np.array(hdf.get('T_var'))\n",
    "        A_var = np.array(hdf.get('A_var'))\n",
    "        \n",
    "        columns = []\n",
    "        columns.append(list(np.array(A_var, dtype='U20')))\n",
    "        columns.append(list(np.array(T_var, dtype='U20')))\n",
    "        columns.append(list(np.array(X_s_var, dtype='U20')))\n",
    "        columns.append(list(np.array(X_v_var, dtype='U20')))\n",
    "        columns.append(list(np.array(W_var, dtype='U20')))\n",
    "        columns.append(['RUL'])\n",
    "        \n",
    "        columns_list = []\n",
    "        for columns_per_category in columns:\n",
    "            columns_list += columns_per_category\n",
    "        \n",
    "    train_set = np.concatenate((A_dev, T_dev, X_s_dev, X_v_dev, W_dev, Y_dev), axis=1)\n",
    "    if load_test_set:\n",
    "        test_set = np.concatenate((A_test, T_test, X_s_test, X_v_test, W_test, Y_test), axis=1)\n",
    "        return pd.DataFrame(data=train_set, columns=columns_list), pd.DataFrame(data=test_set, columns=columns_list), columns\n",
    "    else:\n",
    "        return pd.DataFrame(data=train_set, columns=columns_list), None, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_cycle_info(df, compute_cycle_len=False):\n",
    "    unit_ids = np.unique(df['unit'])\n",
    "    print('Engine units in df: ', unit_ids)\n",
    "    for i in unit_ids:\n",
    "        num_cycles = len(np.unique(df.loc[df['unit'] == i, 'cycle']))\n",
    "        print('Unit: ', i, ' - Number of flight cycles: ', num_cycles)\n",
    "        \n",
    "    if compute_cycle_len:\n",
    "        cycle_ids = np.unique(df['cycle'])\n",
    "        print('Total number of cycles: ', len(cycle_ids))\n",
    "        min_len = np.inf\n",
    "        max_len = 0\n",
    "        for i in cycle_ids:\n",
    "            cycle_len = len(df.loc[df['cycle'] == i, 'cycle'])\n",
    "            if cycle_len < min_len:\n",
    "                min_len = cycle_len\n",
    "            elif cycle_len > max_len:\n",
    "                max_len = cycle_len\n",
    "        print('Min cycle length: ', min_len)\n",
    "        print('Max cycle length: ', max_len)\n",
    "    \n",
    "    return unit_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter constant and quasi-constant features\n",
    "def get_quasi_constant_features(dataset, variance_th=0.01, debug=True):\n",
    "    constant_filter = VarianceThreshold(threshold=variance_th)\n",
    "    constant_filter.fit(dataset)\n",
    "    constant_features = [col for col in dataset.columns \n",
    "                         if col not in dataset.columns[constant_filter.get_support()]]\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Number of non-constant features: \", len(dataset.columns[constant_filter.get_support()]))\n",
    "        \n",
    "        print(\"Number of quasi-constant features: \", len(constant_features))\n",
    "        print(\"Quasi-constant features: \")\n",
    "        for col in constant_features:\n",
    "            print(col)\n",
    "    return constant_features\n",
    "\n",
    "def get_non_correlated_features(dataset, corr_th=0.9, debug=True):\n",
    "    corr_mat = dataset.corr()\n",
    "    corr_mat = np.abs(corr_mat)\n",
    "    \n",
    "    num_cols = corr_mat.shape[0]\n",
    "    columns = np.full((num_cols,), True, dtype=bool)\n",
    "    for i in range(num_cols):\n",
    "        for j in range(i+1, num_cols):\n",
    "            val = corr_mat.iloc[i, j]\n",
    "            if val >= corr_th:\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "                    if debug:\n",
    "                        print(dataset.columns[i], \"|\", dataset.columns[j], \"|\", round(val, 2))\n",
    "    if debug:        \n",
    "        correlated_features = dataset.columns[~columns]\n",
    "        print(\"Number of correlated features: \", len(correlated_features))\n",
    "        print(\"Correlated features: \", list(correlated_features))\n",
    "    \n",
    "    selected_columns = dataset.columns[columns]\n",
    "    if debug:\n",
    "        print(\"Number of selected features: \", len(selected_columns))\n",
    "        print(\"Selected features: \", list(selected_columns))\n",
    "    return selected_columns\n",
    "\n",
    "def cmapss_score_function(actual, predictions, normalize=True):\n",
    "    # diff < 0 -> over-estimation\n",
    "    # diff > 0 -> under-estimation\n",
    "    diff = actual - predictions\n",
    "    alpha = np.full_like(diff, 1/13)\n",
    "    negative_diff_mask = diff < 0\n",
    "    alpha[negative_diff_mask] = 1/10\n",
    "    score = np.sum(np.exp(alpha * np.abs(diff)))\n",
    "    \n",
    "    if normalize:\n",
    "        N = len(predictions)\n",
    "        score /= N\n",
    "    return score\n",
    "\n",
    "def compute_evaluation_metrics(actual, predictions, label='Test'):\n",
    "    mse = mean_squared_error(actual, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    cmapss_score = cmapss_score_function(actual, predictions)\n",
    "    print('{} set:\\nMSE: {:.2f}\\nRMSE: {:.2f}\\nCMAPSS score: {:.2f}\\n'.format(label, mse, rmse, \n",
    "                                                                     cmapss_score))\n",
    "    return mse, rmse, cmapss_score\n",
    "    \n",
    "def plot_loss_curves(history, output_path=None, y_lim=[0, 150]):\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim(y_lim)\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    \n",
    "    if output_path is not None:\n",
    "        plt.savefig(os.path.join(output_path, 'loss_curves.png'), format='png', dpi=300) \n",
    "    plt.show()\n",
    "    \n",
    "def plot_rul(expected, predicted):\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(expected)), expected, label='Expected')\n",
    "    plt.plot(range(len(predicted)), predicted, label='Predicted')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def create_mlp_model(input_dim, hidden_layer_sizes, activation='relu', output_weights_file=None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_sizes[0], \n",
    "                    input_dim=input_dim, \n",
    "                    kernel_initializer='random_normal', \n",
    "                    activation=activation))\n",
    "\n",
    "    for layer_size in hidden_layer_sizes[1:]:\n",
    "        model.add(Dense(layer_size, \n",
    "                        kernel_initializer='random_normal', \n",
    "                        activation=activation))\n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer='random_normal'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    if output_weights_file is not None:\n",
    "        model.save_weights(output_weights_file)\n",
    "    return model\n",
    "\n",
    "def train_model_existing_weights(model, weights_file, x_train, y_train, x_val, y_val, epochs=200, batch_size=512, callbacks=[]):\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.load_weights(weights_file)\n",
    "    return model.fit(x_train, y_train,\n",
    "                     validation_data=(x_val, y_val),\n",
    "                     epochs=epochs,\n",
    "                     batch_size=batch_size,\n",
    "                     verbose=1,\n",
    "                     callbacks=callbacks)\n",
    "\n",
    "def save_history(history, output_file=os.path.join(output_path, \"history.pkl\")):\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(history.history, file)\n",
    "    print(\"Saved training history to file: {}\".format(output_file))\n",
    "\n",
    "def load_history(file):\n",
    "    return pickle.load(open(file, \"rb\"))\n",
    "\n",
    "def model_evaluation(model, x_test, y_test, x_train=None, y_train=None, plot_range=[0, 10**3]):\n",
    "    if x_train is not None and y_train is not None:\n",
    "        predictions_train = model.predict(x_train).flatten()\n",
    "        compute_evaluation_metrics(predictions_train, y_train, 'Train')\n",
    "        \n",
    "        expected = y_train[plot_range[0]:plot_range[1]]\n",
    "        predicted = predictions_train[plot_range[0]:plot_range[1]]\n",
    "        plot_rul(expected, predicted)\n",
    "        \n",
    "    predictions_test = model.predict(x_test).flatten()\n",
    "    compute_evaluation_metrics(predictions_test, y_test)\n",
    "    \n",
    "    expected = y_test[plot_range[0]:plot_range[1]]\n",
    "    predicted = predictions_test[plot_range[0]:plot_range[1]]\n",
    "    plot_rul(expected, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list(string_list, output_file):\n",
    "    output_file.write(\"[\")\n",
    "    n = len(string_list)\n",
    "    for i in range(n - 1):\n",
    "        output_file.write(\"{}, \".format(string_list[i]))\n",
    "    output_file.write(\"{}]\\n\".format(string_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation time (sec):  3.796875\n",
      "\n",
      "Train set shape: (5263447, 47)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.process_time()  \n",
    "train_set, test_set, columns = load_dataset(filename)\n",
    "print(\"Operation time (sec): \" , (time.process_time() - start_time))\n",
    "print()\n",
    "print(\"Train set shape: \" + str(train_set.shape))\n",
    "\n",
    "columns_aux = columns[0] \n",
    "columns_health_params = columns[1] \n",
    "columns_sensor_measurements = columns[2] \n",
    "columns_virtual_sensors = columns[3]\n",
    "columns_operating_conditions = columns[4] \n",
    "target_col = columns[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set['RUL']\n",
    "x_train = train_set.drop(['RUL'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_columns = x_train.columns\n",
    "selected_columns_1 = columns_sensor_measurements + columns_virtual_sensors + columns_operating_conditions\n",
    "\n",
    "x_train.drop(labels=[x for x in initial_columns if x not in selected_columns_1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 238,721\n",
      "Trainable params: 238,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "   1/5038 [..............................] - ETA: 0s - loss: 2032.6047WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0156s). Check your callbacks.\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 169.1904\n",
      "Epoch 00001: val_loss improved from inf to 51.66619, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 169.1113 - val_loss: 51.6662\n",
      "Epoch 2/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 44.9549\n",
      "Epoch 00002: val_loss improved from 51.66619 to 37.89115, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 44.9387 - val_loss: 37.8911\n",
      "Epoch 3/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 38.0929 ETA: 0s\n",
      "Epoch 00003: val_loss did not improve from 37.89115\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.0866 - val_loss: 38.1577\n",
      "Epoch 4/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 33.6959\n",
      "Epoch 00004: val_loss improved from 37.89115 to 34.17670, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.6999 - val_loss: 34.1767\n",
      "Epoch 5/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 30.0832\n",
      "Epoch 00005: val_loss improved from 34.17670 to 26.88734, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 30.0826 - val_loss: 26.8873\n",
      "Epoch 6/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 27.1600\n",
      "Epoch 00006: val_loss did not improve from 26.88734\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 27.1650 - val_loss: 29.2425\n",
      "Epoch 7/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 25.1924\n",
      "Epoch 00007: val_loss did not improve from 26.88734\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 25.1994 - val_loss: 36.8196\n",
      "Epoch 8/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 23.8439\n",
      "Epoch 00008: val_loss improved from 26.88734 to 20.88439, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 23.8363 - val_loss: 20.8844\n",
      "Epoch 9/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 22.9892\n",
      "Epoch 00009: val_loss did not improve from 20.88439\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 22.9904 - val_loss: 22.7767\n",
      "Epoch 10/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 22.0488\n",
      "Epoch 00010: val_loss improved from 20.88439 to 18.91909, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 22.0445 - val_loss: 18.9191\n",
      "Epoch 11/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 21.4301\n",
      "Epoch 00011: val_loss did not improve from 18.91909\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 21.4260 - val_loss: 24.2067\n",
      "Epoch 12/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 20.6664\n",
      "Epoch 00012: val_loss did not improve from 18.91909\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 20.6652 - val_loss: 21.0137\n",
      "Epoch 13/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 20.0009\n",
      "Epoch 00013: val_loss improved from 18.91909 to 15.91813, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 19.9935 - val_loss: 15.9181\n",
      "Epoch 14/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 19.5214\n",
      "Epoch 00014: val_loss did not improve from 15.91813\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.5203 - val_loss: 23.3277\n",
      "Epoch 15/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 19.1297\n",
      "Epoch 00015: val_loss did not improve from 15.91813\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.1281 - val_loss: 26.6994\n",
      "Epoch 16/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 18.8885\n",
      "Epoch 00016: val_loss did not improve from 15.91813\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.8843 - val_loss: 16.3269\n",
      "Epoch 17/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 18.5653\n",
      "Epoch 00017: val_loss did not improve from 15.91813\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.5648 - val_loss: 16.5976\n",
      "Epoch 18/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 18.2181\n",
      "Epoch 00018: val_loss did not improve from 15.91813\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.2151 - val_loss: 16.3813\n",
      "Epoch 19/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 17.8224\n",
      "Epoch 00019: val_loss did not improve from 15.91813\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.8207 - val_loss: 17.1415\n",
      "Epoch 20/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 17.5893\n",
      "Epoch 00020: val_loss did not improve from 15.91813\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.5919 - val_loss: 16.6123\n",
      "Epoch 21/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 17.1875\n",
      "Epoch 00021: val_loss improved from 15.91813 to 15.24956, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.1821 - val_loss: 15.2496\n",
      "Epoch 22/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 16.9025\n",
      "Epoch 00022: val_loss did not improve from 15.24956\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.9032 - val_loss: 15.9827\n",
      "Epoch 23/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 16.7417\n",
      "Epoch 00023: val_loss improved from 15.24956 to 14.94361, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.7413 - val_loss: 14.9436\n",
      "Epoch 24/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 16.4406\n",
      "Epoch 00024: val_loss improved from 14.94361 to 14.39374, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.4418 - val_loss: 14.3937\n",
      "Epoch 25/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 16.2583\n",
      "Epoch 00025: val_loss improved from 14.39374 to 14.35592, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.2566 - val_loss: 14.3559\n",
      "Epoch 26/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 16.1994\n",
      "Epoch 00026: val_loss did not improve from 14.35592\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.2007 - val_loss: 17.7760\n",
      "Epoch 27/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 15.9145\n",
      "Epoch 00027: val_loss improved from 14.35592 to 13.94348, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.9137 - val_loss: 13.9435\n",
      "Epoch 28/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 15.6609\n",
      "Epoch 00028: val_loss did not improve from 13.94348\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.6574 - val_loss: 14.9301\n",
      "Epoch 29/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 15.5720\n",
      "Epoch 00029: val_loss improved from 13.94348 to 13.86644, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.5717 - val_loss: 13.8664\n",
      "Epoch 30/200\n",
      "5022/5038 [============================>.] - ETA: 0s - loss: 15.2567\n",
      "Epoch 00030: val_loss improved from 13.86644 to 12.60709, saving model to DS02/experiment_set_8\\results_all\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.2527 - val_loss: 12.6071\n",
      "Epoch 31/200\n",
      "5022/5038 [============================>.] - ETA: 0s - loss: 15.1234\n",
      "Epoch 00031: val_loss did not improve from 12.60709\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.1239 - val_loss: 13.5342\n",
      "Epoch 32/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 14.9625\n",
      "Epoch 00032: val_loss did not improve from 12.60709\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.9615 - val_loss: 15.1320\n",
      "Epoch 33/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 14.7508\n",
      "Epoch 00033: val_loss did not improve from 12.60709\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.7522 - val_loss: 14.2990\n",
      "Epoch 34/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 14.5152\n",
      "Epoch 00034: val_loss did not improve from 12.60709\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.5158 - val_loss: 31.7973\n",
      "Epoch 35/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 14.4179\n",
      "Epoch 00035: val_loss did not improve from 12.60709\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.4134 - val_loss: 17.5167\n",
      "Epoch 36/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 14.3633\n",
      "Epoch 00036: val_loss did not improve from 12.60709\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.3633 - val_loss: 18.4533\n",
      "Epoch 37/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 14.1989\n",
      "Epoch 00037: val_loss did not improve from 12.60709\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.1995 - val_loss: 17.3765\n",
      "Epoch 38/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 14.1511\n",
      "Epoch 00038: val_loss did not improve from 12.60709\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.1509 - val_loss: 13.7950\n",
      "Epoch 39/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 13.9813\n",
      "Epoch 00039: val_loss did not improve from 12.60709\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.9807 - val_loss: 15.2572\n",
      "Epoch 40/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 13.7105\n",
      "Epoch 00040: val_loss did not improve from 12.60709\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.7110 - val_loss: 16.4105\n",
      "Epoch 00040: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_all\\history.pkl\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "MSE: 12.59\n",
      "RMSE: 3.55\n",
      "CMAPSS score: 1.25\n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 256)               4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 234,369\n",
      "Trainable params: 234,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 166.9143\n",
      "Epoch 00001: val_loss improved from inf to 52.31768, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 166.8621 - val_loss: 52.3177\n",
      "Epoch 2/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 44.0577\n",
      "Epoch 00002: val_loss improved from 52.31768 to 40.12787, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 44.0552 - val_loss: 40.1279\n",
      "Epoch 3/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 37.9972\n",
      "Epoch 00003: val_loss improved from 40.12787 to 33.81430, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.9975 - val_loss: 33.8143\n",
      "Epoch 4/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 34.3943\n",
      "Epoch 00004: val_loss improved from 33.81430 to 32.17805, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 34.3915 - val_loss: 32.1781\n",
      "Epoch 5/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 31.9270\n",
      "Epoch 00005: val_loss did not improve from 32.17805\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 31.9269 - val_loss: 37.1028\n",
      "Epoch 6/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 30.1545\n",
      "Epoch 00006: val_loss improved from 32.17805 to 29.91190, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 30.1534 - val_loss: 29.9119\n",
      "Epoch 7/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 28.4488\n",
      "Epoch 00007: val_loss did not improve from 29.91190\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 28.4460 - val_loss: 41.6353\n",
      "Epoch 8/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 27.2904\n",
      "Epoch 00008: val_loss improved from 29.91190 to 28.83906, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 27.2881 - val_loss: 28.8391\n",
      "Epoch 9/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 26.0383\n",
      "Epoch 00009: val_loss improved from 28.83906 to 27.35488, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 26.0382 - val_loss: 27.3549\n",
      "Epoch 10/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 24.9665\n",
      "Epoch 00010: val_loss improved from 27.35488 to 24.42012, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 24.9675 - val_loss: 24.4201\n",
      "Epoch 11/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 23.8545\n",
      "Epoch 00011: val_loss did not improve from 24.42012\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 23.8513 - val_loss: 26.0574\n",
      "Epoch 12/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 23.0687\n",
      "Epoch 00012: val_loss improved from 24.42012 to 23.73030, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 23.0650 - val_loss: 23.7303\n",
      "Epoch 13/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 22.2520\n",
      "Epoch 00013: val_loss improved from 23.73030 to 21.84699, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 22.2520 - val_loss: 21.8470\n",
      "Epoch 14/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 21.6055\n",
      "Epoch 00014: val_loss did not improve from 21.84699\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.6055 - val_loss: 24.9031\n",
      "Epoch 15/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 21.2390\n",
      "Epoch 00015: val_loss improved from 21.84699 to 19.99387, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.2332 - val_loss: 19.9939\n",
      "Epoch 16/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 20.5942\n",
      "Epoch 00016: val_loss did not improve from 19.99387\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.5914 - val_loss: 20.8577\n",
      "Epoch 17/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 20.1697\n",
      "Epoch 00017: val_loss did not improve from 19.99387\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.1667 - val_loss: 22.0128\n",
      "Epoch 18/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 19.9568\n",
      "Epoch 00018: val_loss did not improve from 19.99387\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 19.9733 - val_loss: 21.0830\n",
      "Epoch 19/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 19.5084\n",
      "Epoch 00019: val_loss did not improve from 19.99387\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.5090 - val_loss: 22.6084\n",
      "Epoch 20/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 19.0034\n",
      "Epoch 00020: val_loss did not improve from 19.99387\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.0041 - val_loss: 26.3925\n",
      "Epoch 21/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 18.6563\n",
      "Epoch 00021: val_loss did not improve from 19.99387\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.6623 - val_loss: 21.9485\n",
      "Epoch 22/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 18.4010\n",
      "Epoch 00022: val_loss improved from 19.99387 to 16.64144, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 18.3990 - val_loss: 16.6414\n",
      "Epoch 23/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 18.0862\n",
      "Epoch 00023: val_loss did not improve from 16.64144\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.0868 - val_loss: 22.4596\n",
      "Epoch 24/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 17.8400\n",
      "Epoch 00024: val_loss did not improve from 16.64144\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.8415 - val_loss: 16.8013\n",
      "Epoch 25/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 17.8695\n",
      "Epoch 00025: val_loss did not improve from 16.64144\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.8863 - val_loss: 25.1722\n",
      "Epoch 26/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 17.6173 ETA\n",
      "Epoch 00026: val_loss did not improve from 16.64144\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.6179 - val_loss: 18.9587\n",
      "Epoch 27/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 17.3317 ETA: 0s - loss: 17.\n",
      "Epoch 00027: val_loss did not improve from 16.64144\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.3310 - val_loss: 16.6509\n",
      "Epoch 28/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 17.2010\n",
      "Epoch 00028: val_loss did not improve from 16.64144\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.1999 - val_loss: 18.5804\n",
      "Epoch 29/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 16.9163\n",
      "Epoch 00029: val_loss did not improve from 16.64144\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.9228 - val_loss: 31.7655\n",
      "Epoch 30/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 16.8015\n",
      "Epoch 00030: val_loss did not improve from 16.64144\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.8011 - val_loss: 16.6668\n",
      "Epoch 31/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 16.3678\n",
      "Epoch 00031: val_loss improved from 16.64144 to 15.49677, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.3663 - val_loss: 15.4968\n",
      "Epoch 32/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 16.2891\n",
      "Epoch 00032: val_loss did not improve from 15.49677\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.2893 - val_loss: 16.0731\n",
      "Epoch 33/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 16.2004\n",
      "Epoch 00033: val_loss improved from 15.49677 to 15.45548, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.2018 - val_loss: 15.4555\n",
      "Epoch 34/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 15.9942\n",
      "Epoch 00034: val_loss improved from 15.45548 to 15.18086, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.9933 - val_loss: 15.1809\n",
      "Epoch 35/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 15.9332\n",
      "Epoch 00035: val_loss did not improve from 15.18086\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.9344 - val_loss: 16.2179\n",
      "Epoch 36/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 15.7820\n",
      "Epoch 00036: val_loss did not improve from 15.18086\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 15.7823 - val_loss: 18.6705\n",
      "Epoch 37/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 15.7024\n",
      "Epoch 00037: val_loss did not improve from 15.18086\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.7024 - val_loss: 16.5089\n",
      "Epoch 38/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 15.4876\n",
      "Epoch 00038: val_loss did not improve from 15.18086\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.4877 - val_loss: 16.0882\n",
      "Epoch 39/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 15.4823\n",
      "Epoch 00039: val_loss did not improve from 15.18086\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.4823 - val_loss: 15.6250\n",
      "Epoch 40/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 15.2369\n",
      "Epoch 00040: val_loss improved from 15.18086 to 14.79221, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.2360 - val_loss: 14.7922\n",
      "Epoch 41/200\n",
      "5022/5038 [============================>.] - ETA: 0s - loss: 15.2174\n",
      "Epoch 00041: val_loss did not improve from 14.79221\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.2186 - val_loss: 14.8829\n",
      "Epoch 42/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 15.0440\n",
      "Epoch 00042: val_loss improved from 14.79221 to 14.68621, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.0440 - val_loss: 14.6862\n",
      "Epoch 43/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 14.9132\n",
      "Epoch 00043: val_loss did not improve from 14.68621\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.9129 - val_loss: 15.3286\n",
      "Epoch 44/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 14.8202\n",
      "Epoch 00044: val_loss did not improve from 14.68621\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.8190 - val_loss: 18.1148\n",
      "Epoch 45/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 14.8219\n",
      "Epoch 00045: val_loss improved from 14.68621 to 14.31156, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.8201 - val_loss: 14.3116\n",
      "Epoch 46/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 14.4456\n",
      "Epoch 00046: val_loss did not improve from 14.31156\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.4448 - val_loss: 16.0984\n",
      "Epoch 47/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 14.7174\n",
      "Epoch 00047: val_loss did not improve from 14.31156\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.7168 - val_loss: 14.7077\n",
      "Epoch 48/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 14.6021\n",
      "Epoch 00048: val_loss did not improve from 14.31156\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.6012 - val_loss: 15.9808\n",
      "Epoch 49/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 14.3982\n",
      "Epoch 00049: val_loss did not improve from 14.31156\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.3982 - val_loss: 16.8684\n",
      "Epoch 50/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 14.4028\n",
      "Epoch 00050: val_loss improved from 14.31156 to 14.07230, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.4025 - val_loss: 14.0723\n",
      "Epoch 51/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 14.1499\n",
      "Epoch 00051: val_loss improved from 14.07230 to 13.85232, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.1486 - val_loss: 13.8523\n",
      "Epoch 52/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 14.1769\n",
      "Epoch 00052: val_loss did not improve from 13.85232\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 14.1785 - val_loss: 16.8457\n",
      "Epoch 53/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 14.1616\n",
      "Epoch 00053: val_loss improved from 13.85232 to 13.40410, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.1616 - val_loss: 13.4041\n",
      "Epoch 54/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 14.0279\n",
      "Epoch 00054: val_loss did not improve from 13.40410\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 14.0303 - val_loss: 19.0400\n",
      "Epoch 55/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 13.9457\n",
      "Epoch 00055: val_loss did not improve from 13.40410\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.9476 - val_loss: 18.6850\n",
      "Epoch 56/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 13.9901\n",
      "Epoch 00056: val_loss did not improve from 13.40410\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.9889 - val_loss: 14.9003\n",
      "Epoch 57/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 13.8919\n",
      "Epoch 00057: val_loss improved from 13.40410 to 13.12229, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.8913 - val_loss: 13.1223\n",
      "Epoch 58/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 13.7100\n",
      "Epoch 00058: val_loss improved from 13.12229 to 12.90136, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.7091 - val_loss: 12.9014\n",
      "Epoch 59/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 13.8020\n",
      "Epoch 00059: val_loss did not improve from 12.90136\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.7989 - val_loss: 12.9602\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5034/5038 [============================>.] - ETA: 0s - loss: 13.5776\n",
      "Epoch 00060: val_loss did not improve from 12.90136\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.5762 - val_loss: 13.2715\n",
      "Epoch 61/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 13.5143\n",
      "Epoch 00061: val_loss did not improve from 12.90136\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.5124 - val_loss: 13.8848\n",
      "Epoch 62/200\n",
      "5022/5038 [============================>.] - ETA: 0s - loss: 13.5606\n",
      "Epoch 00062: val_loss did not improve from 12.90136\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.5561 - val_loss: 13.6332\n",
      "Epoch 63/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 13.4193\n",
      "Epoch 00063: val_loss did not improve from 12.90136\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.4194 - val_loss: 16.5311\n",
      "Epoch 64/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 13.3413\n",
      "Epoch 00064: val_loss did not improve from 12.90136\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.3409 - val_loss: 13.7581\n",
      "Epoch 65/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 13.2664\n",
      "Epoch 00065: val_loss did not improve from 12.90136\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.2648 - val_loss: 16.8963\n",
      "Epoch 66/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 13.3867\n",
      "Epoch 00066: val_loss improved from 12.90136 to 12.80680, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.3854 - val_loss: 12.8068\n",
      "Epoch 67/200\n",
      "5021/5038 [============================>.] - ETA: 0s - loss: 13.1983\n",
      "Epoch 00067: val_loss did not improve from 12.80680\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.1941 - val_loss: 13.0967\n",
      "Epoch 68/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 13.2032\n",
      "Epoch 00068: val_loss did not improve from 12.80680\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.2084 - val_loss: 19.3544\n",
      "Epoch 69/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 13.2989\n",
      "Epoch 00069: val_loss did not improve from 12.80680\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.3108 - val_loss: 16.3587\n",
      "Epoch 70/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 13.0988\n",
      "Epoch 00070: val_loss did not improve from 12.80680\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.0972 - val_loss: 12.8432\n",
      "Epoch 71/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 13.0989\n",
      "Epoch 00071: val_loss improved from 12.80680 to 12.72631, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.0973 - val_loss: 12.7263\n",
      "Epoch 72/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 12.9452\n",
      "Epoch 00072: val_loss did not improve from 12.72631\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.9453 - val_loss: 13.1794\n",
      "Epoch 73/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 12.9380\n",
      "Epoch 00073: val_loss did not improve from 12.72631\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.9378 - val_loss: 22.4293\n",
      "Epoch 74/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 12.9233\n",
      "Epoch 00074: val_loss did not improve from 12.72631\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.9234 - val_loss: 13.1797\n",
      "Epoch 75/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 12.8911\n",
      "Epoch 00075: val_loss did not improve from 12.72631\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.8875 - val_loss: 13.0492\n",
      "Epoch 76/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 12.8209\n",
      "Epoch 00076: val_loss did not improve from 12.72631\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 12.8325 - val_loss: 14.7351\n",
      "Epoch 77/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 12.8513\n",
      "Epoch 00077: val_loss did not improve from 12.72631\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.8511 - val_loss: 14.9287\n",
      "Epoch 78/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 12.7434\n",
      "Epoch 00078: val_loss did not improve from 12.72631\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.7420 - val_loss: 12.7992\n",
      "Epoch 79/200\n",
      "5021/5038 [============================>.] - ETA: 0s - loss: 12.7180\n",
      "Epoch 00079: val_loss did not improve from 12.72631\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 12.7226 - val_loss: 19.6736\n",
      "Epoch 80/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 12.7283\n",
      "Epoch 00080: val_loss did not improve from 12.72631\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.7304 - val_loss: 16.3873\n",
      "Epoch 81/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 12.6797\n",
      "Epoch 00081: val_loss improved from 12.72631 to 12.53781, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.6796 - val_loss: 12.5378\n",
      "Epoch 82/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 12.4852\n",
      "Epoch 00082: val_loss did not improve from 12.53781\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.4860 - val_loss: 14.7848\n",
      "Epoch 83/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 12.6027\n",
      "Epoch 00083: val_loss did not improve from 12.53781\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.6024 - val_loss: 13.5707\n",
      "Epoch 84/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 12.4861\n",
      "Epoch 00084: val_loss did not improve from 12.53781\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.4873 - val_loss: 15.9113\n",
      "Epoch 85/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 12.5415\n",
      "Epoch 00085: val_loss improved from 12.53781 to 11.96178, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 12.5435 - val_loss: 11.9618\n",
      "Epoch 86/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 12.3938\n",
      "Epoch 00086: val_loss did not improve from 11.96178\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.3958 - val_loss: 14.4228\n",
      "Epoch 87/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 12.4998\n",
      "Epoch 00087: val_loss did not improve from 11.96178\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.5045 - val_loss: 14.8944\n",
      "Epoch 88/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 12.4042\n",
      "Epoch 00088: val_loss improved from 11.96178 to 11.90509, saving model to DS02/experiment_set_8\\results_0.99\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.4043 - val_loss: 11.9051\n",
      "Epoch 89/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 12.3407\n",
      "Epoch 00089: val_loss did not improve from 11.90509\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.3421 - val_loss: 14.4587\n",
      "Epoch 90/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 12.2706\n",
      "Epoch 00090: val_loss did not improve from 11.90509\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.2705 - val_loss: 13.7057\n",
      "Epoch 91/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 12.1550\n",
      "Epoch 00091: val_loss did not improve from 11.90509\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.1550 - val_loss: 13.1387\n",
      "Epoch 92/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 12.2239\n",
      "Epoch 00092: val_loss did not improve from 11.90509\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.2260 - val_loss: 12.9209\n",
      "Epoch 93/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 12.1712\n",
      "Epoch 00093: val_loss did not improve from 11.90509\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.1713 - val_loss: 12.7663\n",
      "Epoch 94/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 12.0930 - ETA: 0s - l\n",
      "Epoch 00094: val_loss did not improve from 11.90509\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.0930 - val_loss: 13.3644\n",
      "Epoch 95/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 12.1917\n",
      "Epoch 00095: val_loss did not improve from 11.90509\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.1916 - val_loss: 13.3693\n",
      "Epoch 96/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 12.2330\n",
      "Epoch 00096: val_loss did not improve from 11.90509\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.2327 - val_loss: 12.2015\n",
      "Epoch 97/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 12.0244\n",
      "Epoch 00097: val_loss did not improve from 11.90509\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 12.0243 - val_loss: 15.1494\n",
      "Epoch 98/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 11.9679\n",
      "Epoch 00098: val_loss did not improve from 11.90509\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 11.9675 - val_loss: 13.9507\n",
      "Epoch 00098: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_0.99\\history.pkl\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "MSE: 11.91\n",
      "RMSE: 3.45\n",
      "CMAPSS score: 1.24\n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               2048      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 232,321\n",
      "Trainable params: 232,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 224.0992\n",
      "Epoch 00001: val_loss improved from inf to 90.25690, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 224.0992 - val_loss: 90.2569\n",
      "Epoch 2/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 72.9608 ETA: 0s -\n",
      "Epoch 00002: val_loss improved from 90.25690 to 84.26497, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 72.9536 - val_loss: 84.2650\n",
      "Epoch 3/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 65.7491\n",
      "Epoch 00003: val_loss improved from 84.26497 to 63.23584, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 65.7491 - val_loss: 63.2358\n",
      "Epoch 4/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 60.8529\n",
      "Epoch 00004: val_loss did not improve from 63.23584\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 60.8529 - val_loss: 76.4182\n",
      "Epoch 5/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 57.7527\n",
      "Epoch 00005: val_loss improved from 63.23584 to 55.75940, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 57.7526 - val_loss: 55.7594\n",
      "Epoch 6/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 55.5150\n",
      "Epoch 00006: val_loss did not improve from 55.75940\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 55.5147 - val_loss: 60.1570\n",
      "Epoch 7/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 53.4935\n",
      "Epoch 00007: val_loss improved from 55.75940 to 51.10700, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 53.4917 - val_loss: 51.1070\n",
      "Epoch 8/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 51.8875 ETA: \n",
      "Epoch 00008: val_loss did not improve from 51.10700\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 51.8823 - val_loss: 51.7375\n",
      "Epoch 9/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 50.7197\n",
      "Epoch 00009: val_loss did not improve from 51.10700\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 50.7143 - val_loss: 55.3731\n",
      "Epoch 10/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 49.4014 ETA: 0s - los\n",
      "Epoch 00010: val_loss did not improve from 51.10700\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 49.3913 - val_loss: 68.0612\n",
      "Epoch 11/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 48.3803\n",
      "Epoch 00011: val_loss improved from 51.10700 to 50.41454, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 48.3799 - val_loss: 50.4145\n",
      "Epoch 12/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 47.4589 ETA: 0s - loss:\n",
      "Epoch 00012: val_loss improved from 50.41454 to 47.19965, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 47.4592 - val_loss: 47.1996\n",
      "Epoch 13/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 46.4947\n",
      "Epoch 00013: val_loss did not improve from 47.19965\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 46.4989 - val_loss: 48.9049\n",
      "Epoch 14/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 45.7596\n",
      "Epoch 00014: val_loss did not improve from 47.19965\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 45.7641 - val_loss: 50.4975\n",
      "Epoch 15/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 45.1648\n",
      "Epoch 00015: val_loss improved from 47.19965 to 43.88755, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 45.1646 - val_loss: 43.8876\n",
      "Epoch 16/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 44.1862\n",
      "Epoch 00016: val_loss improved from 43.88755 to 41.36885, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 44.1849 - val_loss: 41.3689\n",
      "Epoch 17/200\n",
      "5022/5038 [============================>.] - ETA: 0s - loss: 43.8560\n",
      "Epoch 00017: val_loss did not improve from 41.36885\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 43.8486 - val_loss: 49.8339\n",
      "Epoch 18/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 43.1292\n",
      "Epoch 00018: val_loss did not improve from 41.36885\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 43.1358 - val_loss: 48.5060\n",
      "Epoch 19/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 42.6177\n",
      "Epoch 00019: val_loss improved from 41.36885 to 39.94914, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 42.6174 - val_loss: 39.9491\n",
      "Epoch 20/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 42.3843\n",
      "Epoch 00020: val_loss did not improve from 39.94914\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 42.3835 - val_loss: 40.2170\n",
      "Epoch 21/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 41.7732\n",
      "Epoch 00021: val_loss improved from 39.94914 to 38.82855, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 41.7731 - val_loss: 38.8285\n",
      "Epoch 22/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 41.1284\n",
      "Epoch 00022: val_loss did not improve from 38.82855\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 41.1282 - val_loss: 45.4460\n",
      "Epoch 23/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 40.8622\n",
      "Epoch 00023: val_loss did not improve from 38.82855\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 40.8648 - val_loss: 42.8429\n",
      "Epoch 24/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 40.3645\n",
      "Epoch 00024: val_loss did not improve from 38.82855\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 40.3660 - val_loss: 42.2055\n",
      "Epoch 25/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 40.1263\n",
      "Epoch 00025: val_loss did not improve from 38.82855\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 40.1225 - val_loss: 41.7247\n",
      "Epoch 26/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 39.5037\n",
      "Epoch 00026: val_loss improved from 38.82855 to 36.91401, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.5007 - val_loss: 36.9140\n",
      "Epoch 27/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 39.1967\n",
      "Epoch 00027: val_loss did not improve from 36.91401\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 39.1945 - val_loss: 39.9815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 38.9703\n",
      "Epoch 00028: val_loss did not improve from 36.91401\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.9646 - val_loss: 38.2019\n",
      "Epoch 29/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 38.6301\n",
      "Epoch 00029: val_loss did not improve from 36.91401\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.6344 - val_loss: 60.4135\n",
      "Epoch 30/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 38.4840\n",
      "Epoch 00030: val_loss did not improve from 36.91401\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.4877 - val_loss: 41.2018\n",
      "Epoch 31/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 37.9989\n",
      "Epoch 00031: val_loss improved from 36.91401 to 36.66567, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.0043 - val_loss: 36.6657\n",
      "Epoch 32/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 37.8018\n",
      "Epoch 00032: val_loss did not improve from 36.66567\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 37.8032 - val_loss: 39.9722\n",
      "Epoch 33/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 37.4330\n",
      "Epoch 00033: val_loss did not improve from 36.66567\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 37.4330 - val_loss: 37.7377\n",
      "Epoch 34/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 37.1406\n",
      "Epoch 00034: val_loss did not improve from 36.66567\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 37.1405 - val_loss: 40.1541\n",
      "Epoch 35/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 36.9949\n",
      "Epoch 00035: val_loss did not improve from 36.66567\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 36.9956 - val_loss: 38.2601\n",
      "Epoch 36/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 36.6818\n",
      "Epoch 00036: val_loss did not improve from 36.66567\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 36.6810 - val_loss: 37.4088\n",
      "Epoch 37/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 36.3202\n",
      "Epoch 00037: val_loss improved from 36.66567 to 35.53048, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.3196 - val_loss: 35.5305\n",
      "Epoch 38/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 36.3428\n",
      "Epoch 00038: val_loss did not improve from 35.53048\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 36.3423 - val_loss: 42.2233\n",
      "Epoch 39/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 36.0248\n",
      "Epoch 00039: val_loss did not improve from 35.53048\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 36.0250 - val_loss: 36.7240\n",
      "Epoch 40/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 35.8490\n",
      "Epoch 00040: val_loss did not improve from 35.53048\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 35.8484 - val_loss: 35.7346\n",
      "Epoch 41/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 35.6909\n",
      "Epoch 00041: val_loss did not improve from 35.53048\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 35.6862 - val_loss: 35.8262\n",
      "Epoch 42/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 35.2392\n",
      "Epoch 00042: val_loss improved from 35.53048 to 34.74645, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.2405 - val_loss: 34.7464\n",
      "Epoch 43/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 35.0934\n",
      "Epoch 00043: val_loss improved from 34.74645 to 33.98352, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.0920 - val_loss: 33.9835\n",
      "Epoch 44/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 35.0733\n",
      "Epoch 00044: val_loss did not improve from 33.98352\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 35.0740 - val_loss: 36.3069\n",
      "Epoch 45/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 34.8416\n",
      "Epoch 00045: val_loss did not improve from 33.98352\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 34.8416 - val_loss: 40.6529\n",
      "Epoch 46/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 34.7499\n",
      "Epoch 00046: val_loss did not improve from 33.98352\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 34.7470 - val_loss: 37.0208\n",
      "Epoch 47/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 34.5833\n",
      "Epoch 00047: val_loss did not improve from 33.98352\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 34.5857 - val_loss: 38.0899\n",
      "Epoch 48/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 34.3732\n",
      "Epoch 00048: val_loss did not improve from 33.98352\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 34.3732 - val_loss: 39.8719\n",
      "Epoch 49/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 34.1649\n",
      "Epoch 00049: val_loss did not improve from 33.98352\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.1643 - val_loss: 34.4612\n",
      "Epoch 50/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 33.9076\n",
      "Epoch 00050: val_loss improved from 33.98352 to 31.34080, saving model to DS02/experiment_set_8\\results_0.95\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.9025 - val_loss: 31.3408\n",
      "Epoch 51/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 34.0469\n",
      "Epoch 00051: val_loss did not improve from 31.34080\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 34.0466 - val_loss: 33.7859\n",
      "Epoch 52/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 33.7248\n",
      "Epoch 00052: val_loss did not improve from 31.34080\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 33.7248 - val_loss: 33.4345\n",
      "Epoch 53/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 33.6630\n",
      "Epoch 00053: val_loss did not improve from 31.34080\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 33.6616 - val_loss: 38.1463\n",
      "Epoch 54/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 33.6873\n",
      "Epoch 00054: val_loss did not improve from 31.34080\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 33.6877 - val_loss: 36.3231\n",
      "Epoch 55/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 33.4445\n",
      "Epoch 00055: val_loss did not improve from 31.34080\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.4477 - val_loss: 36.3902\n",
      "Epoch 56/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 33.2222\n",
      "Epoch 00056: val_loss did not improve from 31.34080\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.2266 - val_loss: 37.6552\n",
      "Epoch 57/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 33.0007\n",
      "Epoch 00057: val_loss did not improve from 31.34080\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 33.0007 - val_loss: 36.6381\n",
      "Epoch 58/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 32.9008\n",
      "Epoch 00058: val_loss did not improve from 31.34080\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 32.8960 - val_loss: 33.3960\n",
      "Epoch 59/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 32.8763\n",
      "Epoch 00059: val_loss did not improve from 31.34080\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 32.8777 - val_loss: 39.6735\n",
      "Epoch 60/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 32.6019\n",
      "Epoch 00060: val_loss did not improve from 31.34080\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 32.6014 - val_loss: 34.8011\n",
      "Epoch 00060: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_0.95\\history.pkl\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "MSE: 31.24\n",
      "RMSE: 5.59\n",
      "CMAPSS score: 1.66\n",
      "\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 256)               1536      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 231,809\n",
      "Trainable params: 231,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 222.0078\n",
      "Epoch 00001: val_loss improved from inf to 81.65204, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 221.7775 - val_loss: 81.6520\n",
      "Epoch 2/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 81.2565\n",
      "Epoch 00002: val_loss improved from 81.65204 to 75.90134, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 81.2539 - val_loss: 75.9013\n",
      "Epoch 3/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 73.8690\n",
      "Epoch 00003: val_loss improved from 75.90134 to 68.64618, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 73.8590 - val_loss: 68.6462\n",
      "Epoch 4/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 68.6850 ETA: 0s -\n",
      "Epoch 00004: val_loss did not improve from 68.64618\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 68.6831 - val_loss: 70.4634\n",
      "Epoch 5/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 65.3076\n",
      "Epoch 00005: val_loss improved from 68.64618 to 65.72013, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 65.3040 - val_loss: 65.7201\n",
      "Epoch 6/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 62.8917\n",
      "Epoch 00006: val_loss improved from 65.72013 to 62.34877, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 62.8864 - val_loss: 62.3488\n",
      "Epoch 7/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 61.0426\n",
      "Epoch 00007: val_loss did not improve from 62.34877\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 61.0364 - val_loss: 62.5581\n",
      "Epoch 8/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 59.0143\n",
      "Epoch 00008: val_loss improved from 62.34877 to 55.74463, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 58.9987 - val_loss: 55.7446\n",
      "Epoch 9/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 57.4456\n",
      "Epoch 00009: val_loss improved from 55.74463 to 54.00256, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 57.4441 - val_loss: 54.0026\n",
      "Epoch 10/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 56.1955\n",
      "Epoch 00010: val_loss did not improve from 54.00256\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 56.2028 - val_loss: 68.4775\n",
      "Epoch 11/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 54.7994\n",
      "Epoch 00011: val_loss improved from 54.00256 to 52.97558, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 54.7968 - val_loss: 52.9756\n",
      "Epoch 12/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 54.0471\n",
      "Epoch 00012: val_loss did not improve from 52.97558\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 54.0419 - val_loss: 55.0824\n",
      "Epoch 13/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 52.8688\n",
      "Epoch 00013: val_loss improved from 52.97558 to 51.86877, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 52.8705 - val_loss: 51.8688\n",
      "Epoch 14/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 51.6261\n",
      "Epoch 00014: val_loss did not improve from 51.86877\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 51.6244 - val_loss: 56.7782\n",
      "Epoch 15/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 51.1049\n",
      "Epoch 00015: val_loss did not improve from 51.86877\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 51.1046 - val_loss: 53.2740\n",
      "Epoch 16/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 50.1851\n",
      "Epoch 00016: val_loss improved from 51.86877 to 48.03629, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 50.1851 - val_loss: 48.0363\n",
      "Epoch 17/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 49.8009\n",
      "Epoch 00017: val_loss did not improve from 48.03629\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 49.7983 - val_loss: 60.9957\n",
      "Epoch 18/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 48.7744\n",
      "Epoch 00018: val_loss did not improve from 48.03629\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 48.7761 - val_loss: 49.4876\n",
      "Epoch 19/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 48.3129\n",
      "Epoch 00019: val_loss did not improve from 48.03629\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 48.3141 - val_loss: 49.6660\n",
      "Epoch 20/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 47.8972\n",
      "Epoch 00020: val_loss improved from 48.03629 to 46.62931, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 47.8986 - val_loss: 46.6293\n",
      "Epoch 21/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 47.3385\n",
      "Epoch 00021: val_loss improved from 46.62931 to 45.60695, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 47.3368 - val_loss: 45.6069\n",
      "Epoch 22/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 46.9552\n",
      "Epoch 00022: val_loss did not improve from 45.60695\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 46.9509 - val_loss: 52.3380\n",
      "Epoch 23/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 46.2047\n",
      "Epoch 00023: val_loss improved from 45.60695 to 45.60105, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 46.2080 - val_loss: 45.6011\n",
      "Epoch 24/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 46.0185\n",
      "Epoch 00024: val_loss did not improve from 45.60105\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 46.0308 - val_loss: 53.2288\n",
      "Epoch 25/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 45.5529\n",
      "Epoch 00025: val_loss did not improve from 45.60105\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 45.5607 - val_loss: 53.2741\n",
      "Epoch 26/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 45.3697\n",
      "Epoch 00026: val_loss improved from 45.60105 to 45.18422, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 45.3670 - val_loss: 45.1842\n",
      "Epoch 27/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 44.6490\n",
      "Epoch 00027: val_loss improved from 45.18422 to 41.70329, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5038/5038 [==============================] - 23s 5ms/step - loss: 44.6481 - val_loss: 41.7033\n",
      "Epoch 28/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 44.3786\n",
      "Epoch 00028: val_loss did not improve from 41.70329\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 44.3800 - val_loss: 47.3247\n",
      "Epoch 29/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 44.1325\n",
      "Epoch 00029: val_loss did not improve from 41.70329\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 44.1276 - val_loss: 43.5566\n",
      "Epoch 30/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 43.4893\n",
      "Epoch 00030: val_loss did not improve from 41.70329\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 43.5002 - val_loss: 55.1596\n",
      "Epoch 31/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 43.4441\n",
      "Epoch 00031: val_loss did not improve from 41.70329\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 43.4521 - val_loss: 44.4744\n",
      "Epoch 32/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 42.9386\n",
      "Epoch 00032: val_loss improved from 41.70329 to 40.81247, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 42.9398 - val_loss: 40.8125\n",
      "Epoch 33/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 42.7102\n",
      "Epoch 00033: val_loss did not improve from 40.81247\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 42.7124 - val_loss: 43.8431\n",
      "Epoch 34/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 42.4093\n",
      "Epoch 00034: val_loss did not improve from 40.81247\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 42.4051 - val_loss: 44.6965\n",
      "Epoch 35/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 41.9608\n",
      "Epoch 00035: val_loss did not improve from 40.81247\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 41.9581 - val_loss: 41.4772\n",
      "Epoch 36/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 41.5654\n",
      "Epoch 00036: val_loss did not improve from 40.81247\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 41.5645 - val_loss: 44.8003\n",
      "Epoch 37/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 41.3891\n",
      "Epoch 00037: val_loss did not improve from 40.81247\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 41.3918 - val_loss: 45.7682\n",
      "Epoch 38/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 41.2873\n",
      "Epoch 00038: val_loss improved from 40.81247 to 39.30827, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 41.2856 - val_loss: 39.3083\n",
      "Epoch 39/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 40.8903\n",
      "Epoch 00039: val_loss did not improve from 39.30827\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 40.8900 - val_loss: 42.3161\n",
      "Epoch 40/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 40.6660\n",
      "Epoch 00040: val_loss did not improve from 39.30827\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 40.6645 - val_loss: 41.2984\n",
      "Epoch 41/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 40.4509\n",
      "Epoch 00041: val_loss did not improve from 39.30827\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 40.4535 - val_loss: 42.6873\n",
      "Epoch 42/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 40.0921\n",
      "Epoch 00042: val_loss did not improve from 39.30827\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 40.0953 - val_loss: 40.5579\n",
      "Epoch 43/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 40.0620\n",
      "Epoch 00043: val_loss did not improve from 39.30827\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 40.0625 - val_loss: 45.4838\n",
      "Epoch 44/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 39.6809\n",
      "Epoch 00044: val_loss improved from 39.30827 to 37.87698, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 39.6763 - val_loss: 37.8770\n",
      "Epoch 45/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 39.5576\n",
      "Epoch 00045: val_loss did not improve from 37.87698\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 39.5576 - val_loss: 41.9082\n",
      "Epoch 46/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 39.1345\n",
      "Epoch 00046: val_loss did not improve from 37.87698\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.1351 - val_loss: 39.9685\n",
      "Epoch 47/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 39.1275\n",
      "Epoch 00047: val_loss did not improve from 37.87698\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 39.1258 - val_loss: 39.3728\n",
      "Epoch 48/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 38.8355\n",
      "Epoch 00048: val_loss did not improve from 37.87698\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.8330 - val_loss: 38.1909\n",
      "Epoch 49/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 38.6327\n",
      "Epoch 00049: val_loss did not improve from 37.87698\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.6286 - val_loss: 39.9602\n",
      "Epoch 50/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 38.5122\n",
      "Epoch 00050: val_loss improved from 37.87698 to 37.48714, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.5121 - val_loss: 37.4871\n",
      "Epoch 51/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 38.1440\n",
      "Epoch 00051: val_loss did not improve from 37.48714\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.1429 - val_loss: 44.4203\n",
      "Epoch 52/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 38.1646\n",
      "Epoch 00052: val_loss did not improve from 37.48714\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.1638 - val_loss: 38.2541\n",
      "Epoch 53/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 38.0410\n",
      "Epoch 00053: val_loss did not improve from 37.48714\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.0377 - val_loss: 38.0700\n",
      "Epoch 54/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 37.5883\n",
      "Epoch 00054: val_loss did not improve from 37.48714\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 37.5861 - val_loss: 38.0278\n",
      "Epoch 55/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 37.6840\n",
      "Epoch 00055: val_loss improved from 37.48714 to 36.03455, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.6841 - val_loss: 36.0345\n",
      "Epoch 56/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 37.2169\n",
      "Epoch 00056: val_loss did not improve from 36.03455\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 37.2247 - val_loss: 44.1496\n",
      "Epoch 57/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 37.3270 E\n",
      "Epoch 00057: val_loss did not improve from 36.03455\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 37.3284 - val_loss: 36.6833\n",
      "Epoch 58/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 37.3232\n",
      "Epoch 00058: val_loss did not improve from 36.03455\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 37.3144 - val_loss: 36.9805\n",
      "Epoch 59/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 36.9748\n",
      "Epoch 00059: val_loss did not improve from 36.03455\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 36.9739 - val_loss: 41.3888\n",
      "Epoch 60/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 36.7880\n",
      "Epoch 00060: val_loss did not improve from 36.03455\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 36.7873 - val_loss: 38.9047\n",
      "Epoch 61/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 36.6268\n",
      "Epoch 00061: val_loss did not improve from 36.03455\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 36.6266 - val_loss: 37.3389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 36.5015\n",
      "Epoch 00062: val_loss did not improve from 36.03455\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.5007 - val_loss: 38.7236\n",
      "Epoch 63/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 36.4189\n",
      "Epoch 00063: val_loss did not improve from 36.03455\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.4189 - val_loss: 39.0980\n",
      "Epoch 64/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 36.2023\n",
      "Epoch 00064: val_loss did not improve from 36.03455\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 36.2004 - val_loss: 37.7818\n",
      "Epoch 65/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 36.2275\n",
      "Epoch 00065: val_loss improved from 36.03455 to 36.01286, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 36.2265 - val_loss: 36.0129\n",
      "Epoch 66/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 35.9785\n",
      "Epoch 00066: val_loss improved from 36.01286 to 33.63259, saving model to DS02/experiment_set_8\\results_0.9\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 35.9721 - val_loss: 33.6326\n",
      "Epoch 67/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 35.7670\n",
      "Epoch 00067: val_loss did not improve from 33.63259\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 35.7707 - val_loss: 35.9972\n",
      "Epoch 68/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 35.8292\n",
      "Epoch 00068: val_loss did not improve from 33.63259\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 35.8295 - val_loss: 37.4114\n",
      "Epoch 69/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 35.6240\n",
      "Epoch 00069: val_loss did not improve from 33.63259\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 35.6200 - val_loss: 34.3501\n",
      "Epoch 70/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 35.3952\n",
      "Epoch 00070: val_loss did not improve from 33.63259\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 35.3934 - val_loss: 36.2552\n",
      "Epoch 71/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 35.5541\n",
      "Epoch 00071: val_loss did not improve from 33.63259\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 35.5555 - val_loss: 35.2570\n",
      "Epoch 72/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 35.1882\n",
      "Epoch 00072: val_loss did not improve from 33.63259\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 35.1848 - val_loss: 34.8054\n",
      "Epoch 73/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 35.1186\n",
      "Epoch 00073: val_loss did not improve from 33.63259\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 35.1188 - val_loss: 38.9721\n",
      "Epoch 74/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 35.0631\n",
      "Epoch 00074: val_loss did not improve from 33.63259\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 35.0621 - val_loss: 36.3239\n",
      "Epoch 75/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 34.9117\n",
      "Epoch 00075: val_loss did not improve from 33.63259\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 34.9078 - val_loss: 38.2315\n",
      "Epoch 76/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 34.9212\n",
      "Epoch 00076: val_loss did not improve from 33.63259\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 34.9188 - val_loss: 36.3621\n",
      "Epoch 00076: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_0.9\\history.pkl\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "MSE: 33.55\n",
      "RMSE: 5.79\n",
      "CMAPSS score: 1.59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "# Test effect of correlation threshold\n",
    "######################################\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "layer_sizes = [256, 256, 512, 64]\n",
    "\n",
    "initial_columns = x_train.columns\n",
    "corr_th_list = [None, 0.99, 0.95, 0.9]\n",
    "\n",
    "results_file = os.path.join(output_path, \"results.txt\")\n",
    "open(results_file, \"w\").close()\n",
    "\n",
    "for corr_th in corr_th_list:\n",
    "    # Select features based on training set\n",
    "    if corr_th is not None:\n",
    "        selected_columns = get_non_correlated_features(x_train, corr_th=corr_th, debug=False)\n",
    "    else:\n",
    "        selected_columns = x_train.columns\n",
    "    \n",
    "    x_train_feature_selection = x_train[selected_columns]\n",
    "    \n",
    "    # Train-validation split for early stopping\n",
    "    x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train_feature_selection, \n",
    "                                                                              y_train, \n",
    "                                                                              test_size=0.3, \n",
    "                                                                              random_state=seed)\n",
    "    # Create output path\n",
    "    results_folder = \"results_all\" if corr_th is None else \"results_{}\".format(corr_th)\n",
    "    results_path = os.path.join(output_path, results_folder)\n",
    "    if not os.path.exists(results_path):\n",
    "        os.makedirs(results_path)\n",
    "    \n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train_split)\n",
    "    x_val_scaled = scaler.transform(x_val_split)\n",
    "    input_dim = x_train_scaled.shape[1]\n",
    "\n",
    "    # Create model\n",
    "    weights_file = os.path.join(results_path, 'mlp_initial_weights.h5')\n",
    "    model_path = os.path.join(results_path, 'mlp_model_trained.h5')\n",
    "    model = create_mlp_model(input_dim, layer_sizes, activation='tanh', \n",
    "                             output_weights_file=weights_file)\n",
    "    model.summary()\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint(model_path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "    # Train model\n",
    "    history = train_model_existing_weights(model, weights_file, \n",
    "                                           x_train_scaled, y_train_split, \n",
    "                                           x_val_scaled, y_val_split, \n",
    "                                           batch_size=batch_size, \n",
    "                                           epochs=epochs, \n",
    "                                           callbacks=[es, mc])\n",
    "    \n",
    "    history_file = os.path.join(results_path, \"history.pkl\")\n",
    "    save_history(history, history_file)\n",
    "    plot_loss_curves(history.history, output_path=results_path)\n",
    "    \n",
    "    # Performance evaluation\n",
    "    x_test_feature_selection = x_test[selected_columns]\n",
    "    x_test_scaled = scaler.transform(x_test_feature_selection)\n",
    "    \n",
    "    loaded_model = load_model(model_path)\n",
    "    predictions_test = loaded_model.predict(x_test_scaled).flatten()\n",
    "    mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)\n",
    "    \n",
    "    with open(results_file, \"a\", newline='') as file:\n",
    "        file.write(\"Experiment {}\\n\".format(corr_th if corr_th is not None else \"ALL\"))\n",
    "        file.write(\"Selected columns: \")\n",
    "        write_list(selected_columns, file)\n",
    "        file.write(\"MSE = {}\\nRMSE = {}\\nCMAPSS = {}\\n\\n\\n\".format(mse, rmse, cmapss_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 238,721\n",
      "Trainable params: 238,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 169.4341\n",
      "Epoch 00001: val_loss improved from inf to 53.84950, saving model to DS02/experiment_set_8\\results_all\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 169.1864 - val_loss: 53.8495\n",
      "Epoch 2/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 45.1566 ETA: 0s - loss: 4 -\n",
      "Epoch 00002: val_loss improved from 53.84950 to 40.68998, saving model to DS02/experiment_set_8\\results_all\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 45.1476 - val_loss: 40.6900\n",
      "Epoch 3/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 38.3328\n",
      "Epoch 00003: val_loss improved from 40.68998 to 35.29070, saving model to DS02/experiment_set_8\\results_all\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.3328 - val_loss: 35.2907\n",
      "Epoch 4/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 33.6361\n",
      "Epoch 00004: val_loss improved from 35.29070 to 31.49156, saving model to DS02/experiment_set_8\\results_all\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.6295 - val_loss: 31.4916\n",
      "Epoch 5/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 30.4072\n",
      "Epoch 00005: val_loss improved from 31.49156 to 29.47140, saving model to DS02/experiment_set_8\\results_all\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 30.4060 - val_loss: 29.4714\n",
      "Epoch 6/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 27.4049\n",
      "Epoch 00006: val_loss improved from 29.47140 to 24.28102, saving model to DS02/experiment_set_8\\results_all\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 27.4047 - val_loss: 24.2810\n",
      "Epoch 7/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 25.4834\n",
      "Epoch 00007: val_loss improved from 24.28102 to 23.44156, saving model to DS02/experiment_set_8\\results_all\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 25.4820 - val_loss: 23.4416\n",
      "Epoch 8/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 23.8083\n",
      "Epoch 00008: val_loss improved from 23.44156 to 20.40741, saving model to DS02/experiment_set_8\\results_all\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 23.8015 - val_loss: 20.4074\n",
      "Epoch 9/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 22.7790\n",
      "Epoch 00009: val_loss did not improve from 20.40741\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 22.7813 - val_loss: 24.1462\n",
      "Epoch 10/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 22.1506\n",
      "Epoch 00010: val_loss improved from 20.40741 to 19.48792, saving model to DS02/experiment_set_8\\results_all\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 22.1493 - val_loss: 19.4879\n",
      "Epoch 11/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 21.3481\n",
      "Epoch 00011: val_loss did not improve from 19.48792\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 21.3495 - val_loss: 25.3114\n",
      "Epoch 12/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 20.5878\n",
      "Epoch 00012: val_loss did not improve from 19.48792\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 20.5863 - val_loss: 20.2262\n",
      "Epoch 13/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 20.1468\n",
      "Epoch 00013: val_loss improved from 19.48792 to 15.71443, saving model to DS02/experiment_set_8\\results_all\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 20.1406 - val_loss: 15.7144\n",
      "Epoch 14/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 19.6178\n",
      "Epoch 00014: val_loss did not improve from 15.71443\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 19.6161 - val_loss: 22.9601\n",
      "Epoch 15/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 19.3643\n",
      "Epoch 00015: val_loss did not improve from 15.71443\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 19.3603 - val_loss: 26.0393\n",
      "Epoch 16/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 18.7099 ETA: 0s - los\n",
      "Epoch 00016: val_loss did not improve from 15.71443\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.7126 - val_loss: 34.9309\n",
      "Epoch 17/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 18.6069\n",
      "Epoch 00017: val_loss improved from 15.71443 to 15.20309, saving model to DS02/experiment_set_8\\results_all\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 18.6056 - val_loss: 15.2031\n",
      "Epoch 18/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 18.2145\n",
      "Epoch 00018: val_loss did not improve from 15.20309\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.2145 - val_loss: 21.0321\n",
      "Epoch 19/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 17.7355\n",
      "Epoch 00019: val_loss did not improve from 15.20309\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 17.7355 - val_loss: 20.5853\n",
      "Epoch 20/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 17.4222\n",
      "Epoch 00020: val_loss did not improve from 15.20309\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.4227 - val_loss: 16.5784\n",
      "Epoch 21/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 17.0932\n",
      "Epoch 00021: val_loss did not improve from 15.20309\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.0924 - val_loss: 15.3289\n",
      "Epoch 22/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 17.0566\n",
      "Epoch 00022: val_loss did not improve from 15.20309\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.0680 - val_loss: 20.5979\n",
      "Epoch 23/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 16.6088\n",
      "Epoch 00023: val_loss did not improve from 15.20309\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.6088 - val_loss: 20.1304\n",
      "Epoch 24/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 16.5908\n",
      "Epoch 00024: val_loss did not improve from 15.20309\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.5909 - val_loss: 17.8411\n",
      "Epoch 25/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 16.0250\n",
      "Epoch 00025: val_loss did not improve from 15.20309\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.0250 - val_loss: 24.6908\n",
      "Epoch 26/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 15.8840\n",
      "Epoch 00026: val_loss did not improve from 15.20309\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.8871 - val_loss: 16.4437\n",
      "Epoch 27/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 15.9220\n",
      "Epoch 00027: val_loss did not improve from 15.20309\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.9193 - val_loss: 15.7711\n",
      "Epoch 00027: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_all\\split_0\\history.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "MSE: 15.20\n",
      "RMSE: 3.90\n",
      "CMAPSS score: 1.30\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 238,721\n",
      "Trainable params: 238,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 169.2110\n",
      "Epoch 00001: val_loss improved from inf to 47.58438, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 169.1813 - val_loss: 47.5844\n",
      "Epoch 2/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 45.3558\n",
      "Epoch 00002: val_loss improved from 47.58438 to 38.66451, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 45.3336 - val_loss: 38.6645\n",
      "Epoch 3/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 38.1967\n",
      "Epoch 00003: val_loss improved from 38.66451 to 35.15981, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.1969 - val_loss: 35.1598\n",
      "Epoch 4/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 33.3248\n",
      "Epoch 00004: val_loss improved from 35.15981 to 31.20448, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.3212 - val_loss: 31.2045\n",
      "Epoch 5/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 29.7776\n",
      "Epoch 00005: val_loss did not improve from 31.20448\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 29.7698 - val_loss: 34.1023\n",
      "Epoch 6/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 26.9326\n",
      "Epoch 00006: val_loss improved from 31.20448 to 27.44238, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 26.9331 - val_loss: 27.4424\n",
      "Epoch 7/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 25.0664\n",
      "Epoch 00007: val_loss did not improve from 27.44238\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 25.0863 - val_loss: 28.3819\n",
      "Epoch 8/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 23.6667\n",
      "Epoch 00008: val_loss improved from 27.44238 to 20.78634, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 23.6625 - val_loss: 20.7863\n",
      "Epoch 9/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 23.0978\n",
      "Epoch 00009: val_loss improved from 20.78634 to 20.06656, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 23.0978 - val_loss: 20.0666\n",
      "Epoch 10/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 21.9417\n",
      "Epoch 00010: val_loss improved from 20.06656 to 18.33284, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 21.9381 - val_loss: 18.3328\n",
      "Epoch 11/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 21.3636\n",
      "Epoch 00011: val_loss did not improve from 18.33284\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 21.3639 - val_loss: 20.7780\n",
      "Epoch 12/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 20.6968\n",
      "Epoch 00012: val_loss did not improve from 18.33284\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 20.7041 - val_loss: 47.5263\n",
      "Epoch 13/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 20.2105\n",
      "Epoch 00013: val_loss improved from 18.33284 to 16.75959, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 20.1994 - val_loss: 16.7596\n",
      "Epoch 14/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 19.6239\n",
      "Epoch 00014: val_loss did not improve from 16.75959\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 19.6236 - val_loss: 24.7523\n",
      "Epoch 15/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 19.2743\n",
      "Epoch 00015: val_loss did not improve from 16.75959\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 19.2730 - val_loss: 20.1038\n",
      "Epoch 16/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 19.0992\n",
      "Epoch 00016: val_loss improved from 16.75959 to 16.71677, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 19.1017 - val_loss: 16.7168\n",
      "Epoch 17/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 18.3763\n",
      "Epoch 00017: val_loss did not improve from 16.71677\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.3770 - val_loss: 17.5485\n",
      "Epoch 18/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 18.3638\n",
      "Epoch 00018: val_loss did not improve from 16.71677\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 18.3638 - val_loss: 18.4654\n",
      "Epoch 19/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 17.8553\n",
      "Epoch 00019: val_loss improved from 16.71677 to 16.15038, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.8612 - val_loss: 16.1504\n",
      "Epoch 20/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 17.5276\n",
      "Epoch 00020: val_loss did not improve from 16.15038\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.5282 - val_loss: 18.3241\n",
      "Epoch 21/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 17.3093\n",
      "Epoch 00021: val_loss did not improve from 16.15038\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.3093 - val_loss: 24.9771\n",
      "Epoch 22/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 16.8486\n",
      "Epoch 00022: val_loss improved from 16.15038 to 13.79770, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.8496 - val_loss: 13.7977\n",
      "Epoch 23/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 16.5303\n",
      "Epoch 00023: val_loss did not improve from 13.79770\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.5302 - val_loss: 15.1927\n",
      "Epoch 24/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 16.5514\n",
      "Epoch 00024: val_loss did not improve from 13.79770\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.5669 - val_loss: 21.8112\n",
      "Epoch 25/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 16.2274\n",
      "Epoch 00025: val_loss did not improve from 13.79770\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.2324 - val_loss: 22.9498\n",
      "Epoch 26/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 15.9141\n",
      "Epoch 00026: val_loss improved from 13.79770 to 13.66935, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.9139 - val_loss: 13.6694\n",
      "Epoch 27/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 16.0755\n",
      "Epoch 00027: val_loss did not improve from 13.66935\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.0755 - val_loss: 19.4146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 15.6468\n",
      "Epoch 00028: val_loss did not improve from 13.66935\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.6403 - val_loss: 15.6003\n",
      "Epoch 29/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 15.4177\n",
      "Epoch 00029: val_loss did not improve from 13.66935\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.4153 - val_loss: 15.0630\n",
      "Epoch 30/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 15.1845\n",
      "Epoch 00030: val_loss improved from 13.66935 to 13.30455, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.1839 - val_loss: 13.3046\n",
      "Epoch 31/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 15.2408\n",
      "Epoch 00031: val_loss did not improve from 13.30455\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.2438 - val_loss: 16.9943\n",
      "Epoch 32/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 14.8638\n",
      "Epoch 00032: val_loss did not improve from 13.30455\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.8638 - val_loss: 15.1443\n",
      "Epoch 33/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 14.7916\n",
      "Epoch 00033: val_loss did not improve from 13.30455\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.7912 - val_loss: 14.6904\n",
      "Epoch 34/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 14.5659\n",
      "Epoch 00034: val_loss did not improve from 13.30455\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.5679 - val_loss: 19.3814\n",
      "Epoch 35/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 14.4204\n",
      "Epoch 00035: val_loss improved from 13.30455 to 12.16548, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.4204 - val_loss: 12.1655\n",
      "Epoch 36/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 14.3053\n",
      "Epoch 00036: val_loss did not improve from 12.16548\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.3058 - val_loss: 27.7371\n",
      "Epoch 37/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 14.4313\n",
      "Epoch 00037: val_loss improved from 12.16548 to 11.45507, saving model to DS02/experiment_set_8\\results_all\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.4298 - val_loss: 11.4551\n",
      "Epoch 38/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 14.1874\n",
      "Epoch 00038: val_loss did not improve from 11.45507\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.1869 - val_loss: 12.7108\n",
      "Epoch 39/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 14.0133\n",
      "Epoch 00039: val_loss did not improve from 11.45507\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.0107 - val_loss: 13.0959\n",
      "Epoch 40/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 14.1231\n",
      "Epoch 00040: val_loss did not improve from 11.45507\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.1256 - val_loss: 19.0267\n",
      "Epoch 41/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 13.6137\n",
      "Epoch 00041: val_loss did not improve from 11.45507\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.6138 - val_loss: 12.0547\n",
      "Epoch 42/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 13.6369\n",
      "Epoch 00042: val_loss did not improve from 11.45507\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.6365 - val_loss: 12.5120\n",
      "Epoch 43/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 13.6395\n",
      "Epoch 00043: val_loss did not improve from 11.45507\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.6441 - val_loss: 21.7007\n",
      "Epoch 44/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 13.6382\n",
      "Epoch 00044: val_loss did not improve from 11.45507\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.6375 - val_loss: 12.9358\n",
      "Epoch 45/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 13.6459\n",
      "Epoch 00045: val_loss did not improve from 11.45507\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.6422 - val_loss: 12.8348\n",
      "Epoch 46/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 13.5328\n",
      "Epoch 00046: val_loss did not improve from 11.45507\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.5285 - val_loss: 13.8717\n",
      "Epoch 47/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 13.4597\n",
      "Epoch 00047: val_loss did not improve from 11.45507\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.4630 - val_loss: 12.4461\n",
      "Epoch 00047: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_all\\split_1\\history.pkl\n",
      "Test set:\n",
      "MSE: 11.48\n",
      "RMSE: 3.39\n",
      "CMAPSS score: 1.23\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 238,721\n",
      "Trainable params: 238,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 169.5807\n",
      "Epoch 00001: val_loss improved from inf to 50.95187, saving model to DS02/experiment_set_8\\results_all\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 169.4289 - val_loss: 50.9519\n",
      "Epoch 2/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 45.2726\n",
      "Epoch 00002: val_loss improved from 50.95187 to 38.23264, saving model to DS02/experiment_set_8\\results_all\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 45.2523 - val_loss: 38.2326\n",
      "Epoch 3/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 38.0945\n",
      "Epoch 00003: val_loss improved from 38.23264 to 35.04873, saving model to DS02/experiment_set_8\\results_all\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 38.0922 - val_loss: 35.0487\n",
      "Epoch 4/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 33.7167\n",
      "Epoch 00004: val_loss improved from 35.04873 to 29.41243, saving model to DS02/experiment_set_8\\results_all\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 33.7115 - val_loss: 29.4124\n",
      "Epoch 5/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 30.3079\n",
      "Epoch 00005: val_loss did not improve from 29.41243\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 30.3020 - val_loss: 29.8820\n",
      "Epoch 6/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 27.0250\n",
      "Epoch 00006: val_loss improved from 29.41243 to 21.82777, saving model to DS02/experiment_set_8\\results_all\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 27.0228 - val_loss: 21.8278\n",
      "Epoch 7/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 25.6250\n",
      "Epoch 00007: val_loss did not improve from 21.82777\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 25.6268 - val_loss: 25.6378\n",
      "Epoch 8/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 24.0428\n",
      "Epoch 00008: val_loss improved from 21.82777 to 20.97129, saving model to DS02/experiment_set_8\\results_all\\split_2\\mlp_model_trained.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5038/5038 [==============================] - 25s 5ms/step - loss: 24.0414 - val_loss: 20.9713\n",
      "Epoch 9/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 22.8400\n",
      "Epoch 00009: val_loss improved from 20.97129 to 18.35376, saving model to DS02/experiment_set_8\\results_all\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 22.8377 - val_loss: 18.3538\n",
      "Epoch 10/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 22.1294\n",
      "Epoch 00010: val_loss did not improve from 18.35376\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 22.1284 - val_loss: 24.8601\n",
      "Epoch 11/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 21.6550\n",
      "Epoch 00011: val_loss did not improve from 18.35376\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 21.6494 - val_loss: 21.3363\n",
      "Epoch 12/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 20.9364\n",
      "Epoch 00012: val_loss did not improve from 18.35376\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 20.9337 - val_loss: 21.5268\n",
      "Epoch 13/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 20.0864\n",
      "Epoch 00013: val_loss improved from 18.35376 to 17.03781, saving model to DS02/experiment_set_8\\results_all\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 20.0857 - val_loss: 17.0378\n",
      "Epoch 14/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 19.8381\n",
      "Epoch 00014: val_loss did not improve from 17.03781\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 19.8361 - val_loss: 22.0742\n",
      "Epoch 15/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 19.2273\n",
      "Epoch 00015: val_loss did not improve from 17.03781\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 19.2259 - val_loss: 25.8626\n",
      "Epoch 16/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 18.9353\n",
      "Epoch 00016: val_loss did not improve from 17.03781\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 18.9354 - val_loss: 19.7224\n",
      "Epoch 17/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 18.3767\n",
      "Epoch 00017: val_loss improved from 17.03781 to 16.37184, saving model to DS02/experiment_set_8\\results_all\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 18.3715 - val_loss: 16.3718\n",
      "Epoch 18/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 18.3310\n",
      "Epoch 00018: val_loss did not improve from 16.37184\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 18.3313 - val_loss: 18.1582\n",
      "Epoch 19/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 17.8897\n",
      "Epoch 00019: val_loss did not improve from 16.37184\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.8895 - val_loss: 18.1593\n",
      "Epoch 20/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 17.5511\n",
      "Epoch 00020: val_loss did not improve from 16.37184\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.5561 - val_loss: 17.0669\n",
      "Epoch 21/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 17.2922\n",
      "Epoch 00021: val_loss did not improve from 16.37184\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 17.2921 - val_loss: 20.7302\n",
      "Epoch 22/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 17.2637\n",
      "Epoch 00022: val_loss did not improve from 16.37184\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 17.2649 - val_loss: 17.8699\n",
      "Epoch 23/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 16.5961\n",
      "Epoch 00023: val_loss did not improve from 16.37184\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 16.5948 - val_loss: 16.9072\n",
      "Epoch 24/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 16.6947\n",
      "Epoch 00024: val_loss did not improve from 16.37184\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 16.6947 - val_loss: 16.9271\n",
      "Epoch 25/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 16.2058\n",
      "Epoch 00025: val_loss did not improve from 16.37184\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.2063 - val_loss: 27.7416\n",
      "Epoch 26/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 15.9249\n",
      "Epoch 00026: val_loss improved from 16.37184 to 14.48248, saving model to DS02/experiment_set_8\\results_all\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.9253 - val_loss: 14.4825\n",
      "Epoch 27/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 15.9299\n",
      "Epoch 00027: val_loss did not improve from 14.48248\n",
      "5038/5038 [==============================] - 27s 5ms/step - loss: 15.9293 - val_loss: 20.8765\n",
      "Epoch 28/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 15.6578\n",
      "Epoch 00028: val_loss did not improve from 14.48248\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.6553 - val_loss: 29.4152\n",
      "Epoch 29/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 15.5579\n",
      "Epoch 00029: val_loss did not improve from 14.48248\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 15.5643 - val_loss: 32.2619\n",
      "Epoch 30/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 15.3103\n",
      "Epoch 00030: val_loss did not improve from 14.48248\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.3087 - val_loss: 16.8103\n",
      "Epoch 31/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 15.3043\n",
      "Epoch 00031: val_loss improved from 14.48248 to 13.35108, saving model to DS02/experiment_set_8\\results_all\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.3038 - val_loss: 13.3511\n",
      "Epoch 32/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 14.9805\n",
      "Epoch 00032: val_loss did not improve from 13.35108\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.9805 - val_loss: 13.7375\n",
      "Epoch 33/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 14.8394\n",
      "Epoch 00033: val_loss did not improve from 13.35108\n",
      "5038/5038 [==============================] - 29s 6ms/step - loss: 14.8391 - val_loss: 15.3370\n",
      "Epoch 34/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 14.6380\n",
      "Epoch 00034: val_loss improved from 13.35108 to 12.66603, saving model to DS02/experiment_set_8\\results_all\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 28s 6ms/step - loss: 14.6380 - val_loss: 12.6660\n",
      "Epoch 35/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 14.3588\n",
      "Epoch 00035: val_loss did not improve from 12.66603\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 14.3586 - val_loss: 20.0079\n",
      "Epoch 36/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 14.4807\n",
      "Epoch 00036: val_loss did not improve from 12.66603\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.4812 - val_loss: 28.9055\n",
      "Epoch 37/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 14.0762\n",
      "Epoch 00037: val_loss did not improve from 12.66603\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.0745 - val_loss: 14.9614\n",
      "Epoch 38/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 14.0436\n",
      "Epoch 00038: val_loss did not improve from 12.66603\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.0436 - val_loss: 22.6146\n",
      "Epoch 39/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 14.0158\n",
      "Epoch 00039: val_loss did not improve from 12.66603\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.0158 - val_loss: 13.0448\n",
      "Epoch 40/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 13.8716\n",
      "Epoch 00040: val_loss did not improve from 12.66603\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.8691 - val_loss: 22.1347\n",
      "Epoch 41/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 13.9713\n",
      "Epoch 00041: val_loss did not improve from 12.66603\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.9711 - val_loss: 14.0272\n",
      "Epoch 42/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 13.5852\n",
      "Epoch 00042: val_loss improved from 12.66603 to 11.28531, saving model to DS02/experiment_set_8\\results_all\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.5835 - val_loss: 11.2853\n",
      "Epoch 43/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 13.6011\n",
      "Epoch 00043: val_loss did not improve from 11.28531\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.5999 - val_loss: 18.0585\n",
      "Epoch 44/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 13.3352\n",
      "Epoch 00044: val_loss did not improve from 11.28531\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.3322 - val_loss: 12.8452\n",
      "Epoch 45/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 13.4898\n",
      "Epoch 00045: val_loss did not improve from 11.28531\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.4853 - val_loss: 12.2065\n",
      "Epoch 46/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 13.1966\n",
      "Epoch 00046: val_loss did not improve from 11.28531\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.2002 - val_loss: 22.2726\n",
      "Epoch 47/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 13.2141\n",
      "Epoch 00047: val_loss did not improve from 11.28531\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.2141 - val_loss: 14.4715\n",
      "Epoch 48/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 13.0631\n",
      "Epoch 00048: val_loss did not improve from 11.28531\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.0626 - val_loss: 27.2543\n",
      "Epoch 49/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 12.9349\n",
      "Epoch 00049: val_loss did not improve from 11.28531\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 12.9386 - val_loss: 17.5392\n",
      "Epoch 50/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 12.6939\n",
      "Epoch 00050: val_loss did not improve from 11.28531\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.6939 - val_loss: 15.2367\n",
      "Epoch 51/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 12.8584\n",
      "Epoch 00051: val_loss did not improve from 11.28531\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.8530 - val_loss: 12.1258\n",
      "Epoch 52/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 12.4273\n",
      "Epoch 00052: val_loss did not improve from 11.28531\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 12.4247 - val_loss: 20.7352\n",
      "Epoch 00052: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_all\\split_2\\history.pkl\n",
      "Test set:\n",
      "MSE: 11.31\n",
      "RMSE: 3.36\n",
      "CMAPSS score: 1.23\n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 256)               4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 234,369\n",
      "Trainable params: 234,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 166.8818\n",
      "Epoch 00001: val_loss improved from inf to 48.78790, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 166.8818 - val_loss: 48.7879\n",
      "Epoch 2/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 44.3089\n",
      "Epoch 00002: val_loss improved from 48.78790 to 37.91762, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 44.2977 - val_loss: 37.9176\n",
      "Epoch 3/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 37.7116\n",
      "Epoch 00003: val_loss improved from 37.91762 to 35.22710, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 37.7104 - val_loss: 35.2271\n",
      "Epoch 4/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 34.1284\n",
      "Epoch 00004: val_loss improved from 35.22710 to 31.97725, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 34.1267 - val_loss: 31.9773\n",
      "Epoch 5/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 31.5863\n",
      "Epoch 00005: val_loss did not improve from 31.97725\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 31.5868 - val_loss: 41.1592\n",
      "Epoch 6/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 29.8095\n",
      "Epoch 00006: val_loss improved from 31.97725 to 28.84609, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 29.8108 - val_loss: 28.8461\n",
      "Epoch 7/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 28.2975\n",
      "Epoch 00007: val_loss did not improve from 28.84609\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 28.2969 - val_loss: 35.2430\n",
      "Epoch 8/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 26.8440\n",
      "Epoch 00008: val_loss improved from 28.84609 to 25.68246, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 26.8396 - val_loss: 25.6825\n",
      "Epoch 9/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 25.4335\n",
      "Epoch 00009: val_loss did not improve from 25.68246\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 25.4335 - val_loss: 27.4791\n",
      "Epoch 10/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 24.4232\n",
      "Epoch 00010: val_loss improved from 25.68246 to 23.44014, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 24.4222 - val_loss: 23.4401\n",
      "Epoch 11/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 23.3458\n",
      "Epoch 00011: val_loss improved from 23.44014 to 22.41366, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 23.3458 - val_loss: 22.4137\n",
      "Epoch 12/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 22.6841\n",
      "Epoch 00012: val_loss improved from 22.41366 to 20.39038, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 22.6797 - val_loss: 20.3904\n",
      "Epoch 13/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 21.9228\n",
      "Epoch 00013: val_loss did not improve from 20.39038\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 21.9188 - val_loss: 23.2901\n",
      "Epoch 14/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 21.1595\n",
      "Epoch 00014: val_loss did not improve from 20.39038\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 21.1582 - val_loss: 23.0211\n",
      "Epoch 15/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 20.8161\n",
      "Epoch 00015: val_loss did not improve from 20.39038\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 20.8257 - val_loss: 21.5241\n",
      "Epoch 16/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 20.1651\n",
      "Epoch 00016: val_loss improved from 20.39038 to 18.90743, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 20.1631 - val_loss: 18.9074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 19.8526\n",
      "Epoch 00017: val_loss improved from 18.90743 to 18.38799, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 19.8527 - val_loss: 18.3880\n",
      "Epoch 18/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 19.6944\n",
      "Epoch 00018: val_loss did not improve from 18.38799\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 19.6944 - val_loss: 20.7426\n",
      "Epoch 19/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 19.0910\n",
      "Epoch 00019: val_loss improved from 18.38799 to 17.65604, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 19.0929 - val_loss: 17.6560\n",
      "Epoch 20/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 18.6269\n",
      "Epoch 00020: val_loss improved from 17.65604 to 17.60097, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.6257 - val_loss: 17.6010\n",
      "Epoch 21/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 18.5023\n",
      "Epoch 00021: val_loss did not improve from 17.60097\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.5029 - val_loss: 17.7602\n",
      "Epoch 22/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 18.2160\n",
      "Epoch 00022: val_loss improved from 17.60097 to 16.57103, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.2152 - val_loss: 16.5710\n",
      "Epoch 23/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 17.9680\n",
      "Epoch 00023: val_loss did not improve from 16.57103\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.9716 - val_loss: 18.9363\n",
      "Epoch 24/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 17.6537\n",
      "Epoch 00024: val_loss improved from 16.57103 to 15.78923, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.6532 - val_loss: 15.7892\n",
      "Epoch 25/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 17.5092\n",
      "Epoch 00025: val_loss did not improve from 15.78923\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 17.5098 - val_loss: 22.2627\n",
      "Epoch 26/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 17.3070\n",
      "Epoch 00026: val_loss did not improve from 15.78923\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.3057 - val_loss: 17.3463\n",
      "Epoch 27/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 17.0846\n",
      "Epoch 00027: val_loss did not improve from 15.78923\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.0846 - val_loss: 17.0960\n",
      "Epoch 28/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 16.9095\n",
      "Epoch 00028: val_loss did not improve from 15.78923\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.9067 - val_loss: 17.1403\n",
      "Epoch 29/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 16.5094\n",
      "Epoch 00029: val_loss did not improve from 15.78923\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.5112 - val_loss: 20.6741\n",
      "Epoch 30/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 16.4428\n",
      "Epoch 00030: val_loss did not improve from 15.78923\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.4445 - val_loss: 18.4845\n",
      "Epoch 31/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 16.1935\n",
      "Epoch 00031: val_loss did not improve from 15.78923\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.1938 - val_loss: 16.2176\n",
      "Epoch 32/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 16.0999\n",
      "Epoch 00032: val_loss did not improve from 15.78923\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.0987 - val_loss: 16.0981\n",
      "Epoch 33/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 15.9199\n",
      "Epoch 00033: val_loss did not improve from 15.78923\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.9198 - val_loss: 16.4531\n",
      "Epoch 34/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 15.9519\n",
      "Epoch 00034: val_loss improved from 15.78923 to 14.90986, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.9512 - val_loss: 14.9099\n",
      "Epoch 35/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 15.8725\n",
      "Epoch 00035: val_loss did not improve from 14.90986\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.8718 - val_loss: 16.2233\n",
      "Epoch 36/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 15.6543\n",
      "Epoch 00036: val_loss did not improve from 14.90986\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.6545 - val_loss: 15.0760\n",
      "Epoch 37/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 15.5593\n",
      "Epoch 00037: val_loss improved from 14.90986 to 13.96378, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.5584 - val_loss: 13.9638\n",
      "Epoch 38/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 15.2812\n",
      "Epoch 00038: val_loss did not improve from 13.96378\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.2816 - val_loss: 17.7501\n",
      "Epoch 39/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 15.1986\n",
      "Epoch 00039: val_loss did not improve from 13.96378\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.1946 - val_loss: 14.7023\n",
      "Epoch 40/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 15.1507\n",
      "Epoch 00040: val_loss did not improve from 13.96378\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.1490 - val_loss: 16.1018\n",
      "Epoch 41/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 15.0403\n",
      "Epoch 00041: val_loss did not improve from 13.96378\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.0427 - val_loss: 14.5102\n",
      "Epoch 42/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 14.9414\n",
      "Epoch 00042: val_loss did not improve from 13.96378\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.9414 - val_loss: 14.3916\n",
      "Epoch 43/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 14.8077\n",
      "Epoch 00043: val_loss improved from 13.96378 to 13.91340, saving model to DS02/experiment_set_8\\results_0.99\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.8076 - val_loss: 13.9134\n",
      "Epoch 44/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 14.6094\n",
      "Epoch 00044: val_loss did not improve from 13.91340\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.6095 - val_loss: 14.6888\n",
      "Epoch 45/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 14.6729\n",
      "Epoch 00045: val_loss did not improve from 13.91340\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.6725 - val_loss: 14.9093\n",
      "Epoch 46/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 14.5294\n",
      "Epoch 00046: val_loss did not improve from 13.91340\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.5294 - val_loss: 15.6371\n",
      "Epoch 47/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 14.4961\n",
      "Epoch 00047: val_loss did not improve from 13.91340\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.4958 - val_loss: 16.0468\n",
      "Epoch 48/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 14.4318\n",
      "Epoch 00048: val_loss did not improve from 13.91340\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.4288 - val_loss: 14.7256\n",
      "Epoch 49/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 14.2785\n",
      "Epoch 00049: val_loss did not improve from 13.91340\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.2785 - val_loss: 14.6639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 14.2563\n",
      "Epoch 00050: val_loss did not improve from 13.91340\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.2548 - val_loss: 14.8894\n",
      "Epoch 51/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 14.1428\n",
      "Epoch 00051: val_loss did not improve from 13.91340\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.1436 - val_loss: 15.3078\n",
      "Epoch 52/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 14.0413\n",
      "Epoch 00052: val_loss did not improve from 13.91340\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.0452 - val_loss: 16.7999\n",
      "Epoch 53/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 14.0762\n",
      "Epoch 00053: val_loss did not improve from 13.91340\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.0782 - val_loss: 17.2753\n",
      "Epoch 00053: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_0.99\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 13.92\n",
      "RMSE: 3.73\n",
      "CMAPSS score: 1.28\n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               4096      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 234,369\n",
      "Trainable params: 234,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 167.1537\n",
      "Epoch 00001: val_loss improved from inf to 49.01165, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 167.1012 - val_loss: 49.0117\n",
      "Epoch 2/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 44.2190\n",
      "Epoch 00002: val_loss improved from 49.01165 to 38.20718, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 44.2110 - val_loss: 38.2072\n",
      "Epoch 3/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 37.6377\n",
      "Epoch 00003: val_loss improved from 38.20718 to 34.84026, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 37.6380 - val_loss: 34.8403\n",
      "Epoch 4/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 34.0173\n",
      "Epoch 00004: val_loss did not improve from 34.84026\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.0178 - val_loss: 35.6200\n",
      "Epoch 5/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 31.4634\n",
      "Epoch 00005: val_loss did not improve from 34.84026\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 31.4639 - val_loss: 37.7697\n",
      "Epoch 6/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 29.6460\n",
      "Epoch 00006: val_loss improved from 34.84026 to 27.27347, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 29.6459 - val_loss: 27.2735\n",
      "Epoch 7/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 28.0384\n",
      "Epoch 00007: val_loss did not improve from 27.27347\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 28.0375 - val_loss: 36.6105\n",
      "Epoch 8/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 26.6186\n",
      "Epoch 00008: val_loss improved from 27.27347 to 25.65658, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 26.6141 - val_loss: 25.6566\n",
      "Epoch 9/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 25.3002\n",
      "Epoch 00009: val_loss improved from 25.65658 to 23.77640, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 25.3013 - val_loss: 23.7764\n",
      "Epoch 10/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 24.2879\n",
      "Epoch 00010: val_loss did not improve from 23.77640\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 24.2873 - val_loss: 23.7951\n",
      "Epoch 11/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 23.3251\n",
      "Epoch 00011: val_loss did not improve from 23.77640\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 23.3243 - val_loss: 26.5140\n",
      "Epoch 12/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 22.6002\n",
      "Epoch 00012: val_loss did not improve from 23.77640\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 22.5956 - val_loss: 25.5295\n",
      "Epoch 13/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 21.8917\n",
      "Epoch 00013: val_loss improved from 23.77640 to 19.60127, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 21.8900 - val_loss: 19.6013\n",
      "Epoch 14/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 21.2373 ETA\n",
      "Epoch 00014: val_loss did not improve from 19.60127\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 21.2358 - val_loss: 22.2951\n",
      "Epoch 15/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 20.9100\n",
      "Epoch 00015: val_loss did not improve from 19.60127\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 20.9094 - val_loss: 20.2264\n",
      "Epoch 16/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 20.2805\n",
      "Epoch 00016: val_loss did not improve from 19.60127\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 20.2794 - val_loss: 19.7745\n",
      "Epoch 17/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 19.7254\n",
      "Epoch 00017: val_loss improved from 19.60127 to 19.13729, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 19.7255 - val_loss: 19.1373\n",
      "Epoch 18/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 19.6657\n",
      "Epoch 00018: val_loss did not improve from 19.13729\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 19.6635 - val_loss: 20.2329\n",
      "Epoch 19/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 19.1698\n",
      "Epoch 00019: val_loss did not improve from 19.13729\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 19.1707 - val_loss: 20.3226\n",
      "Epoch 20/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 18.8461\n",
      "Epoch 00020: val_loss improved from 19.13729 to 18.77670, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.8459 - val_loss: 18.7767\n",
      "Epoch 21/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 18.5544\n",
      "Epoch 00021: val_loss improved from 18.77670 to 18.44855, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 18.5529 - val_loss: 18.4485\n",
      "Epoch 22/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 18.2861\n",
      "Epoch 00022: val_loss did not improve from 18.44855\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.2872 - val_loss: 19.5922\n",
      "Epoch 23/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 17.9464\n",
      "Epoch 00023: val_loss did not improve from 18.44855\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.9476 - val_loss: 24.7790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 17.8758\n",
      "Epoch 00024: val_loss improved from 18.44855 to 18.23912, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.8783 - val_loss: 18.2391\n",
      "Epoch 25/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 17.7901\n",
      "Epoch 00025: val_loss improved from 18.23912 to 17.17035, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 17.7914 - val_loss: 17.1703\n",
      "Epoch 26/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 17.4750\n",
      "Epoch 00026: val_loss did not improve from 17.17035\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.4733 - val_loss: 18.5672\n",
      "Epoch 27/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 17.0963\n",
      "Epoch 00027: val_loss did not improve from 17.17035\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.0978 - val_loss: 18.2941\n",
      "Epoch 28/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 17.0479\n",
      "Epoch 00028: val_loss did not improve from 17.17035\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.0448 - val_loss: 18.0722\n",
      "Epoch 29/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 16.9373\n",
      "Epoch 00029: val_loss did not improve from 17.17035\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.9425 - val_loss: 20.9300\n",
      "Epoch 30/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 16.6958\n",
      "Epoch 00030: val_loss did not improve from 17.17035\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.6960 - val_loss: 21.2734\n",
      "Epoch 31/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 16.4033\n",
      "Epoch 00031: val_loss did not improve from 17.17035\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.4045 - val_loss: 19.1140\n",
      "Epoch 32/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 16.2328\n",
      "Epoch 00032: val_loss improved from 17.17035 to 16.80406, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.2328 - val_loss: 16.8041\n",
      "Epoch 33/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 16.2394\n",
      "Epoch 00033: val_loss improved from 16.80406 to 16.28016, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 16.2399 - val_loss: 16.2802\n",
      "Epoch 34/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 15.9741\n",
      "Epoch 00034: val_loss improved from 16.28016 to 15.35888, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 15.9741 - val_loss: 15.3589\n",
      "Epoch 35/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 15.9798\n",
      "Epoch 00035: val_loss did not improve from 15.35888\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.9789 - val_loss: 15.4028\n",
      "Epoch 36/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 15.7829\n",
      "Epoch 00036: val_loss improved from 15.35888 to 14.66806, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.7832 - val_loss: 14.6681\n",
      "Epoch 37/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 15.7215\n",
      "Epoch 00037: val_loss did not improve from 14.66806\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.7206 - val_loss: 20.3096\n",
      "Epoch 38/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 15.4555\n",
      "Epoch 00038: val_loss did not improve from 14.66806\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.4556 - val_loss: 15.2337\n",
      "Epoch 39/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 15.4266\n",
      "Epoch 00039: val_loss did not improve from 14.66806\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.4272 - val_loss: 15.1040\n",
      "Epoch 40/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 15.1869\n",
      "Epoch 00040: val_loss did not improve from 14.66806\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.1869 - val_loss: 17.3195\n",
      "Epoch 41/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 15.1031\n",
      "Epoch 00041: val_loss did not improve from 14.66806\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.1053 - val_loss: 15.5398\n",
      "Epoch 42/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 15.0609\n",
      "Epoch 00042: val_loss did not improve from 14.66806\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.0618 - val_loss: 14.6917\n",
      "Epoch 43/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 14.8263\n",
      "Epoch 00043: val_loss improved from 14.66806 to 14.62311, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.8269 - val_loss: 14.6231\n",
      "Epoch 44/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 15.0115\n",
      "Epoch 00044: val_loss improved from 14.62311 to 14.39938, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 15.0098 - val_loss: 14.3994\n",
      "Epoch 45/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 14.7305\n",
      "Epoch 00045: val_loss did not improve from 14.39938\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.7302 - val_loss: 16.9533\n",
      "Epoch 46/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 14.8465\n",
      "Epoch 00046: val_loss did not improve from 14.39938\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.8457 - val_loss: 15.9405\n",
      "Epoch 47/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 14.5999\n",
      "Epoch 00047: val_loss did not improve from 14.39938\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.5995 - val_loss: 15.8432\n",
      "Epoch 48/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 14.6576\n",
      "Epoch 00048: val_loss improved from 14.39938 to 13.32915, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.6565 - val_loss: 13.3292\n",
      "Epoch 49/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 14.3799\n",
      "Epoch 00049: val_loss did not improve from 13.32915\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.3799 - val_loss: 19.8625\n",
      "Epoch 50/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 14.3370\n",
      "Epoch 00050: val_loss did not improve from 13.32915\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.3401 - val_loss: 21.0464\n",
      "Epoch 51/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 14.2991\n",
      "Epoch 00051: val_loss did not improve from 13.32915\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.2982 - val_loss: 15.1501\n",
      "Epoch 52/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 14.3040 ETA: 0s\n",
      "Epoch 00052: val_loss did not improve from 13.32915\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.3049 - val_loss: 13.7722\n",
      "Epoch 53/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 14.0966\n",
      "Epoch 00053: val_loss did not improve from 13.32915\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.0984 - val_loss: 15.2924\n",
      "Epoch 54/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 14.0603\n",
      "Epoch 00054: val_loss did not improve from 13.32915\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.0597 - val_loss: 14.6353\n",
      "Epoch 55/200\n",
      "5022/5038 [============================>.] - ETA: 0s - loss: 14.0590\n",
      "Epoch 00055: val_loss did not improve from 13.32915\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.0576 - val_loss: 17.2336\n",
      "Epoch 56/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 13.9801\n",
      "Epoch 00056: val_loss did not improve from 13.32915\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.9804 - val_loss: 22.5566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 13.8356\n",
      "Epoch 00057: val_loss did not improve from 13.32915\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.8359 - val_loss: 15.0542\n",
      "Epoch 58/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 13.7746\n",
      "Epoch 00058: val_loss improved from 13.32915 to 13.19444, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.7732 - val_loss: 13.1944\n",
      "Epoch 59/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 13.7208\n",
      "Epoch 00059: val_loss did not improve from 13.19444\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.7204 - val_loss: 13.9162\n",
      "Epoch 60/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 13.6349\n",
      "Epoch 00060: val_loss did not improve from 13.19444\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.6329 - val_loss: 13.7937\n",
      "Epoch 61/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 13.7110\n",
      "Epoch 00061: val_loss did not improve from 13.19444\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.7087 - val_loss: 13.6738\n",
      "Epoch 62/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 13.6213\n",
      "Epoch 00062: val_loss did not improve from 13.19444\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.6158 - val_loss: 13.7215\n",
      "Epoch 63/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 13.5852\n",
      "Epoch 00063: val_loss did not improve from 13.19444\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.5846 - val_loss: 13.2675\n",
      "Epoch 64/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 13.4647\n",
      "Epoch 00064: val_loss did not improve from 13.19444\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.4646 - val_loss: 13.8065\n",
      "Epoch 65/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 13.3917\n",
      "Epoch 00065: val_loss did not improve from 13.19444\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.3944 - val_loss: 13.2729\n",
      "Epoch 66/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 13.4543\n",
      "Epoch 00066: val_loss did not improve from 13.19444\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.4520 - val_loss: 13.8390\n",
      "Epoch 67/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 13.2875\n",
      "Epoch 00067: val_loss improved from 13.19444 to 12.93765, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.2890 - val_loss: 12.9376\n",
      "Epoch 68/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 13.2289\n",
      "Epoch 00068: val_loss did not improve from 12.93765\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.2301 - val_loss: 14.3975\n",
      "Epoch 69/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 13.2771\n",
      "Epoch 00069: val_loss did not improve from 12.93765\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.2875 - val_loss: 18.8421\n",
      "Epoch 70/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 13.1275\n",
      "Epoch 00070: val_loss improved from 12.93765 to 12.90466, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.1250 - val_loss: 12.9047\n",
      "Epoch 71/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 13.0499\n",
      "Epoch 00071: val_loss did not improve from 12.90466\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 13.0504 - val_loss: 14.2111\n",
      "Epoch 72/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 13.1163\n",
      "Epoch 00072: val_loss did not improve from 12.90466\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 13.1153 - val_loss: 12.9670\n",
      "Epoch 73/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 12.9338\n",
      "Epoch 00073: val_loss did not improve from 12.90466\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 12.9361 - val_loss: 13.3505\n",
      "Epoch 74/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 12.9406\n",
      "Epoch 00074: val_loss did not improve from 12.90466\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 12.9433 - val_loss: 14.5758\n",
      "Epoch 75/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 12.9213\n",
      "Epoch 00075: val_loss did not improve from 12.90466\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 12.9224 - val_loss: 24.4027\n",
      "Epoch 76/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 12.8427\n",
      "Epoch 00076: val_loss improved from 12.90466 to 12.73866, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 12.8427 - val_loss: 12.7387\n",
      "Epoch 77/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 12.9823\n",
      "Epoch 00077: val_loss improved from 12.73866 to 12.69697, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 12.9823 - val_loss: 12.6970\n",
      "Epoch 78/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 12.6551\n",
      "Epoch 00078: val_loss did not improve from 12.69697\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.6570 - val_loss: 14.3261\n",
      "Epoch 79/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 12.6136\n",
      "Epoch 00079: val_loss improved from 12.69697 to 12.35557, saving model to DS02/experiment_set_8\\results_0.99\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.6165 - val_loss: 12.3556\n",
      "Epoch 80/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 12.7955\n",
      "Epoch 00080: val_loss did not improve from 12.35557\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.7955 - val_loss: 15.4080\n",
      "Epoch 81/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 12.6251\n",
      "Epoch 00081: val_loss did not improve from 12.35557\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.6249 - val_loss: 12.8545\n",
      "Epoch 82/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 12.6737\n",
      "Epoch 00082: val_loss did not improve from 12.35557\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.6747 - val_loss: 13.7778\n",
      "Epoch 83/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 12.6492\n",
      "Epoch 00083: val_loss did not improve from 12.35557\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.6482 - val_loss: 14.2896\n",
      "Epoch 84/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 12.5618\n",
      "Epoch 00084: val_loss did not improve from 12.35557\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.5614 - val_loss: 13.2726\n",
      "Epoch 85/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 12.5034\n",
      "Epoch 00085: val_loss did not improve from 12.35557\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.5032 - val_loss: 19.2654\n",
      "Epoch 86/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 12.3926\n",
      "Epoch 00086: val_loss did not improve from 12.35557\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.3923 - val_loss: 13.7371\n",
      "Epoch 87/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 12.4438\n",
      "Epoch 00087: val_loss did not improve from 12.35557\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.4471 - val_loss: 13.1475\n",
      "Epoch 88/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 12.4921\n",
      "Epoch 00088: val_loss did not improve from 12.35557\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.4922 - val_loss: 13.4551\n",
      "Epoch 89/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 12.2816\n",
      "Epoch 00089: val_loss did not improve from 12.35557\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 12.2829 - val_loss: 13.3246\n",
      "Epoch 00089: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_0.99\\split_1\\history.pkl\n",
      "Test set:\n",
      "MSE: 12.30\n",
      "RMSE: 3.51\n",
      "CMAPSS score: 1.24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 256)               4096      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 234,369\n",
      "Trainable params: 234,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 167.1330\n",
      "Epoch 00001: val_loss improved from inf to 55.35311, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 166.9847 - val_loss: 55.3531\n",
      "Epoch 2/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 44.2783\n",
      "Epoch 00002: val_loss improved from 55.35311 to 39.65090, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 44.2711 - val_loss: 39.6509\n",
      "Epoch 3/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 38.0398\n",
      "Epoch 00003: val_loss improved from 39.65090 to 33.39829, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 38.0335 - val_loss: 33.3983\n",
      "Epoch 4/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 34.3960\n",
      "Epoch 00004: val_loss did not improve from 33.39829\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 34.3941 - val_loss: 33.5462\n",
      "Epoch 5/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 32.2244\n",
      "Epoch 00005: val_loss did not improve from 33.39829\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 32.2239 - val_loss: 34.6076\n",
      "Epoch 6/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 30.3152\n",
      "Epoch 00006: val_loss improved from 33.39829 to 28.81951, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 30.3151 - val_loss: 28.8195\n",
      "Epoch 7/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 28.6706\n",
      "Epoch 00007: val_loss improved from 28.81951 to 28.07268, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 28.6703 - val_loss: 28.0727\n",
      "Epoch 8/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 27.1962\n",
      "Epoch 00008: val_loss improved from 28.07268 to 25.38146, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 27.1915 - val_loss: 25.3815\n",
      "Epoch 9/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 25.6419\n",
      "Epoch 00009: val_loss did not improve from 25.38146\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 25.6419 - val_loss: 26.8076\n",
      "Epoch 10/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 24.6824\n",
      "Epoch 00010: val_loss improved from 25.38146 to 22.59830, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 24.6821 - val_loss: 22.5983\n",
      "Epoch 11/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 23.7545\n",
      "Epoch 00011: val_loss improved from 22.59830 to 22.04983, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 23.7530 - val_loss: 22.0498\n",
      "Epoch 12/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 22.9632\n",
      "Epoch 00012: val_loss did not improve from 22.04983\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 22.9625 - val_loss: 24.0202\n",
      "Epoch 13/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 22.3419\n",
      "Epoch 00013: val_loss did not improve from 22.04983\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 22.3443 - val_loss: 22.9802\n",
      "Epoch 14/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 21.8158\n",
      "Epoch 00014: val_loss did not improve from 22.04983\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 21.8164 - val_loss: 24.2395\n",
      "Epoch 15/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 21.3178\n",
      "Epoch 00015: val_loss improved from 22.04983 to 20.46958, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 21.3153 - val_loss: 20.4696\n",
      "Epoch 16/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 20.7408\n",
      "Epoch 00016: val_loss improved from 20.46958 to 20.03529, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 20.7386 - val_loss: 20.0353\n",
      "Epoch 17/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 20.2601\n",
      "Epoch 00017: val_loss did not improve from 20.03529\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 20.2601 - val_loss: 22.8208\n",
      "Epoch 18/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 19.8407\n",
      "Epoch 00018: val_loss did not improve from 20.03529\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 19.8441 - val_loss: 22.6896\n",
      "Epoch 19/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 19.5502\n",
      "Epoch 00019: val_loss improved from 20.03529 to 18.51765, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 19.5501 - val_loss: 18.5177\n",
      "Epoch 20/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 19.1516\n",
      "Epoch 00020: val_loss did not improve from 18.51765\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 19.1542 - val_loss: 25.9449\n",
      "Epoch 21/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 18.8900\n",
      "Epoch 00021: val_loss did not improve from 18.51765\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 18.8883 - val_loss: 19.3475\n",
      "Epoch 22/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 18.6709\n",
      "Epoch 00022: val_loss improved from 18.51765 to 17.61205, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 18.6728 - val_loss: 17.6121\n",
      "Epoch 23/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 18.0857\n",
      "Epoch 00023: val_loss did not improve from 17.61205\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 18.0849 - val_loss: 20.5965\n",
      "Epoch 24/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 18.0995\n",
      "Epoch 00024: val_loss improved from 17.61205 to 16.76340, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 18.1024 - val_loss: 16.7634\n",
      "Epoch 25/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 17.8061\n",
      "Epoch 00025: val_loss did not improve from 16.76340\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 17.8081 - val_loss: 24.3464\n",
      "Epoch 26/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 17.6407\n",
      "Epoch 00026: val_loss did not improve from 16.76340\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 17.6399 - val_loss: 17.2958\n",
      "Epoch 27/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 17.4160\n",
      "Epoch 00027: val_loss did not improve from 16.76340\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 17.4183 - val_loss: 17.5039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 17.2659\n",
      "Epoch 00028: val_loss did not improve from 16.76340\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 17.2659 - val_loss: 25.4505\n",
      "Epoch 29/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 17.0255 ETA: 0s - loss: 17.\n",
      "Epoch 00029: val_loss did not improve from 16.76340\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 17.0266 - val_loss: 20.3897\n",
      "Epoch 30/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 16.7598\n",
      "Epoch 00030: val_loss did not improve from 16.76340\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 16.7590 - val_loss: 18.9056\n",
      "Epoch 31/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 16.5219\n",
      "Epoch 00031: val_loss did not improve from 16.76340\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 16.5298 - val_loss: 25.5712\n",
      "Epoch 32/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 16.4457\n",
      "Epoch 00032: val_loss improved from 16.76340 to 16.29480, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 16.4456 - val_loss: 16.2948\n",
      "Epoch 33/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 16.2819\n",
      "Epoch 00033: val_loss did not improve from 16.29480\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 16.2819 - val_loss: 17.3381\n",
      "Epoch 34/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 16.2183\n",
      "Epoch 00034: val_loss improved from 16.29480 to 15.23457, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 16.2143 - val_loss: 15.2346\n",
      "Epoch 35/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 16.0384\n",
      "Epoch 00035: val_loss did not improve from 15.23457\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 16.0383 - val_loss: 16.5324\n",
      "Epoch 36/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 15.8635\n",
      "Epoch 00036: val_loss did not improve from 15.23457\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 15.8643 - val_loss: 18.3357\n",
      "Epoch 37/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 15.8470\n",
      "Epoch 00037: val_loss improved from 15.23457 to 14.14539, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 15.8432 - val_loss: 14.1454\n",
      "Epoch 38/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 15.4210 ETA: 0s - l\n",
      "Epoch 00038: val_loss did not improve from 14.14539\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 15.4261 - val_loss: 17.7831\n",
      "Epoch 39/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 15.6008\n",
      "Epoch 00039: val_loss did not improve from 14.14539\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 15.5959 - val_loss: 20.1180\n",
      "Epoch 40/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 15.3906\n",
      "Epoch 00040: val_loss did not improve from 14.14539\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 15.3913 - val_loss: 15.1703\n",
      "Epoch 41/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 15.2187\n",
      "Epoch 00041: val_loss did not improve from 14.14539\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 15.2188 - val_loss: 18.2205\n",
      "Epoch 42/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 15.1499\n",
      "Epoch 00042: val_loss did not improve from 14.14539\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 15.1506 - val_loss: 16.2523\n",
      "Epoch 43/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 15.0861\n",
      "Epoch 00043: val_loss did not improve from 14.14539\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 15.0863 - val_loss: 15.2448\n",
      "Epoch 44/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 14.9604\n",
      "Epoch 00044: val_loss did not improve from 14.14539\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.9600 - val_loss: 17.9858\n",
      "Epoch 45/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 14.8289\n",
      "Epoch 00045: val_loss did not improve from 14.14539\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.8291 - val_loss: 17.3224\n",
      "Epoch 46/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 14.8692\n",
      "Epoch 00046: val_loss did not improve from 14.14539\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.8675 - val_loss: 14.5025\n",
      "Epoch 47/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 14.7276\n",
      "Epoch 00047: val_loss improved from 14.14539 to 13.97054, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.7269 - val_loss: 13.9705\n",
      "Epoch 48/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 14.7382\n",
      "Epoch 00048: val_loss did not improve from 13.97054\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.7399 - val_loss: 15.1301\n",
      "Epoch 49/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 14.6127\n",
      "Epoch 00049: val_loss did not improve from 13.97054\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 14.6138 - val_loss: 19.5101\n",
      "Epoch 50/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 14.5396\n",
      "Epoch 00050: val_loss improved from 13.97054 to 13.42949, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.5383 - val_loss: 13.4295\n",
      "Epoch 51/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 14.4399\n",
      "Epoch 00051: val_loss improved from 13.42949 to 13.19421, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 14.4397 - val_loss: 13.1942\n",
      "Epoch 52/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 14.3846\n",
      "Epoch 00052: val_loss did not improve from 13.19421\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.3846 - val_loss: 15.6984\n",
      "Epoch 53/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 14.3336\n",
      "Epoch 00053: val_loss did not improve from 13.19421\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.3336 - val_loss: 16.2072\n",
      "Epoch 54/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 14.1921\n",
      "Epoch 00054: val_loss did not improve from 13.19421\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.1934 - val_loss: 16.3459\n",
      "Epoch 55/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 14.2396\n",
      "Epoch 00055: val_loss did not improve from 13.19421\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.2415 - val_loss: 18.1817\n",
      "Epoch 56/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 14.1176\n",
      "Epoch 00056: val_loss did not improve from 13.19421\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 14.1176 - val_loss: 19.8337\n",
      "Epoch 57/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 13.9892\n",
      "Epoch 00057: val_loss did not improve from 13.19421\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.9890 - val_loss: 13.4306\n",
      "Epoch 58/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 13.9838\n",
      "Epoch 00058: val_loss improved from 13.19421 to 12.63406, saving model to DS02/experiment_set_8\\results_0.99\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.9818 - val_loss: 12.6341\n",
      "Epoch 59/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 13.9030\n",
      "Epoch 00059: val_loss did not improve from 12.63406\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.9016 - val_loss: 14.7225\n",
      "Epoch 60/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 13.7745\n",
      "Epoch 00060: val_loss did not improve from 12.63406\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.7747 - val_loss: 15.7390\n",
      "Epoch 61/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 13.6506\n",
      "Epoch 00061: val_loss did not improve from 12.63406\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.6568 - val_loss: 18.9091\n",
      "Epoch 62/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 13.8732\n",
      "Epoch 00062: val_loss did not improve from 12.63406\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.8698 - val_loss: 14.9611\n",
      "Epoch 63/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 13.5008\n",
      "Epoch 00063: val_loss did not improve from 12.63406\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.5001 - val_loss: 13.6302\n",
      "Epoch 64/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 13.5543\n",
      "Epoch 00064: val_loss did not improve from 12.63406\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.5553 - val_loss: 14.8563\n",
      "Epoch 65/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 13.4292\n",
      "Epoch 00065: val_loss did not improve from 12.63406\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.4283 - val_loss: 14.6811\n",
      "Epoch 66/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 13.4683\n",
      "Epoch 00066: val_loss did not improve from 12.63406\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.4702 - val_loss: 15.7629\n",
      "Epoch 67/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 13.3690\n",
      "Epoch 00067: val_loss did not improve from 12.63406\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.3696 - val_loss: 15.1225\n",
      "Epoch 68/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 13.3982\n",
      "Epoch 00068: val_loss did not improve from 12.63406\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 13.3981 - val_loss: 13.8120\n",
      "Epoch 00068: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_0.99\\split_2\\history.pkl\n",
      "Test set:\n",
      "MSE: 12.62\n",
      "RMSE: 3.55\n",
      "CMAPSS score: 1.26\n",
      "\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 256)               2048      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 232,321\n",
      "Trainable params: 232,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 222.7063\n",
      "Epoch 00001: val_loss improved from inf to 92.84197, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 222.5279 - val_loss: 92.8420\n",
      "Epoch 2/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 72.7503\n",
      "Epoch 00002: val_loss improved from 92.84197 to 71.33571, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 72.7490 - val_loss: 71.3357\n",
      "Epoch 3/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 65.0939\n",
      "Epoch 00003: val_loss improved from 71.33571 to 60.44155, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 65.0894 - val_loss: 60.4415\n",
      "Epoch 4/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 60.6308\n",
      "Epoch 00004: val_loss improved from 60.44155 to 56.86023, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 60.6297 - val_loss: 56.8602\n",
      "Epoch 5/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 57.2440\n",
      "Epoch 00005: val_loss improved from 56.86023 to 53.85823, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 57.2404 - val_loss: 53.8582\n",
      "Epoch 6/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 55.3436\n",
      "Epoch 00006: val_loss did not improve from 53.85823\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 55.3381 - val_loss: 58.7682\n",
      "Epoch 7/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 53.2678\n",
      "Epoch 00007: val_loss did not improve from 53.85823\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 53.2676 - val_loss: 60.3619\n",
      "Epoch 8/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 51.8327\n",
      "Epoch 00008: val_loss improved from 53.85823 to 49.33037, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 51.8254 - val_loss: 49.3304\n",
      "Epoch 9/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 50.5699\n",
      "Epoch 00009: val_loss did not improve from 49.33037\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 50.5689 - val_loss: 50.8266\n",
      "Epoch 10/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 49.2313\n",
      "Epoch 00010: val_loss did not improve from 49.33037\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 49.2312 - val_loss: 56.5945\n",
      "Epoch 11/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 48.6571\n",
      "Epoch 00011: val_loss improved from 49.33037 to 48.56122, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 48.6571 - val_loss: 48.5612\n",
      "Epoch 12/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 47.6035\n",
      "Epoch 00012: val_loss improved from 48.56122 to 45.80824, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 47.6016 - val_loss: 45.8082\n",
      "Epoch 13/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 46.7058\n",
      "Epoch 00013: val_loss did not improve from 45.80824\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 46.7060 - val_loss: 47.3974\n",
      "Epoch 14/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 45.9815\n",
      "Epoch 00014: val_loss did not improve from 45.80824\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 45.9833 - val_loss: 50.6833\n",
      "Epoch 15/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 45.2332\n",
      "Epoch 00015: val_loss improved from 45.80824 to 42.78847, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 45.2271 - val_loss: 42.7885\n",
      "Epoch 16/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 44.6255\n",
      "Epoch 00016: val_loss improved from 42.78847 to 41.49381, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 44.6163 - val_loss: 41.4938\n",
      "Epoch 17/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 43.7586\n",
      "Epoch 00017: val_loss did not improve from 41.49381\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 43.7561 - val_loss: 48.4703\n",
      "Epoch 18/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 43.3845\n",
      "Epoch 00018: val_loss did not improve from 41.49381\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 43.3843 - val_loss: 44.4066\n",
      "Epoch 19/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 43.0133\n",
      "Epoch 00019: val_loss improved from 41.49381 to 41.48912, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 43.0103 - val_loss: 41.4891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 42.2777\n",
      "Epoch 00020: val_loss improved from 41.48912 to 39.02573, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 42.2773 - val_loss: 39.0257\n",
      "Epoch 21/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 41.8000\n",
      "Epoch 00021: val_loss did not improve from 39.02573\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 41.7957 - val_loss: 44.0034\n",
      "Epoch 22/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 41.2121\n",
      "Epoch 00022: val_loss did not improve from 39.02573\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 41.2152 - val_loss: 46.1223\n",
      "Epoch 23/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 41.0008\n",
      "Epoch 00023: val_loss did not improve from 39.02573\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 41.0015 - val_loss: 40.3893\n",
      "Epoch 24/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 40.4967\n",
      "Epoch 00024: val_loss did not improve from 39.02573\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 40.4962 - val_loss: 45.6195\n",
      "Epoch 25/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 40.0914\n",
      "Epoch 00025: val_loss did not improve from 39.02573\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 40.0999 - val_loss: 44.3867\n",
      "Epoch 26/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 39.9362\n",
      "Epoch 00026: val_loss did not improve from 39.02573\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.9388 - val_loss: 44.3568\n",
      "Epoch 27/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 39.3878\n",
      "Epoch 00027: val_loss improved from 39.02573 to 37.95648, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 39.3849 - val_loss: 37.9565\n",
      "Epoch 28/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 38.8859\n",
      "Epoch 00028: val_loss did not improve from 37.95648\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 38.8845 - val_loss: 41.6018\n",
      "Epoch 29/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 38.6575\n",
      "Epoch 00029: val_loss improved from 37.95648 to 37.05218, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 38.6540 - val_loss: 37.0522\n",
      "Epoch 30/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 38.3741\n",
      "Epoch 00030: val_loss did not improve from 37.05218\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 38.3743 - val_loss: 40.5573\n",
      "Epoch 31/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 38.0532\n",
      "Epoch 00031: val_loss improved from 37.05218 to 35.67329, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.0577 - val_loss: 35.6733\n",
      "Epoch 32/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 37.8907\n",
      "Epoch 00032: val_loss did not improve from 35.67329\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.8834 - val_loss: 36.9924\n",
      "Epoch 33/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 37.4948\n",
      "Epoch 00033: val_loss did not improve from 35.67329\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 37.4936 - val_loss: 37.7056\n",
      "Epoch 34/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 37.1866\n",
      "Epoch 00034: val_loss did not improve from 35.67329\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.1858 - val_loss: 36.3188\n",
      "Epoch 35/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 36.9733\n",
      "Epoch 00035: val_loss did not improve from 35.67329\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.9713 - val_loss: 37.0241\n",
      "Epoch 36/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 36.3214\n",
      "Epoch 00036: val_loss improved from 35.67329 to 35.33146, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.3160 - val_loss: 35.3315\n",
      "Epoch 37/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 36.2420\n",
      "Epoch 00037: val_loss did not improve from 35.33146\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.2423 - val_loss: 35.8396\n",
      "Epoch 38/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 36.2663\n",
      "Epoch 00038: val_loss did not improve from 35.33146\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.2655 - val_loss: 36.4459\n",
      "Epoch 39/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 36.1234\n",
      "Epoch 00039: val_loss did not improve from 35.33146\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.1233 - val_loss: 39.5882\n",
      "Epoch 40/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 35.6951\n",
      "Epoch 00040: val_loss did not improve from 35.33146\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.6906 - val_loss: 37.9409\n",
      "Epoch 41/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 35.5326\n",
      "Epoch 00041: val_loss did not improve from 35.33146\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.5304 - val_loss: 36.5160\n",
      "Epoch 42/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 35.3084\n",
      "Epoch 00042: val_loss did not improve from 35.33146\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.3108 - val_loss: 37.2358\n",
      "Epoch 43/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 35.1324\n",
      "Epoch 00043: val_loss improved from 35.33146 to 33.69600, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.1413 - val_loss: 33.6960\n",
      "Epoch 44/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 35.0674\n",
      "Epoch 00044: val_loss did not improve from 33.69600\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.0692 - val_loss: 41.8343\n",
      "Epoch 45/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 34.8694\n",
      "Epoch 00045: val_loss did not improve from 33.69600\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.8694 - val_loss: 35.3958\n",
      "Epoch 46/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 34.6080\n",
      "Epoch 00046: val_loss improved from 33.69600 to 33.56783, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.6056 - val_loss: 33.5678\n",
      "Epoch 47/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 34.6594\n",
      "Epoch 00047: val_loss improved from 33.56783 to 33.27193, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.6555 - val_loss: 33.2719\n",
      "Epoch 48/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 34.3613\n",
      "Epoch 00048: val_loss improved from 33.27193 to 32.81226, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.3608 - val_loss: 32.8123\n",
      "Epoch 49/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 34.0216\n",
      "Epoch 00049: val_loss did not improve from 32.81226\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.0254 - val_loss: 37.3264\n",
      "Epoch 50/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 33.9232\n",
      "Epoch 00050: val_loss improved from 32.81226 to 32.23044, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 33.9222 - val_loss: 32.2304\n",
      "Epoch 51/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 33.7754\n",
      "Epoch 00051: val_loss did not improve from 32.23044\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.7848 - val_loss: 36.8437\n",
      "Epoch 52/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 33.8614\n",
      "Epoch 00052: val_loss improved from 32.23044 to 31.98083, saving model to DS02/experiment_set_8\\results_0.95\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 33.8618 - val_loss: 31.9808\n",
      "Epoch 53/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 33.7142\n",
      "Epoch 00053: val_loss did not improve from 31.98083\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.7141 - val_loss: 33.3887\n",
      "Epoch 54/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 33.5039\n",
      "Epoch 00054: val_loss did not improve from 31.98083\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.5018 - val_loss: 34.0228\n",
      "Epoch 55/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 33.4945\n",
      "Epoch 00055: val_loss did not improve from 31.98083\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.4951 - val_loss: 33.0050\n",
      "Epoch 56/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 33.2317\n",
      "Epoch 00056: val_loss did not improve from 31.98083\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.2350 - val_loss: 38.1085\n",
      "Epoch 57/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 33.2092\n",
      "Epoch 00057: val_loss did not improve from 31.98083\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 33.2092 - val_loss: 34.5873\n",
      "Epoch 58/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 33.0139 ETA: \n",
      "Epoch 00058: val_loss did not improve from 31.98083\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.0067 - val_loss: 33.3038\n",
      "Epoch 59/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 32.9102\n",
      "Epoch 00059: val_loss did not improve from 31.98083\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.9121 - val_loss: 39.1786\n",
      "Epoch 60/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 32.8419\n",
      "Epoch 00060: val_loss did not improve from 31.98083\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.8435 - val_loss: 35.5847\n",
      "Epoch 61/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 32.6743\n",
      "Epoch 00061: val_loss did not improve from 31.98083\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.6700 - val_loss: 32.7917\n",
      "Epoch 62/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 32.7951\n",
      "Epoch 00062: val_loss did not improve from 31.98083\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.7949 - val_loss: 34.2921\n",
      "Epoch 00062: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_0.95\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 31.93\n",
      "RMSE: 5.65\n",
      "CMAPSS score: 1.58\n",
      "\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 256)               2048      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 232,321\n",
      "Trainable params: 232,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 221.6044- ETA: 0s - loss: 223\n",
      "Epoch 00001: val_loss improved from inf to 74.79413, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 221.5066 - val_loss: 74.7941\n",
      "Epoch 2/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 72.9972\n",
      "Epoch 00002: val_loss improved from 74.79413 to 63.86757, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 72.9964 - val_loss: 63.8676\n",
      "Epoch 3/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 65.2946\n",
      "Epoch 00003: val_loss did not improve from 63.86757\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 65.2958 - val_loss: 67.7209\n",
      "Epoch 4/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 61.0976\n",
      "Epoch 00004: val_loss improved from 63.86757 to 58.91352, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 61.0943 - val_loss: 58.9135\n",
      "Epoch 5/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 57.4223\n",
      "Epoch 00005: val_loss improved from 58.91352 to 55.62556, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 57.4210 - val_loss: 55.6256\n",
      "Epoch 6/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 55.1693\n",
      "Epoch 00006: val_loss did not improve from 55.62556\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 55.1708 - val_loss: 58.0961\n",
      "Epoch 7/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 53.3945\n",
      "Epoch 00007: val_loss did not improve from 55.62556\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 53.3921 - val_loss: 57.9881\n",
      "Epoch 8/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 51.8487\n",
      "Epoch 00008: val_loss improved from 55.62556 to 47.64456, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 51.8398 - val_loss: 47.6446\n",
      "Epoch 9/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 50.5810\n",
      "Epoch 00009: val_loss did not improve from 47.64456\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 50.5804 - val_loss: 49.0919\n",
      "Epoch 10/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 49.5234\n",
      "Epoch 00010: val_loss did not improve from 47.64456\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 49.5319 - val_loss: 55.1037\n",
      "Epoch 11/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 48.6325\n",
      "Epoch 00011: val_loss did not improve from 47.64456\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 48.6376 - val_loss: 51.9716\n",
      "Epoch 12/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 47.2357\n",
      "Epoch 00012: val_loss did not improve from 47.64456\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 47.2361 - val_loss: 47.7226\n",
      "Epoch 13/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 46.5056\n",
      "Epoch 00013: val_loss did not improve from 47.64456\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 46.5033 - val_loss: 50.4131\n",
      "Epoch 14/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 45.7087\n",
      "Epoch 00014: val_loss did not improve from 47.64456\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 45.7093 - val_loss: 54.3775\n",
      "Epoch 15/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 44.9281\n",
      "Epoch 00015: val_loss improved from 47.64456 to 43.79659, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 44.9267 - val_loss: 43.7966\n",
      "Epoch 16/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 44.3209\n",
      "Epoch 00016: val_loss improved from 43.79659 to 42.24129, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 44.3203 - val_loss: 42.2413\n",
      "Epoch 17/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 43.5808\n",
      "Epoch 00017: val_loss did not improve from 42.24129\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 43.5821 - val_loss: 48.7021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 43.0336\n",
      "Epoch 00018: val_loss did not improve from 42.24129\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 43.0370 - val_loss: 42.4249\n",
      "Epoch 19/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 42.6839\n",
      "Epoch 00019: val_loss improved from 42.24129 to 41.13953, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 42.6887 - val_loss: 41.1395\n",
      "Epoch 20/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 42.2238\n",
      "Epoch 00020: val_loss did not improve from 41.13953\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 42.2257 - val_loss: 46.3388\n",
      "Epoch 21/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 41.5451\n",
      "Epoch 00021: val_loss improved from 41.13953 to 40.45379, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 41.5421 - val_loss: 40.4538\n",
      "Epoch 22/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 41.1035\n",
      "Epoch 00022: val_loss did not improve from 40.45379\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 41.1086 - val_loss: 46.9563\n",
      "Epoch 23/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 40.7546\n",
      "Epoch 00023: val_loss did not improve from 40.45379\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 40.7564 - val_loss: 40.6936\n",
      "Epoch 24/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 40.2201\n",
      "Epoch 00024: val_loss did not improve from 40.45379\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 40.2254 - val_loss: 45.5940\n",
      "Epoch 25/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 39.8293\n",
      "Epoch 00025: val_loss did not improve from 40.45379\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.8293 - val_loss: 41.0577\n",
      "Epoch 26/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 39.5101\n",
      "Epoch 00026: val_loss improved from 40.45379 to 37.83981, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.5075 - val_loss: 37.8398\n",
      "Epoch 27/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 39.0038\n",
      "Epoch 00027: val_loss did not improve from 37.83981\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.0026 - val_loss: 37.9699\n",
      "Epoch 28/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 38.7271\n",
      "Epoch 00028: val_loss improved from 37.83981 to 36.16381, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.7276 - val_loss: 36.1638\n",
      "Epoch 29/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 38.3800\n",
      "Epoch 00029: val_loss did not improve from 36.16381\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.3800 - val_loss: 38.9656\n",
      "Epoch 30/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 38.1077\n",
      "Epoch 00030: val_loss did not improve from 36.16381\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.1109 - val_loss: 39.9025\n",
      "Epoch 31/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 37.8044\n",
      "Epoch 00031: val_loss improved from 36.16381 to 35.05131, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.8112 - val_loss: 35.0513\n",
      "Epoch 32/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 37.4935\n",
      "Epoch 00032: val_loss did not improve from 35.05131\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.4926 - val_loss: 38.8978\n",
      "Epoch 33/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 37.3426\n",
      "Epoch 00033: val_loss did not improve from 35.05131\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.3465 - val_loss: 38.3651\n",
      "Epoch 34/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 36.9219\n",
      "Epoch 00034: val_loss did not improve from 35.05131\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.9191 - val_loss: 35.5920\n",
      "Epoch 35/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 36.5941\n",
      "Epoch 00035: val_loss did not improve from 35.05131\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.5881 - val_loss: 36.2215\n",
      "Epoch 36/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 36.3785\n",
      "Epoch 00036: val_loss improved from 35.05131 to 34.92405, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.3726 - val_loss: 34.9240\n",
      "Epoch 37/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 36.2524\n",
      "Epoch 00037: val_loss did not improve from 34.92405\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.2538 - val_loss: 35.4264\n",
      "Epoch 38/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 36.1104\n",
      "Epoch 00038: val_loss did not improve from 34.92405\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.1152 - val_loss: 37.7736\n",
      "Epoch 39/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 35.8648\n",
      "Epoch 00039: val_loss did not improve from 34.92405\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.8616 - val_loss: 39.6949\n",
      "Epoch 40/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 35.6307\n",
      "Epoch 00040: val_loss did not improve from 34.92405\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.6307 - val_loss: 35.6471\n",
      "Epoch 41/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 35.5833\n",
      "Epoch 00041: val_loss did not improve from 34.92405\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.5865 - val_loss: 37.9616\n",
      "Epoch 42/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 35.1084\n",
      "Epoch 00042: val_loss did not improve from 34.92405\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.1112 - val_loss: 36.4272\n",
      "Epoch 43/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 34.9070\n",
      "Epoch 00043: val_loss did not improve from 34.92405\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 34.9074 - val_loss: 36.2054\n",
      "Epoch 44/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 34.9828\n",
      "Epoch 00044: val_loss did not improve from 34.92405\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.9848 - val_loss: 38.4149\n",
      "Epoch 45/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 34.4916\n",
      "Epoch 00045: val_loss did not improve from 34.92405\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.4919 - val_loss: 41.2111\n",
      "Epoch 46/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 34.4670\n",
      "Epoch 00046: val_loss improved from 34.92405 to 34.03861, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.4622 - val_loss: 34.0386\n",
      "Epoch 47/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 34.2951\n",
      "Epoch 00047: val_loss did not improve from 34.03861\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.2951 - val_loss: 36.4023\n",
      "Epoch 48/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 34.2719\n",
      "Epoch 00048: val_loss did not improve from 34.03861\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 34.2730 - val_loss: 40.2166\n",
      "Epoch 49/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 33.9776\n",
      "Epoch 00049: val_loss did not improve from 34.03861\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 33.9798 - val_loss: 34.8767\n",
      "Epoch 50/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 33.8299\n",
      "Epoch 00050: val_loss improved from 34.03861 to 32.64883, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.8296 - val_loss: 32.6488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 33.8433\n",
      "Epoch 00051: val_loss did not improve from 32.64883\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 33.8442 - val_loss: 35.1370\n",
      "Epoch 52/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 33.5762 ETA: 0s - loss: 33.58\n",
      "Epoch 00052: val_loss improved from 32.64883 to 31.28840, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.5777 - val_loss: 31.2884\n",
      "Epoch 53/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 33.4071 ETA: 0s - loss: 33.\n",
      "Epoch 00053: val_loss improved from 31.28840 to 31.19529, saving model to DS02/experiment_set_8\\results_0.95\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 33.4039 - val_loss: 31.1953\n",
      "Epoch 54/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 33.1340\n",
      "Epoch 00054: val_loss did not improve from 31.19529\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.1350 - val_loss: 33.2500\n",
      "Epoch 55/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 32.9985\n",
      "Epoch 00055: val_loss did not improve from 31.19529\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.0036 - val_loss: 35.1553\n",
      "Epoch 56/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 33.1463\n",
      "Epoch 00056: val_loss did not improve from 31.19529\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.1486 - val_loss: 39.4640\n",
      "Epoch 57/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 32.9318\n",
      "Epoch 00057: val_loss did not improve from 31.19529\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.9296 - val_loss: 35.1601\n",
      "Epoch 58/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 32.7231\n",
      "Epoch 00058: val_loss did not improve from 31.19529\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.7184 - val_loss: 32.4125\n",
      "Epoch 59/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 32.7893\n",
      "Epoch 00059: val_loss did not improve from 31.19529\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 32.7861 - val_loss: 34.2384\n",
      "Epoch 60/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 32.4571\n",
      "Epoch 00060: val_loss did not improve from 31.19529\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.4526 - val_loss: 33.0062\n",
      "Epoch 61/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 32.3921\n",
      "Epoch 00061: val_loss did not improve from 31.19529\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.3909 - val_loss: 33.8389\n",
      "Epoch 62/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 32.3314\n",
      "Epoch 00062: val_loss did not improve from 31.19529\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.3325 - val_loss: 35.8263\n",
      "Epoch 63/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 32.0295\n",
      "Epoch 00063: val_loss did not improve from 31.19529\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 32.0295 - val_loss: 36.6635\n",
      "Epoch 00063: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_0.95\\split_1\\history.pkl\n",
      "Test set:\n",
      "MSE: 31.09\n",
      "RMSE: 5.58\n",
      "CMAPSS score: 1.71\n",
      "\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 256)               2048      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 232,321\n",
      "Trainable params: 232,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 221.9168\n",
      "Epoch 00001: val_loss improved from inf to 79.25468, saving model to DS02/experiment_set_8\\results_0.95\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 221.8179 - val_loss: 79.2547\n",
      "Epoch 2/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 73.4068\n",
      "Epoch 00002: val_loss did not improve from 79.25468\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 73.4001 - val_loss: 100.0562\n",
      "Epoch 3/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 66.1725\n",
      "Epoch 00003: val_loss improved from 79.25468 to 68.87688, saving model to DS02/experiment_set_8\\results_0.95\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 66.1716 - val_loss: 68.8769\n",
      "Epoch 4/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 60.9966\n",
      "Epoch 00004: val_loss did not improve from 68.87688\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 60.9955 - val_loss: 74.5041\n",
      "Epoch 5/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 58.0681\n",
      "Epoch 00005: val_loss improved from 68.87688 to 59.03612, saving model to DS02/experiment_set_8\\results_0.95\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 58.0618 - val_loss: 59.0361\n",
      "Epoch 6/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 55.6504\n",
      "Epoch 00006: val_loss did not improve from 59.03612\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 55.6511 - val_loss: 61.6099\n",
      "Epoch 7/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 53.7133\n",
      "Epoch 00007: val_loss improved from 59.03612 to 51.24239, saving model to DS02/experiment_set_8\\results_0.95\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 53.7111 - val_loss: 51.2424\n",
      "Epoch 8/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 52.0173\n",
      "Epoch 00008: val_loss improved from 51.24239 to 50.38696, saving model to DS02/experiment_set_8\\results_0.95\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 52.0239 - val_loss: 50.3870\n",
      "Epoch 9/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 50.5910\n",
      "Epoch 00009: val_loss did not improve from 50.38696\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 50.5919 - val_loss: 68.6794\n",
      "Epoch 10/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 49.5788 ETA: 0s - loss:\n",
      "Epoch 00010: val_loss did not improve from 50.38696\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 49.5819 - val_loss: 59.5312\n",
      "Epoch 11/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 48.4375\n",
      "Epoch 00011: val_loss did not improve from 50.38696\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 48.4371 - val_loss: 59.4282\n",
      "Epoch 12/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 47.4590\n",
      "Epoch 00012: val_loss improved from 50.38696 to 44.82315, saving model to DS02/experiment_set_8\\results_0.95\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 47.4487 - val_loss: 44.8232\n",
      "Epoch 13/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 46.4728\n",
      "Epoch 00013: val_loss did not improve from 44.82315\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 46.4681 - val_loss: 47.5051\n",
      "Epoch 14/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 46.0817\n",
      "Epoch 00014: val_loss did not improve from 44.82315\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 46.0817 - val_loss: 46.3670\n",
      "Epoch 15/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 45.1807\n",
      "Epoch 00015: val_loss improved from 44.82315 to 42.19038, saving model to DS02/experiment_set_8\\results_0.95\\split_2\\mlp_model_trained.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5038/5038 [==============================] - 24s 5ms/step - loss: 45.1770 - val_loss: 42.1904\n",
      "Epoch 16/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 44.5361\n",
      "Epoch 00016: val_loss improved from 42.19038 to 41.13252, saving model to DS02/experiment_set_8\\results_0.95\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 44.5327 - val_loss: 41.1325\n",
      "Epoch 17/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 43.9807\n",
      "Epoch 00017: val_loss did not improve from 41.13252\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 43.9825 - val_loss: 53.5692\n",
      "Epoch 18/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 43.2802\n",
      "Epoch 00018: val_loss did not improve from 41.13252\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 43.2832 - val_loss: 52.0206\n",
      "Epoch 19/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 42.7298\n",
      "Epoch 00019: val_loss did not improve from 41.13252\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 42.7367 - val_loss: 43.0277\n",
      "Epoch 20/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 42.1005\n",
      "Epoch 00020: val_loss did not improve from 41.13252\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 42.1088 - val_loss: 42.1804\n",
      "Epoch 21/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 41.5531\n",
      "Epoch 00021: val_loss improved from 41.13252 to 38.67521, saving model to DS02/experiment_set_8\\results_0.95\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 41.5488 - val_loss: 38.6752\n",
      "Epoch 22/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 41.1788\n",
      "Epoch 00022: val_loss did not improve from 38.67521\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 41.1798 - val_loss: 43.4518\n",
      "Epoch 23/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 40.8431\n",
      "Epoch 00023: val_loss did not improve from 38.67521\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 40.8458 - val_loss: 42.1884\n",
      "Epoch 24/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 40.0146\n",
      "Epoch 00024: val_loss did not improve from 38.67521\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 40.0155 - val_loss: 44.6187\n",
      "Epoch 25/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 40.1252\n",
      "Epoch 00025: val_loss did not improve from 38.67521\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 40.1237 - val_loss: 39.1283\n",
      "Epoch 26/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 39.8809\n",
      "Epoch 00026: val_loss improved from 38.67521 to 38.33640, saving model to DS02/experiment_set_8\\results_0.95\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.8805 - val_loss: 38.3364\n",
      "Epoch 27/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 39.2205\n",
      "Epoch 00027: val_loss improved from 38.33640 to 37.12959, saving model to DS02/experiment_set_8\\results_0.95\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 39.2160 - val_loss: 37.1296\n",
      "Epoch 28/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 39.2430\n",
      "Epoch 00028: val_loss improved from 37.12959 to 36.25643, saving model to DS02/experiment_set_8\\results_0.95\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 39.2374 - val_loss: 36.2564\n",
      "Epoch 29/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 38.7205\n",
      "Epoch 00029: val_loss did not improve from 36.25643\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.7257 - val_loss: 40.5528\n",
      "Epoch 30/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 38.3064\n",
      "Epoch 00030: val_loss did not improve from 36.25643\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.3071 - val_loss: 39.6038\n",
      "Epoch 31/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 37.9889\n",
      "Epoch 00031: val_loss did not improve from 36.25643\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.9923 - val_loss: 37.4835\n",
      "Epoch 32/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 37.9712\n",
      "Epoch 00032: val_loss did not improve from 36.25643\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.9693 - val_loss: 39.9489\n",
      "Epoch 33/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 37.8131\n",
      "Epoch 00033: val_loss did not improve from 36.25643\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.8134 - val_loss: 38.3004\n",
      "Epoch 34/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 37.4464\n",
      "Epoch 00034: val_loss did not improve from 36.25643\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.4437 - val_loss: 37.4936\n",
      "Epoch 35/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 37.0111\n",
      "Epoch 00035: val_loss did not improve from 36.25643\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.0075 - val_loss: 37.4502\n",
      "Epoch 36/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 36.6121\n",
      "Epoch 00036: val_loss did not improve from 36.25643\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.6121 - val_loss: 37.2581\n",
      "Epoch 37/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 36.6089\n",
      "Epoch 00037: val_loss did not improve from 36.25643\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.6100 - val_loss: 42.7276\n",
      "Epoch 38/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 36.3604\n",
      "Epoch 00038: val_loss did not improve from 36.25643\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.3601 - val_loss: 36.4301\n",
      "Epoch 00038: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_0.95\\split_2\\history.pkl\n",
      "Test set:\n",
      "MSE: 36.23\n",
      "RMSE: 6.02\n",
      "CMAPSS score: 1.89\n",
      "\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 256)               1536      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 231,809\n",
      "Trainable params: 231,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 217.6601\n",
      "Epoch 00001: val_loss improved from inf to 81.80391, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 217.3869 - val_loss: 81.8039\n",
      "Epoch 2/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 80.0984\n",
      "Epoch 00002: val_loss improved from 81.80391 to 79.81971, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 80.0830 - val_loss: 79.8197\n",
      "Epoch 3/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 73.5744\n",
      "Epoch 00003: val_loss improved from 79.81971 to 65.45998, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 73.5632 - val_loss: 65.4600\n",
      "Epoch 4/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 68.5294\n",
      "Epoch 00004: val_loss did not improve from 65.45998\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 68.5375 - val_loss: 76.1603\n",
      "Epoch 5/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 64.9725\n",
      "Epoch 00005: val_loss did not improve from 65.45998\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 64.9664 - val_loss: 67.5480\n",
      "Epoch 6/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 62.3278\n",
      "Epoch 00006: val_loss improved from 65.45998 to 64.92421, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 62.3209 - val_loss: 64.9242\n",
      "Epoch 7/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 60.1966\n",
      "Epoch 00007: val_loss improved from 64.92421 to 64.10509, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 60.1930 - val_loss: 64.1051\n",
      "Epoch 8/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 58.2397\n",
      "Epoch 00008: val_loss improved from 64.10509 to 55.81192, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 58.2336 - val_loss: 55.8119\n",
      "Epoch 9/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 56.5748\n",
      "Epoch 00009: val_loss improved from 55.81192 to 54.89460, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 56.5739 - val_loss: 54.8946\n",
      "Epoch 10/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 55.2501\n",
      "Epoch 00010: val_loss did not improve from 54.89460\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 55.2594 - val_loss: 71.6701\n",
      "Epoch 11/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 54.1969\n",
      "Epoch 00011: val_loss improved from 54.89460 to 53.24504, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 54.1959 - val_loss: 53.2450\n",
      "Epoch 12/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 53.3166\n",
      "Epoch 00012: val_loss did not improve from 53.24504\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 53.3158 - val_loss: 55.0200\n",
      "Epoch 13/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 52.4001\n",
      "Epoch 00013: val_loss improved from 53.24504 to 51.16877, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 52.3986 - val_loss: 51.1688\n",
      "Epoch 14/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 51.3629\n",
      "Epoch 00014: val_loss improved from 51.16877 to 51.08223, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 51.3606 - val_loss: 51.0822\n",
      "Epoch 15/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 50.6114\n",
      "Epoch 00015: val_loss improved from 51.08223 to 49.47017, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 50.6114 - val_loss: 49.4702\n",
      "Epoch 16/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 49.9435\n",
      "Epoch 00016: val_loss improved from 49.47017 to 46.29911, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 49.9353 - val_loss: 46.2991\n",
      "Epoch 17/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 49.3328\n",
      "Epoch 00017: val_loss did not improve from 46.29911\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 49.3307 - val_loss: 60.1487\n",
      "Epoch 18/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 48.8018\n",
      "Epoch 00018: val_loss did not improve from 46.29911\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 48.8038 - val_loss: 47.9296\n",
      "Epoch 19/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 48.2460\n",
      "Epoch 00019: val_loss did not improve from 46.29911\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 48.2472 - val_loss: 48.9944\n",
      "Epoch 20/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 47.5633\n",
      "Epoch 00020: val_loss did not improve from 46.29911\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 47.5616 - val_loss: 50.3629\n",
      "Epoch 21/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 47.1009\n",
      "Epoch 00021: val_loss did not improve from 46.29911\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 47.0971 - val_loss: 46.4636\n",
      "Epoch 22/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 46.7487\n",
      "Epoch 00022: val_loss did not improve from 46.29911\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 46.7518 - val_loss: 55.3602\n",
      "Epoch 23/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 46.1955\n",
      "Epoch 00023: val_loss improved from 46.29911 to 45.68179, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 46.1955 - val_loss: 45.6818\n",
      "Epoch 24/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 45.9007\n",
      "Epoch 00024: val_loss did not improve from 45.68179\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 45.9046 - val_loss: 48.1756\n",
      "Epoch 25/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 45.6390\n",
      "Epoch 00025: val_loss did not improve from 45.68179\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 45.6428 - val_loss: 57.8612\n",
      "Epoch 26/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 45.1748\n",
      "Epoch 00026: val_loss improved from 45.68179 to 43.26967, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 26s 5ms/step - loss: 45.1704 - val_loss: 43.2697\n",
      "Epoch 27/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 44.5760\n",
      "Epoch 00027: val_loss did not improve from 43.26967\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 44.5789 - val_loss: 43.5443\n",
      "Epoch 28/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 44.2693\n",
      "Epoch 00028: val_loss improved from 43.26967 to 42.43222, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 44.2631 - val_loss: 42.4322\n",
      "Epoch 29/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 44.0907\n",
      "Epoch 00029: val_loss did not improve from 42.43222\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 44.0858 - val_loss: 42.9288\n",
      "Epoch 30/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 43.4931\n",
      "Epoch 00030: val_loss did not improve from 42.43222\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 43.4941 - val_loss: 50.6263\n",
      "Epoch 31/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 43.3357\n",
      "Epoch 00031: val_loss did not improve from 42.43222\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 43.3377 - val_loss: 42.8704\n",
      "Epoch 32/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 43.1827\n",
      "Epoch 00032: val_loss did not improve from 42.43222\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 43.1820 - val_loss: 42.7124\n",
      "Epoch 33/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 42.7637\n",
      "Epoch 00033: val_loss improved from 42.43222 to 42.03813, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 42.7637 - val_loss: 42.0381\n",
      "Epoch 34/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 42.5227\n",
      "Epoch 00034: val_loss did not improve from 42.03813\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 42.5222 - val_loss: 43.7702\n",
      "Epoch 35/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 41.9442\n",
      "Epoch 00035: val_loss improved from 42.03813 to 40.81044, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 41.9442 - val_loss: 40.8104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 41.7408\n",
      "Epoch 00036: val_loss did not improve from 40.81044\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 41.7454 - val_loss: 42.9941\n",
      "Epoch 37/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 41.5893\n",
      "Epoch 00037: val_loss did not improve from 40.81044\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 41.5914 - val_loss: 42.6355\n",
      "Epoch 38/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 41.1441\n",
      "Epoch 00038: val_loss improved from 40.81044 to 38.82792, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 41.1424 - val_loss: 38.8279\n",
      "Epoch 39/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 41.1151\n",
      "Epoch 00039: val_loss did not improve from 38.82792\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 41.1129 - val_loss: 48.4896\n",
      "Epoch 40/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 40.9667\n",
      "Epoch 00040: val_loss did not improve from 38.82792\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 40.9668 - val_loss: 41.1957\n",
      "Epoch 41/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 40.6146\n",
      "Epoch 00041: val_loss did not improve from 38.82792\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 40.6145 - val_loss: 40.6214\n",
      "Epoch 42/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 40.2610\n",
      "Epoch 00042: val_loss improved from 38.82792 to 38.80693, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 40.2649 - val_loss: 38.8069\n",
      "Epoch 43/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 40.1801\n",
      "Epoch 00043: val_loss did not improve from 38.80693\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 40.1813 - val_loss: 40.1315\n",
      "Epoch 44/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 39.9763\n",
      "Epoch 00044: val_loss did not improve from 38.80693\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.9701 - val_loss: 40.5842\n",
      "Epoch 45/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 39.6523\n",
      "Epoch 00045: val_loss did not improve from 38.80693\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.6501 - val_loss: 43.5272\n",
      "Epoch 46/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 39.4647\n",
      "Epoch 00046: val_loss did not improve from 38.80693\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.4622 - val_loss: 40.1087\n",
      "Epoch 47/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 39.4161\n",
      "Epoch 00047: val_loss did not improve from 38.80693\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.4161 - val_loss: 44.8692\n",
      "Epoch 48/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 39.0483\n",
      "Epoch 00048: val_loss improved from 38.80693 to 37.35453, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.0470 - val_loss: 37.3545\n",
      "Epoch 49/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 38.8666\n",
      "Epoch 00049: val_loss did not improve from 37.35453\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.8641 - val_loss: 42.5375\n",
      "Epoch 50/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 38.5770\n",
      "Epoch 00050: val_loss improved from 37.35453 to 37.29427, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.5744 - val_loss: 37.2943\n",
      "Epoch 51/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 38.6038\n",
      "Epoch 00051: val_loss did not improve from 37.29427\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.6038 - val_loss: 40.4377\n",
      "Epoch 52/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 38.3927\n",
      "Epoch 00052: val_loss improved from 37.29427 to 36.60917, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.3936 - val_loss: 36.6092\n",
      "Epoch 53/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 38.2399\n",
      "Epoch 00053: val_loss did not improve from 36.60917\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.2390 - val_loss: 39.8776\n",
      "Epoch 54/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 38.1379\n",
      "Epoch 00054: val_loss did not improve from 36.60917\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.1387 - val_loss: 42.3395\n",
      "Epoch 55/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 37.8678\n",
      "Epoch 00055: val_loss did not improve from 36.60917\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.8700 - val_loss: 38.7369\n",
      "Epoch 56/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 37.7307\n",
      "Epoch 00056: val_loss did not improve from 36.60917\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.7342 - val_loss: 40.8873\n",
      "Epoch 57/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 37.5781\n",
      "Epoch 00057: val_loss did not improve from 36.60917\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.5843 - val_loss: 44.1010\n",
      "Epoch 58/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 37.4251\n",
      "Epoch 00058: val_loss did not improve from 36.60917\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.4236 - val_loss: 38.4204\n",
      "Epoch 59/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 37.0844\n",
      "Epoch 00059: val_loss did not improve from 36.60917\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.0845 - val_loss: 40.6232\n",
      "Epoch 60/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 37.0464\n",
      "Epoch 00060: val_loss improved from 36.60917 to 35.68857, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.0456 - val_loss: 35.6886\n",
      "Epoch 61/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 36.9889\n",
      "Epoch 00061: val_loss did not improve from 35.68857\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.9880 - val_loss: 37.3292\n",
      "Epoch 62/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 36.7092\n",
      "Epoch 00062: val_loss did not improve from 35.68857\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.7131 - val_loss: 40.9166\n",
      "Epoch 63/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 36.7045\n",
      "Epoch 00063: val_loss did not improve from 35.68857\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.7041 - val_loss: 36.8400\n",
      "Epoch 64/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 36.4621\n",
      "Epoch 00064: val_loss did not improve from 35.68857\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.4671 - val_loss: 51.9954\n",
      "Epoch 65/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 36.4158\n",
      "Epoch 00065: val_loss did not improve from 35.68857\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.4106 - val_loss: 37.7120\n",
      "Epoch 66/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 36.3010\n",
      "Epoch 00066: val_loss did not improve from 35.68857\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.3030 - val_loss: 37.1703\n",
      "Epoch 67/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 36.1474\n",
      "Epoch 00067: val_loss improved from 35.68857 to 35.09718, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.1482 - val_loss: 35.0972\n",
      "Epoch 68/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 36.0370\n",
      "Epoch 00068: val_loss did not improve from 35.09718\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.0368 - val_loss: 35.7966\n",
      "Epoch 69/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 35.9467\n",
      "Epoch 00069: val_loss did not improve from 35.09718\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.9470 - val_loss: 38.9655\n",
      "Epoch 70/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 35.5290\n",
      "Epoch 00070: val_loss did not improve from 35.09718\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.5282 - val_loss: 37.2303\n",
      "Epoch 71/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 35.8555\n",
      "Epoch 00071: val_loss did not improve from 35.09718\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.8520 - val_loss: 35.1543\n",
      "Epoch 72/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 35.3383\n",
      "Epoch 00072: val_loss improved from 35.09718 to 34.58123, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.3357 - val_loss: 34.5812\n",
      "Epoch 73/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 35.2710\n",
      "Epoch 00073: val_loss improved from 34.58123 to 33.90781, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.2700 - val_loss: 33.9078\n",
      "Epoch 74/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 35.3016\n",
      "Epoch 00074: val_loss did not improve from 33.90781\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.3015 - val_loss: 34.6191\n",
      "Epoch 75/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 35.1672\n",
      "Epoch 00075: val_loss did not improve from 33.90781\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.1641 - val_loss: 37.1119\n",
      "Epoch 76/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 35.1430\n",
      "Epoch 00076: val_loss did not improve from 33.90781\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.1442 - val_loss: 36.1226\n",
      "Epoch 77/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 34.8441\n",
      "Epoch 00077: val_loss did not improve from 33.90781\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.8469 - val_loss: 35.9657\n",
      "Epoch 78/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 34.8074\n",
      "Epoch 00078: val_loss did not improve from 33.90781\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.8030 - val_loss: 41.8252\n",
      "Epoch 79/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 34.6045\n",
      "Epoch 00079: val_loss did not improve from 33.90781\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.6038 - val_loss: 34.9738\n",
      "Epoch 80/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 34.6423\n",
      "Epoch 00080: val_loss did not improve from 33.90781\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.6452 - val_loss: 35.4540\n",
      "Epoch 81/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 34.6018\n",
      "Epoch 00081: val_loss did not improve from 33.90781\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.5986 - val_loss: 34.1677\n",
      "Epoch 82/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 34.3755\n",
      "Epoch 00082: val_loss improved from 33.90781 to 33.65592, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.3747 - val_loss: 33.6559\n",
      "Epoch 83/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 34.5108\n",
      "Epoch 00083: val_loss did not improve from 33.65592\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.5056 - val_loss: 34.5394\n",
      "Epoch 84/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 34.2113\n",
      "Epoch 00084: val_loss improved from 33.65592 to 33.08285, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.2094 - val_loss: 33.0828\n",
      "Epoch 85/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 34.2435\n",
      "Epoch 00085: val_loss did not improve from 33.08285\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.2493 - val_loss: 39.0552\n",
      "Epoch 86/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 34.0867\n",
      "Epoch 00086: val_loss did not improve from 33.08285\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.0881 - val_loss: 35.3531\n",
      "Epoch 87/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 33.8425\n",
      "Epoch 00087: val_loss did not improve from 33.08285\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.8398 - val_loss: 35.2404\n",
      "Epoch 88/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 33.9208\n",
      "Epoch 00088: val_loss did not improve from 33.08285\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.9208 - val_loss: 36.3626\n",
      "Epoch 89/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 33.8243\n",
      "Epoch 00089: val_loss did not improve from 33.08285\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.8223 - val_loss: 34.5734\n",
      "Epoch 90/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 33.7416\n",
      "Epoch 00090: val_loss did not improve from 33.08285\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.7418 - val_loss: 39.3456\n",
      "Epoch 91/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 33.6831\n",
      "Epoch 00091: val_loss improved from 33.08285 to 32.68524, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 27s 5ms/step - loss: 33.6830 - val_loss: 32.6852\n",
      "Epoch 92/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 33.5159\n",
      "Epoch 00092: val_loss improved from 32.68524 to 32.20710, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.5115 - val_loss: 32.2071\n",
      "Epoch 93/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 33.4267\n",
      "Epoch 00093: val_loss did not improve from 32.20710\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.4267 - val_loss: 34.2495\n",
      "Epoch 94/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 33.2130\n",
      "Epoch 00094: val_loss did not improve from 32.20710\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.2129 - val_loss: 37.5516\n",
      "Epoch 95/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 33.3389\n",
      "Epoch 00095: val_loss did not improve from 32.20710\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.3374 - val_loss: 32.7511\n",
      "Epoch 96/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 33.3137\n",
      "Epoch 00096: val_loss did not improve from 32.20710\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.3142 - val_loss: 34.4712\n",
      "Epoch 97/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 33.1632\n",
      "Epoch 00097: val_loss did not improve from 32.20710\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.1610 - val_loss: 33.9110\n",
      "Epoch 98/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 32.8183\n",
      "Epoch 00098: val_loss did not improve from 32.20710\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.8216 - val_loss: 34.3985\n",
      "Epoch 99/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 33.0042\n",
      "Epoch 00099: val_loss improved from 32.20710 to 32.08615, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.0032 - val_loss: 32.0861\n",
      "Epoch 100/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 32.8635\n",
      "Epoch 00100: val_loss improved from 32.08615 to 31.69511, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 32.8641 - val_loss: 31.6951\n",
      "Epoch 101/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 32.8590\n",
      "Epoch 00101: val_loss did not improve from 31.69511\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.8569 - val_loss: 36.1713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 32.7795\n",
      "Epoch 00102: val_loss did not improve from 31.69511\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 32.7753 - val_loss: 31.9300\n",
      "Epoch 103/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 32.8246\n",
      "Epoch 00103: val_loss did not improve from 31.69511\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.8263 - val_loss: 32.5216\n",
      "Epoch 104/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 32.6454\n",
      "Epoch 00104: val_loss did not improve from 31.69511\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.6445 - val_loss: 34.8194\n",
      "Epoch 105/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 32.5360\n",
      "Epoch 00105: val_loss did not improve from 31.69511\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.5360 - val_loss: 34.6694\n",
      "Epoch 106/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 32.4381\n",
      "Epoch 00106: val_loss improved from 31.69511 to 31.47732, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 32.4382 - val_loss: 31.4773\n",
      "Epoch 107/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 32.3377\n",
      "Epoch 00107: val_loss did not improve from 31.47732\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.3389 - val_loss: 39.0400\n",
      "Epoch 108/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 32.3822\n",
      "Epoch 00108: val_loss did not improve from 31.47732\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.3831 - val_loss: 42.1670\n",
      "Epoch 109/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 32.2952\n",
      "Epoch 00109: val_loss did not improve from 31.47732\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 32.2932 - val_loss: 33.7001\n",
      "Epoch 110/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 32.2803\n",
      "Epoch 00110: val_loss did not improve from 31.47732\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.2841 - val_loss: 32.7630\n",
      "Epoch 111/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 32.2575\n",
      "Epoch 00111: val_loss improved from 31.47732 to 30.90144, saving model to DS02/experiment_set_8\\results_0.9\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.2590 - val_loss: 30.9014\n",
      "Epoch 112/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 32.2873\n",
      "Epoch 00112: val_loss did not improve from 30.90144\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.2890 - val_loss: 32.9082\n",
      "Epoch 113/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 31.9319\n",
      "Epoch 00113: val_loss did not improve from 30.90144\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 31.9323 - val_loss: 36.3457\n",
      "Epoch 114/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 32.0449\n",
      "Epoch 00114: val_loss did not improve from 30.90144\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.0456 - val_loss: 33.8993\n",
      "Epoch 115/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 32.0109\n",
      "Epoch 00115: val_loss did not improve from 30.90144\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.0103 - val_loss: 34.2651\n",
      "Epoch 116/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 32.0790\n",
      "Epoch 00116: val_loss did not improve from 30.90144\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 32.0752 - val_loss: 31.2404\n",
      "Epoch 117/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 31.9206\n",
      "Epoch 00117: val_loss did not improve from 30.90144\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 31.9141 - val_loss: 34.2193\n",
      "Epoch 118/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 31.8209\n",
      "Epoch 00118: val_loss did not improve from 30.90144\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 31.8224 - val_loss: 33.1989\n",
      "Epoch 119/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 31.8395\n",
      "Epoch 00119: val_loss did not improve from 30.90144\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 31.8423 - val_loss: 33.5578\n",
      "Epoch 120/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 31.8279 ETA: 0s - loss:\n",
      "Epoch 00120: val_loss did not improve from 30.90144\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 31.8278 - val_loss: 31.3721\n",
      "Epoch 121/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 31.6844\n",
      "Epoch 00121: val_loss did not improve from 30.90144\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 31.6830 - val_loss: 34.9341\n",
      "Epoch 00121: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_0.9\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 30.81\n",
      "RMSE: 5.55\n",
      "CMAPSS score: 1.56\n",
      "\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 256)               1536      \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 231,809\n",
      "Trainable params: 231,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 223.8482\n",
      "Epoch 00001: val_loss improved from inf to 90.49572, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 223.7565 - val_loss: 90.4957\n",
      "Epoch 2/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 81.8068\n",
      "Epoch 00002: val_loss improved from 90.49572 to 77.68697, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 81.7870 - val_loss: 77.6870\n",
      "Epoch 3/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 74.7768\n",
      "Epoch 00003: val_loss did not improve from 77.68697\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 74.7865 - val_loss: 96.6048\n",
      "Epoch 4/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 69.4090\n",
      "Epoch 00004: val_loss improved from 77.68697 to 64.49705, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 69.4059 - val_loss: 64.4970\n",
      "Epoch 5/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 66.0814\n",
      "Epoch 00005: val_loss improved from 64.49705 to 60.48510, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 66.0717 - val_loss: 60.4851\n",
      "Epoch 6/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 62.9828\n",
      "Epoch 00006: val_loss did not improve from 60.48510\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 62.9863 - val_loss: 73.6527\n",
      "Epoch 7/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 61.0496\n",
      "Epoch 00007: val_loss did not improve from 60.48510\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 61.0425 - val_loss: 65.4202\n",
      "Epoch 8/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 59.0238\n",
      "Epoch 00008: val_loss improved from 60.48510 to 54.81264, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 59.0234 - val_loss: 54.8126\n",
      "Epoch 9/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5027/5038 [============================>.] - ETA: 0s - loss: 57.1615\n",
      "Epoch 00009: val_loss did not improve from 54.81264\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 57.1592 - val_loss: 55.5930\n",
      "Epoch 10/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 56.1154\n",
      "Epoch 00010: val_loss did not improve from 54.81264\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 56.1210 - val_loss: 67.8468\n",
      "Epoch 11/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 54.8440\n",
      "Epoch 00011: val_loss improved from 54.81264 to 51.99168, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 54.8423 - val_loss: 51.9917\n",
      "Epoch 12/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 53.8846\n",
      "Epoch 00012: val_loss did not improve from 51.99168\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 53.8842 - val_loss: 58.0570\n",
      "Epoch 13/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 52.7553\n",
      "Epoch 00013: val_loss did not improve from 51.99168\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 52.7555 - val_loss: 52.4915\n",
      "Epoch 14/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 51.6746\n",
      "Epoch 00014: val_loss improved from 51.99168 to 51.28644, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 51.6746 - val_loss: 51.2864\n",
      "Epoch 15/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 51.0544\n",
      "Epoch 00015: val_loss improved from 51.28644 to 51.16357, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 51.0548 - val_loss: 51.1636\n",
      "Epoch 16/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 50.3862\n",
      "Epoch 00016: val_loss improved from 51.16357 to 46.65279, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 50.3859 - val_loss: 46.6528\n",
      "Epoch 17/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 49.6922\n",
      "Epoch 00017: val_loss did not improve from 46.65279\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 49.6922 - val_loss: 72.0270\n",
      "Epoch 18/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 48.9349\n",
      "Epoch 00018: val_loss did not improve from 46.65279\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 48.9373 - val_loss: 51.6632\n",
      "Epoch 19/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 48.4731\n",
      "Epoch 00019: val_loss did not improve from 46.65279\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 48.4791 - val_loss: 53.1273\n",
      "Epoch 20/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 48.0695\n",
      "Epoch 00020: val_loss improved from 46.65279 to 44.87226, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 48.0694 - val_loss: 44.8723\n",
      "Epoch 21/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 47.3194\n",
      "Epoch 00021: val_loss improved from 44.87226 to 44.56922, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 47.3189 - val_loss: 44.5692\n",
      "Epoch 22/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 47.1481\n",
      "Epoch 00022: val_loss did not improve from 44.56922\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 47.1508 - val_loss: 56.7377\n",
      "Epoch 23/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 46.5926\n",
      "Epoch 00023: val_loss did not improve from 44.56922\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 46.5935 - val_loss: 45.6535\n",
      "Epoch 24/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 46.1211\n",
      "Epoch 00024: val_loss did not improve from 44.56922\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 46.1233 - val_loss: 45.8207\n",
      "Epoch 25/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 45.5949\n",
      "Epoch 00025: val_loss did not improve from 44.56922\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 45.5928 - val_loss: 57.3153\n",
      "Epoch 26/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 45.4860\n",
      "Epoch 00026: val_loss did not improve from 44.56922\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 45.4822 - val_loss: 47.5032\n",
      "Epoch 27/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 44.8886\n",
      "Epoch 00027: val_loss did not improve from 44.56922\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 44.8886 - val_loss: 44.7605\n",
      "Epoch 28/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 44.5177\n",
      "Epoch 00028: val_loss did not improve from 44.56922\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 44.5075 - val_loss: 45.2896\n",
      "Epoch 29/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 44.2967\n",
      "Epoch 00029: val_loss did not improve from 44.56922\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 44.2961 - val_loss: 50.6380\n",
      "Epoch 30/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 43.6719\n",
      "Epoch 00030: val_loss did not improve from 44.56922\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 43.6721 - val_loss: 47.2485\n",
      "Epoch 31/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 43.6509\n",
      "Epoch 00031: val_loss improved from 44.56922 to 43.91682, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 43.6553 - val_loss: 43.9168\n",
      "Epoch 32/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 43.1333\n",
      "Epoch 00032: val_loss improved from 43.91682 to 43.44263, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 43.1315 - val_loss: 43.4426\n",
      "Epoch 33/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 42.6418\n",
      "Epoch 00033: val_loss improved from 43.44263 to 42.46561, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 42.6415 - val_loss: 42.4656\n",
      "Epoch 34/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 42.6866\n",
      "Epoch 00034: val_loss did not improve from 42.46561\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 42.6848 - val_loss: 44.3142\n",
      "Epoch 35/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 42.1818\n",
      "Epoch 00035: val_loss did not improve from 42.46561\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 42.1806 - val_loss: 43.3014\n",
      "Epoch 36/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 41.8120\n",
      "Epoch 00036: val_loss did not improve from 42.46561\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 41.8120 - val_loss: 43.0813\n",
      "Epoch 37/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 41.3692\n",
      "Epoch 00037: val_loss did not improve from 42.46561\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 41.3696 - val_loss: 47.4243\n",
      "Epoch 38/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 41.1589\n",
      "Epoch 00038: val_loss improved from 42.46561 to 41.90558, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 41.1534 - val_loss: 41.9056\n",
      "Epoch 39/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 41.0782\n",
      "Epoch 00039: val_loss did not improve from 41.90558\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 41.0780 - val_loss: 48.1430\n",
      "Epoch 40/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 40.9230\n",
      "Epoch 00040: val_loss did not improve from 41.90558\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 40.9250 - val_loss: 42.9785\n",
      "Epoch 41/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 40.6333\n",
      "Epoch 00041: val_loss improved from 41.90558 to 40.63076, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 40.6404 - val_loss: 40.6308\n",
      "Epoch 42/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 40.3148\n",
      "Epoch 00042: val_loss improved from 40.63076 to 39.81073, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 40.3148 - val_loss: 39.8107\n",
      "Epoch 43/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 39.9254\n",
      "Epoch 00043: val_loss did not improve from 39.81073\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 39.9335 - val_loss: 44.2444\n",
      "Epoch 44/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 39.9831\n",
      "Epoch 00044: val_loss did not improve from 39.81073\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 39.9831 - val_loss: 40.8339\n",
      "Epoch 45/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 39.4498\n",
      "Epoch 00045: val_loss did not improve from 39.81073\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 39.4498 - val_loss: 54.1200\n",
      "Epoch 46/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 39.5639\n",
      "Epoch 00046: val_loss did not improve from 39.81073\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 39.5639 - val_loss: 40.2839\n",
      "Epoch 47/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 39.2683\n",
      "Epoch 00047: val_loss did not improve from 39.81073\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 39.2695 - val_loss: 43.8250\n",
      "Epoch 48/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 38.9927\n",
      "Epoch 00048: val_loss improved from 39.81073 to 38.05223, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.9900 - val_loss: 38.0522\n",
      "Epoch 49/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 38.6304\n",
      "Epoch 00049: val_loss did not improve from 38.05223\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 38.6294 - val_loss: 44.5480\n",
      "Epoch 50/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 38.4351\n",
      "Epoch 00050: val_loss improved from 38.05223 to 37.00047, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.4317 - val_loss: 37.0005\n",
      "Epoch 51/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 38.5664\n",
      "Epoch 00051: val_loss did not improve from 37.00047\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 38.5666 - val_loss: 37.6619\n",
      "Epoch 52/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 38.3105\n",
      "Epoch 00052: val_loss did not improve from 37.00047\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 38.3124 - val_loss: 39.9470\n",
      "Epoch 53/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 38.1504\n",
      "Epoch 00053: val_loss improved from 37.00047 to 36.85084, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.1502 - val_loss: 36.8508\n",
      "Epoch 54/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 37.9760\n",
      "Epoch 00054: val_loss did not improve from 36.85084\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 37.9773 - val_loss: 43.8206\n",
      "Epoch 55/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 38.0649\n",
      "Epoch 00055: val_loss did not improve from 36.85084\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 38.0659 - val_loss: 36.8707\n",
      "Epoch 56/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 37.6305\n",
      "Epoch 00056: val_loss did not improve from 36.85084\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 37.6412 - val_loss: 45.3173\n",
      "Epoch 57/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 37.3720\n",
      "Epoch 00057: val_loss improved from 36.85084 to 35.43850, saving model to DS02/experiment_set_8\\results_0.9\\split_1\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 37.3721 - val_loss: 35.4385\n",
      "Epoch 58/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 37.5727\n",
      "Epoch 00058: val_loss did not improve from 35.43850\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 37.5685 - val_loss: 38.4609\n",
      "Epoch 59/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 37.2468\n",
      "Epoch 00059: val_loss did not improve from 35.43850\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 37.2432 - val_loss: 36.3409\n",
      "Epoch 60/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 37.0444\n",
      "Epoch 00060: val_loss did not improve from 35.43850\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 37.0381 - val_loss: 35.6602\n",
      "Epoch 61/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 37.0425\n",
      "Epoch 00061: val_loss did not improve from 35.43850\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 37.0427 - val_loss: 42.9150\n",
      "Epoch 62/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 36.7548\n",
      "Epoch 00062: val_loss did not improve from 35.43850\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 36.7596 - val_loss: 48.2699\n",
      "Epoch 63/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 36.6574\n",
      "Epoch 00063: val_loss did not improve from 35.43850\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 36.6533 - val_loss: 37.6030\n",
      "Epoch 64/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 36.4329\n",
      "Epoch 00064: val_loss did not improve from 35.43850\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 36.4277 - val_loss: 38.8464\n",
      "Epoch 65/200\n",
      "5021/5038 [============================>.] - ETA: 0s - loss: 36.3878\n",
      "Epoch 00065: val_loss did not improve from 35.43850\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 36.3763 - val_loss: 39.9067\n",
      "Epoch 66/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 36.2650\n",
      "Epoch 00066: val_loss did not improve from 35.43850\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 36.2606 - val_loss: 39.5132\n",
      "Epoch 67/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 36.0991\n",
      "Epoch 00067: val_loss did not improve from 35.43850\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 36.0966 - val_loss: 37.8358\n",
      "Epoch 00067: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_0.9\\split_1\\history.pkl\n",
      "Test set:\n",
      "MSE: 35.47\n",
      "RMSE: 5.96\n",
      "CMAPSS score: 1.61\n",
      "\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 256)               1536      \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 231,809\n",
      "Trainable params: 231,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 219.4236\n",
      "Epoch 00001: val_loss improved from inf to 128.37581, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 219.1547 - val_loss: 128.3758\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5028/5038 [============================>.] - ETA: 0s - loss: 80.4407\n",
      "Epoch 00002: val_loss improved from 128.37581 to 72.80910, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 80.4123 - val_loss: 72.8091\n",
      "Epoch 3/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 73.1256\n",
      "Epoch 00003: val_loss improved from 72.80910 to 71.11420, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 73.1269 - val_loss: 71.1142\n",
      "Epoch 4/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 68.3161\n",
      "Epoch 00004: val_loss improved from 71.11420 to 63.98215, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 68.3177 - val_loss: 63.9822\n",
      "Epoch 5/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 65.0697\n",
      "Epoch 00005: val_loss did not improve from 63.98215\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 65.0660 - val_loss: 65.0248\n",
      "Epoch 6/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 62.4164\n",
      "Epoch 00006: val_loss did not improve from 63.98215\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 62.4188 - val_loss: 72.7775\n",
      "Epoch 7/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 60.4045\n",
      "Epoch 00007: val_loss improved from 63.98215 to 58.18304, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 60.4029 - val_loss: 58.1830\n",
      "Epoch 8/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 58.8494\n",
      "Epoch 00008: val_loss improved from 58.18304 to 57.63034, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 58.8494 - val_loss: 57.6303\n",
      "Epoch 9/200\n",
      "5022/5038 [============================>.] - ETA: 0s - loss: 56.9825\n",
      "Epoch 00009: val_loss did not improve from 57.63034\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 56.9763 - val_loss: 60.1832\n",
      "Epoch 10/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 55.6549\n",
      "Epoch 00010: val_loss did not improve from 57.63034\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 55.6588 - val_loss: 77.3917\n",
      "Epoch 11/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 54.5988\n",
      "Epoch 00011: val_loss improved from 57.63034 to 51.34777, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 54.5887 - val_loss: 51.3478\n",
      "Epoch 12/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 53.5475\n",
      "Epoch 00012: val_loss did not improve from 51.34777\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 53.5436 - val_loss: 55.4457\n",
      "Epoch 13/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 52.5818\n",
      "Epoch 00013: val_loss did not improve from 51.34777\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 52.5788 - val_loss: 54.0232\n",
      "Epoch 14/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 51.6793\n",
      "Epoch 00014: val_loss did not improve from 51.34777\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 51.6768 - val_loss: 52.9355\n",
      "Epoch 15/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 50.9421\n",
      "Epoch 00015: val_loss did not improve from 51.34777\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 50.9430 - val_loss: 51.9075\n",
      "Epoch 16/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 50.1339\n",
      "Epoch 00016: val_loss improved from 51.34777 to 47.90210, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 50.1340 - val_loss: 47.9021\n",
      "Epoch 17/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 49.8514\n",
      "Epoch 00017: val_loss did not improve from 47.90210\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 49.8456 - val_loss: 64.2376\n",
      "Epoch 18/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 49.0651\n",
      "Epoch 00018: val_loss did not improve from 47.90210\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 49.0682 - val_loss: 51.6638\n",
      "Epoch 19/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 48.5642\n",
      "Epoch 00019: val_loss did not improve from 47.90210\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 48.5665 - val_loss: 48.7287\n",
      "Epoch 20/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 48.0219\n",
      "Epoch 00020: val_loss did not improve from 47.90210\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 48.0248 - val_loss: 48.0970\n",
      "Epoch 21/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 47.5366\n",
      "Epoch 00021: val_loss improved from 47.90210 to 45.05950, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 47.5328 - val_loss: 45.0595\n",
      "Epoch 22/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 47.1611\n",
      "Epoch 00022: val_loss did not improve from 45.05950\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 47.1637 - val_loss: 58.6561\n",
      "Epoch 23/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 46.6839\n",
      "Epoch 00023: val_loss did not improve from 45.05950\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 46.6866 - val_loss: 48.1765\n",
      "Epoch 24/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 46.1504\n",
      "Epoch 00024: val_loss did not improve from 45.05950\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 46.1525 - val_loss: 50.9991\n",
      "Epoch 25/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 45.7442\n",
      "Epoch 00025: val_loss did not improve from 45.05950\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 45.7447 - val_loss: 54.0591\n",
      "Epoch 26/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 45.3915\n",
      "Epoch 00026: val_loss did not improve from 45.05950\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 45.3885 - val_loss: 46.7719\n",
      "Epoch 27/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 44.9282\n",
      "Epoch 00027: val_loss improved from 45.05950 to 43.57727, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 44.9272 - val_loss: 43.5773\n",
      "Epoch 28/200\n",
      "5022/5038 [============================>.] - ETA: 0s - loss: 44.4812\n",
      "Epoch 00028: val_loss improved from 43.57727 to 43.09490, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 44.4686 - val_loss: 43.0949\n",
      "Epoch 29/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 44.2870\n",
      "Epoch 00029: val_loss did not improve from 43.09490\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 44.2870 - val_loss: 44.0633\n",
      "Epoch 30/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 43.7125\n",
      "Epoch 00030: val_loss did not improve from 43.09490\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 43.7125 - val_loss: 46.2604\n",
      "Epoch 31/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 43.4873\n",
      "Epoch 00031: val_loss improved from 43.09490 to 40.88822, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 43.4907 - val_loss: 40.8882\n",
      "Epoch 32/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 43.3346\n",
      "Epoch 00032: val_loss did not improve from 40.88822\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 43.3376 - val_loss: 43.5133\n",
      "Epoch 33/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 42.7982\n",
      "Epoch 00033: val_loss did not improve from 40.88822\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 42.7999 - val_loss: 47.3654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 42.4327\n",
      "Epoch 00034: val_loss did not improve from 40.88822\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 42.4365 - val_loss: 56.1003\n",
      "Epoch 35/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 42.1839\n",
      "Epoch 00035: val_loss improved from 40.88822 to 40.47413, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 42.1814 - val_loss: 40.4741\n",
      "Epoch 36/200\n",
      "5022/5038 [============================>.] - ETA: 0s - loss: 41.8086\n",
      "Epoch 00036: val_loss did not improve from 40.47413\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 41.8060 - val_loss: 40.5354\n",
      "Epoch 37/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 41.6895\n",
      "Epoch 00037: val_loss did not improve from 40.47413\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 41.6936 - val_loss: 50.6919\n",
      "Epoch 38/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 41.3376\n",
      "Epoch 00038: val_loss improved from 40.47413 to 39.79967, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 41.3372 - val_loss: 39.7997\n",
      "Epoch 39/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 41.1435\n",
      "Epoch 00039: val_loss did not improve from 39.79967\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 41.1429 - val_loss: 42.3630\n",
      "Epoch 40/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 40.8926\n",
      "Epoch 00040: val_loss did not improve from 39.79967\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 40.8935 - val_loss: 39.8888\n",
      "Epoch 41/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 40.7218\n",
      "Epoch 00041: val_loss did not improve from 39.79967\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 40.7286 - val_loss: 46.7090\n",
      "Epoch 42/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 40.3791\n",
      "Epoch 00042: val_loss did not improve from 39.79967\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 40.3771 - val_loss: 43.1365\n",
      "Epoch 43/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 40.2256\n",
      "Epoch 00043: val_loss did not improve from 39.79967\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 40.2330 - val_loss: 40.2796\n",
      "Epoch 44/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 39.9962\n",
      "Epoch 00044: val_loss did not improve from 39.79967\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 40.0018 - val_loss: 48.9886\n",
      "Epoch 45/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 39.7539\n",
      "Epoch 00045: val_loss did not improve from 39.79967\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.7487 - val_loss: 41.7495\n",
      "Epoch 46/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 39.4961\n",
      "Epoch 00046: val_loss did not improve from 39.79967\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 39.4963 - val_loss: 41.3698\n",
      "Epoch 47/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 39.4685\n",
      "Epoch 00047: val_loss did not improve from 39.79967\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 39.4696 - val_loss: 52.4365\n",
      "Epoch 48/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 39.1747\n",
      "Epoch 00048: val_loss improved from 39.79967 to 39.16362, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 39.1628 - val_loss: 39.1636\n",
      "Epoch 49/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 38.9501\n",
      "Epoch 00049: val_loss did not improve from 39.16362\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.9498 - val_loss: 44.8045\n",
      "Epoch 50/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 38.7402\n",
      "Epoch 00050: val_loss improved from 39.16362 to 35.86333, saving model to DS02/experiment_set_8\\results_0.9\\split_2\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 38.7348 - val_loss: 35.8633\n",
      "Epoch 51/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 38.5312\n",
      "Epoch 00051: val_loss did not improve from 35.86333\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.5297 - val_loss: 42.3205\n",
      "Epoch 52/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 38.1917\n",
      "Epoch 00052: val_loss did not improve from 35.86333\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.1887 - val_loss: 36.9855\n",
      "Epoch 53/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 38.3216\n",
      "Epoch 00053: val_loss did not improve from 35.86333\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.3228 - val_loss: 42.5204\n",
      "Epoch 54/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 38.0345\n",
      "Epoch 00054: val_loss did not improve from 35.86333\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.0402 - val_loss: 44.0867\n",
      "Epoch 55/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 37.9697\n",
      "Epoch 00055: val_loss did not improve from 35.86333\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 37.9684 - val_loss: 35.9198\n",
      "Epoch 56/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 37.6619\n",
      "Epoch 00056: val_loss did not improve from 35.86333\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.6664 - val_loss: 42.6272\n",
      "Epoch 57/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 37.6467\n",
      "Epoch 00057: val_loss did not improve from 35.86333\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 37.6505 - val_loss: 39.9990\n",
      "Epoch 58/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 37.5263\n",
      "Epoch 00058: val_loss did not improve from 35.86333\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 37.5215 - val_loss: 38.8945\n",
      "Epoch 59/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 37.2028\n",
      "Epoch 00059: val_loss did not improve from 35.86333\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 37.2098 - val_loss: 41.5267\n",
      "Epoch 60/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 37.2687\n",
      "Epoch 00060: val_loss did not improve from 35.86333\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 37.2616 - val_loss: 36.6379\n",
      "Epoch 00060: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_0.9\\split_2\\history.pkl\n",
      "Test set:\n",
      "MSE: 35.79\n",
      "RMSE: 5.98\n",
      "CMAPSS score: 1.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "# Test effect of correlation threshold\n",
    "######################################\n",
    "NUM_TRIALS = 3\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "layer_sizes = [256, 256, 512, 64]\n",
    "\n",
    "initial_columns = x_train.columns\n",
    "corr_th_list = [None, 0.99, 0.95, 0.9]\n",
    "\n",
    "results_file = os.path.join(output_path, \"results.txt\")\n",
    "open(results_file, \"w\").close()\n",
    "\n",
    "for corr_th in corr_th_list:\n",
    "    # Select features based on training set\n",
    "    if corr_th is not None:\n",
    "        selected_columns = get_non_correlated_features(x_train, corr_th=corr_th, debug=False)\n",
    "    else:\n",
    "        selected_columns = x_train.columns\n",
    "    \n",
    "    x_train_feature_selection = x_train[selected_columns]\n",
    "    \n",
    "    mse_vals = []\n",
    "    rmse_vals = []\n",
    "    cmapss_vals = []\n",
    "    \n",
    "    for random_seed in range(NUM_TRIALS):\n",
    "        # Train-validation split for early stopping\n",
    "        x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train_feature_selection, \n",
    "                                                                                  y_train, \n",
    "                                                                                  test_size=0.3, \n",
    "                                                                                  random_state=seed)\n",
    "        # Create output path\n",
    "        results_folder = \"results_all\" if corr_th is None else \"results_{}\".format(corr_th)\n",
    "        results_path_crr_th = os.path.join(output_path, results_folder)\n",
    "        results_path_crr_split = os.path.join(results_path_crr_th, \"split_{}\".format(random_seed))\n",
    "        if not os.path.exists(results_path_crr_split):\n",
    "            os.makedirs(results_path_crr_split)\n",
    "\n",
    "        # Standardization\n",
    "        scaler = StandardScaler()\n",
    "        x_train_scaled = scaler.fit_transform(x_train_split)\n",
    "        x_val_scaled = scaler.transform(x_val_split)\n",
    "        input_dim = x_train_scaled.shape[1]\n",
    "\n",
    "        # Create model\n",
    "        weights_file = os.path.join(results_path_crr_th, 'mlp_initial_weights.h5')\n",
    "        model_path = os.path.join(results_path_crr_split, 'mlp_model_trained.h5')\n",
    "        model = create_mlp_model(input_dim, layer_sizes, activation='tanh')\n",
    "        model.summary()\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint(model_path, monitor='val_loss', mode='min', verbose=2, \n",
    "                             save_best_only=True)\n",
    "\n",
    "        # Train model\n",
    "        history = train_model_existing_weights(model, weights_file, \n",
    "                                               x_train_scaled, y_train_split, \n",
    "                                               x_val_scaled, y_val_split, \n",
    "                                               batch_size=batch_size, \n",
    "                                               epochs=epochs, \n",
    "                                               callbacks=[es, mc])\n",
    "\n",
    "        history_file = os.path.join(results_path_crr_split, \"history.pkl\")\n",
    "        save_history(history, history_file)\n",
    "\n",
    "        # Performance evaluation\n",
    "        x_test_feature_selection = x_test[selected_columns]\n",
    "        x_test_scaled = scaler.transform(x_test_feature_selection)\n",
    "\n",
    "        loaded_model = load_model(model_path)\n",
    "        predictions_test = loaded_model.predict(x_test_scaled).flatten()\n",
    "        mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)\n",
    "        \n",
    "        mse_vals.append(mse)\n",
    "        rmse_vals.append(rmse)\n",
    "        cmapss_vals.append(cmapss_score)\n",
    "    \n",
    "    mse_mean = np.mean(mse_vals)\n",
    "    mse_std = np.std(mse_vals)\n",
    "    rmse_mean = np.mean(rmse_vals)\n",
    "    rmse_std = np.std(rmse_vals)\n",
    "    cmapss_mean = np.mean(cmapss_vals)\n",
    "    cmapss_std = np.std(cmapss_vals)\n",
    "    \n",
    "    with open(results_file, \"a\", newline='') as file:\n",
    "        file.write(\"Experiment {}\\n\".format(corr_th if corr_th is not None else \"ALL\"))\n",
    "        file.write(\"Selected columns: \")\n",
    "        write_list(selected_columns, file)\n",
    "        file.write(\"Number of features: {}\".format(len(selected_columns)))\n",
    "        file.write(\"\\nMSE = \")\n",
    "        write_list(mse_vals, file)\n",
    "        file.write(\"\\nMSE mean = {}\\nMSE std = {}\\n\".format(mse_mean, mse_std))\n",
    "        \n",
    "        file.write(\"\\nRMSE = \")\n",
    "        write_list(rmse_vals, file)\n",
    "        file.write(\"\\nRMSE mean = {}\\nRMSE std = {}\\n\".format(rmse_mean, rmse_std))\n",
    "        \n",
    "        file.write(\"\\nCMAPSS = \")\n",
    "        write_list(cmapss_vals, file)\n",
    "        file.write(\"\\nCMAPSS mean = {}\\nCMAPSS std = {}\\n\".format(cmapss_mean, cmapss_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature sets used in previous experiment\n",
    "sel_features_1 = \"T24, T30, T48, T50, P15, P2, P21, P24, Ps30, P40, P50, Nf, Nc, Wf, T40, P30, P45, W21, W22, W25, W31, W32, W48, W50, SmFan, SmLPC, SmHPC, phi, alt, Mach, TRA, T2\".split(\",\")\n",
    "sel_features_2 = \"T24, T30, T48, T50, P15, P2, P24, Ps30, Nf, Wf, SmFan, SmLPC, SmHPC, Mach, TRA\".split(\",\")\n",
    "sel_features_3 = \"T24, T30, P15, Nf, SmFan, SmLPC, Mach\".split(\",\")\n",
    "sel_features_4 = \"T24, T30, P15, SmFan, SmLPC\".split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' W22',\n",
       " ' P50',\n",
       " ' W48',\n",
       " ' Nc',\n",
       " ' alt',\n",
       " ' W31',\n",
       " ' P21',\n",
       " ' P40',\n",
       " ' P30',\n",
       " ' W32',\n",
       " ' T40',\n",
       " ' W21',\n",
       " ' phi',\n",
       " ' P45',\n",
       " ' W25',\n",
       " ' T2',\n",
       " ' W50']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(sel_features_1) - set(sel_features_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' T48', ' T50', ' P2', ' SmHPC', ' Ps30', ' P24', ' Wf', ' TRA']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(sel_features_2) - set(sel_features_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Nf', ' Mach']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(sel_features_3) - set(sel_features_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T30 | Nc | 1.0\n",
      "T48 | T40 | 1.0\n",
      "P15 | P21 | 1.0\n",
      "P15 | P50 | 0.99\n",
      "P2 | alt | 0.99\n",
      "P2 | T2 | 0.99\n",
      "P24 | W21 | 1.0\n",
      "Ps30 | P40 | 1.0\n",
      "Ps30 | P30 | 1.0\n",
      "Ps30 | P45 | 1.0\n",
      "Ps30 | W22 | 0.99\n",
      "Ps30 | W25 | 0.99\n",
      "Ps30 | W31 | 0.99\n",
      "Ps30 | W32 | 0.99\n",
      "Ps30 | W48 | 0.99\n",
      "Ps30 | W50 | 0.99\n",
      "SmHPC | phi | 0.99\n",
      "Number of correlated features:  17\n",
      "Correlated features:  ['P21', 'P40', 'P50', 'Nc', 'T40', 'P30', 'P45', 'W21', 'W22', 'W25', 'W31', 'W32', 'W48', 'W50', 'phi', 'alt', 'T2']\n",
      "Number of selected features:  15\n",
      "Selected features:  ['T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P24', 'Ps30', 'Nf', 'Wf', 'SmFan', 'SmLPC', 'SmHPC', 'Mach', 'TRA']\n"
     ]
    }
   ],
   "source": [
    "selected_columns_2 = get_non_correlated_features(x_train, corr_th=0.99, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T24 | Ps30 | 0.99\n",
      "T24 | P40 | 0.99\n",
      "T24 | Wf | 0.97\n",
      "T24 | P30 | 0.99\n",
      "T24 | P45 | 0.98\n",
      "T24 | W22 | 0.98\n",
      "T24 | W25 | 0.98\n",
      "T24 | W31 | 0.98\n",
      "T24 | W32 | 0.98\n",
      "T24 | W48 | 0.98\n",
      "T24 | W50 | 0.98\n",
      "T30 | T48 | 0.97\n",
      "T30 | T50 | 0.95\n",
      "T30 | Nc | 1.0\n",
      "T30 | T40 | 0.97\n",
      "T48 | SmHPC | 0.96\n",
      "T48 | phi | 0.98\n",
      "P15 | P2 | 0.97\n",
      "P15 | P21 | 1.0\n",
      "P15 | P24 | 0.99\n",
      "P15 | P50 | 0.99\n",
      "P15 | W21 | 0.99\n",
      "P15 | alt | 0.96\n",
      "P15 | T2 | 0.97\n",
      "Nf | TRA | 0.97\n",
      "Number of correlated features:  25\n",
      "Correlated features:  ['T48', 'T50', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nc', 'Wf', 'T40', 'P30', 'P45', 'W21', 'W22', 'W25', 'W31', 'W32', 'W48', 'W50', 'SmHPC', 'phi', 'alt', 'TRA', 'T2']\n",
      "Number of selected features:  7\n",
      "Selected features:  ['T24', 'T30', 'P15', 'Nf', 'SmFan', 'SmLPC', 'Mach']\n"
     ]
    }
   ],
   "source": [
    "selected_columns_3 = get_non_correlated_features(x_train, corr_th=0.95, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_list_to_string(feature_list):\n",
    "    return \"__\".join(feature_list)\n",
    "\n",
    "selected_columns_list = [\n",
    "    ['T24', 'T48', 'T50', 'T30', 'P15', 'Nf', 'SmFan', 'SmLPC', 'Mach'],\n",
    "    ['T24', 'T48', 'T30', 'P15', 'Nf', 'SmFan', 'SmLPC', 'Mach'],\n",
    "    ['T24', 'T50', 'T30', 'P15', 'Nf', 'SmFan', 'SmLPC', 'Mach'],\n",
    "    ['T24', 'T48', 'T50', 'P15', 'Nf', 'SmFan', 'SmLPC', 'Mach'],\n",
    "\n",
    "    ['T24', 'Ps30', 'Wf', 'T30', 'P15', 'Nf', 'SmFan', 'SmLPC', 'Mach'],\n",
    "    ['T24', 'Ps30', 'T30', 'P15', 'Nf', 'SmFan', 'SmLPC', 'Mach'],\n",
    "    ['T24', 'Wf', 'T30', 'P15', 'Nf', 'SmFan', 'SmLPC', 'Mach'],\n",
    "    ['Ps30', 'Wf', 'T30', 'P15', 'Nf', 'SmFan', 'SmLPC', 'Mach'],\n",
    "\n",
    "    ['T24', 'T30', 'P2', 'P15', 'P24', 'Nf', 'SmFan', 'SmLPC', 'Mach'],\n",
    "    ['T24', 'T30', 'P2', 'P24', 'Nf', 'SmFan', 'SmLPC', 'Mach'],\n",
    "    ['T24', 'T30', 'P15', 'P24', 'Nf', 'SmFan', 'SmLPC', 'Mach'],\n",
    "    ['T24', 'T30', 'P2', 'P15', 'Nf', 'SmFan', 'SmLPC', 'Mach'],\n",
    "\n",
    "    ['T24', 'T30', 'P15', 'Nf', 'SmHPC', 'TRA', 'SmFan', 'SmLPC', 'Mach'],\n",
    "    ['T24', 'T30', 'P15', 'Nf', 'SmHPC', 'SmFan', 'SmLPC', 'Mach'],\n",
    "    ['T24', 'T30', 'P15', 'Nf', 'TRA', 'SmFan', 'SmLPC', 'Mach'],\n",
    "    ['T24', 'T30', 'P15', 'SmHPC', 'TRA', 'SmFan', 'SmLPC', 'Mach']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               2560      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 232,833\n",
      "Trainable params: 232,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 162.7417\n",
      "Epoch 00001: val_loss improved from inf to 46.67076, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 162.7417 - val_loss: 46.6708\n",
      "Epoch 2/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 44.0872\n",
      "Epoch 00002: val_loss improved from 46.67076 to 40.83056, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 44.0747 - val_loss: 40.8306\n",
      "Epoch 3/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 37.8734 ETA: 0s - loss: 3 - ETA: 0s - loss: 37.8634\n",
      "Epoch 00003: val_loss improved from 40.83056 to 35.45304, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.8610 - val_loss: 35.4530\n",
      "Epoch 4/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 34.2608\n",
      "Epoch 00004: val_loss improved from 35.45304 to 34.16340, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.2577 - val_loss: 34.1634\n",
      "Epoch 5/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 31.9420 ETA: 0s - l\n",
      "Epoch 00005: val_loss improved from 34.16340 to 33.88854, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 31.9403 - val_loss: 33.8885\n",
      "Epoch 6/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 30.2667\n",
      "Epoch 00006: val_loss improved from 33.88854 to 29.71565, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 30.2667 - val_loss: 29.7156\n",
      "Epoch 7/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 28.8912\n",
      "Epoch 00007: val_loss did not improve from 29.71565\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 28.8897 - val_loss: 31.7623\n",
      "Epoch 8/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 27.8186 ETA: \n",
      "Epoch 00008: val_loss improved from 29.71565 to 27.80017, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 27.8145 - val_loss: 27.8002\n",
      "Epoch 9/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 27.0129\n",
      "Epoch 00009: val_loss did not improve from 27.80017\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 27.0129 - val_loss: 27.8034\n",
      "Epoch 10/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 26.1726\n",
      "Epoch 00010: val_loss improved from 27.80017 to 26.93721, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 26.1726 - val_loss: 26.9372\n",
      "Epoch 11/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 25.5709\n",
      "Epoch 00011: val_loss improved from 26.93721 to 24.14433, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 25.5712 - val_loss: 24.1443\n",
      "Epoch 12/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 25.0915\n",
      "Epoch 00012: val_loss did not improve from 24.14433\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 25.0919 - val_loss: 26.6822\n",
      "Epoch 13/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 24.5482\n",
      "Epoch 00013: val_loss did not improve from 24.14433\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 24.5489 - val_loss: 24.5791\n",
      "Epoch 14/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 24.1533\n",
      "Epoch 00014: val_loss did not improve from 24.14433\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 24.1531 - val_loss: 27.7373\n",
      "Epoch 15/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 23.6385\n",
      "Epoch 00015: val_loss did not improve from 24.14433\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 23.6375 - val_loss: 24.5132\n",
      "Epoch 16/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 23.2672\n",
      "Epoch 00016: val_loss improved from 24.14433 to 22.92644, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 23.2667 - val_loss: 22.9264\n",
      "Epoch 17/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 23.0936\n",
      "Epoch 00017: val_loss did not improve from 22.92644\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 23.0927 - val_loss: 26.0209\n",
      "Epoch 18/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 22.6362\n",
      "Epoch 00018: val_loss improved from 22.92644 to 22.92065, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 22.6376 - val_loss: 22.9207\n",
      "Epoch 19/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 22.3277\n",
      "Epoch 00019: val_loss did not improve from 22.92065\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 22.3323 - val_loss: 24.4409\n",
      "Epoch 20/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 22.1042\n",
      "Epoch 00020: val_loss improved from 22.92065 to 22.35950, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 22.1048 - val_loss: 22.3595\n",
      "Epoch 21/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 21.8024\n",
      "Epoch 00021: val_loss improved from 22.35950 to 21.01541, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.7997 - val_loss: 21.0154\n",
      "Epoch 22/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 21.4484\n",
      "Epoch 00022: val_loss did not improve from 21.01541\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 21.4499 - val_loss: 23.8652\n",
      "Epoch 23/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 21.2994\n",
      "Epoch 00023: val_loss improved from 21.01541 to 20.80016, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 21.2995 - val_loss: 20.8002\n",
      "Epoch 24/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 21.0518\n",
      "Epoch 00024: val_loss did not improve from 20.80016\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 21.0510 - val_loss: 21.0222\n",
      "Epoch 25/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 20.7266\n",
      "Epoch 00025: val_loss did not improve from 20.80016\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.7269 - val_loss: 22.5411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 20.4371\n",
      "Epoch 00026: val_loss improved from 20.80016 to 19.32666, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.4368 - val_loss: 19.3267\n",
      "Epoch 27/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 20.2093\n",
      "Epoch 00027: val_loss did not improve from 19.32666\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 20.2084 - val_loss: 20.0199\n",
      "Epoch 28/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 19.7476\n",
      "Epoch 00028: val_loss did not improve from 19.32666\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.7464 - val_loss: 25.3217\n",
      "Epoch 29/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 19.4767\n",
      "Epoch 00029: val_loss did not improve from 19.32666\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 19.4774 - val_loss: 20.6153\n",
      "Epoch 30/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 19.1397\n",
      "Epoch 00030: val_loss did not improve from 19.32666\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.1442 - val_loss: 19.8313\n",
      "Epoch 31/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 18.9467\n",
      "Epoch 00031: val_loss improved from 19.32666 to 18.52425, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.9485 - val_loss: 18.5242\n",
      "Epoch 32/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 18.7187\n",
      "Epoch 00032: val_loss did not improve from 18.52425\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.7137 - val_loss: 18.9291\n",
      "Epoch 33/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 18.3917 ETA: 0s - loss: 1\n",
      "Epoch 00033: val_loss improved from 18.52425 to 17.92312, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.3917 - val_loss: 17.9231\n",
      "Epoch 34/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 18.2193\n",
      "Epoch 00034: val_loss did not improve from 17.92312\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.2167 - val_loss: 18.6209\n",
      "Epoch 35/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 18.1675\n",
      "Epoch 00035: val_loss did not improve from 17.92312\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.1656 - val_loss: 22.4093\n",
      "Epoch 36/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 17.9252\n",
      "Epoch 00036: val_loss did not improve from 17.92312\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.9250 - val_loss: 18.3170\n",
      "Epoch 37/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 17.6834\n",
      "Epoch 00037: val_loss improved from 17.92312 to 17.67428, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.6813 - val_loss: 17.6743\n",
      "Epoch 38/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 17.5633\n",
      "Epoch 00038: val_loss did not improve from 17.67428\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.5649 - val_loss: 18.4645\n",
      "Epoch 39/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 17.3449\n",
      "Epoch 00039: val_loss did not improve from 17.67428\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 17.3444 - val_loss: 18.1039\n",
      "Epoch 40/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 17.2029\n",
      "Epoch 00040: val_loss did not improve from 17.67428\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.2045 - val_loss: 18.4710\n",
      "Epoch 41/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 17.1526\n",
      "Epoch 00041: val_loss did not improve from 17.67428\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.1557 - val_loss: 18.6835\n",
      "Epoch 42/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 17.1075\n",
      "Epoch 00042: val_loss improved from 17.67428 to 17.11423, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.1074 - val_loss: 17.1142\n",
      "Epoch 43/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 16.8555\n",
      "Epoch 00043: val_loss improved from 17.11423 to 16.24196, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.8555 - val_loss: 16.2420\n",
      "Epoch 44/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 16.8478\n",
      "Epoch 00044: val_loss did not improve from 16.24196\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.8478 - val_loss: 22.4901\n",
      "Epoch 45/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 16.6728\n",
      "Epoch 00045: val_loss did not improve from 16.24196\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 16.6710 - val_loss: 16.5464\n",
      "Epoch 46/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 16.6167\n",
      "Epoch 00046: val_loss did not improve from 16.24196\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 16.6136 - val_loss: 17.1319\n",
      "Epoch 47/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 16.4324\n",
      "Epoch 00047: val_loss did not improve from 16.24196\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.4303 - val_loss: 16.3751\n",
      "Epoch 48/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 16.4762\n",
      "Epoch 00048: val_loss did not improve from 16.24196\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 16.4753 - val_loss: 16.5679\n",
      "Epoch 49/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 16.2074\n",
      "Epoch 00049: val_loss did not improve from 16.24196\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.2074 - val_loss: 16.4910\n",
      "Epoch 50/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 16.1301\n",
      "Epoch 00050: val_loss did not improve from 16.24196\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.1320 - val_loss: 16.3524\n",
      "Epoch 51/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 16.0580\n",
      "Epoch 00051: val_loss improved from 16.24196 to 15.81454, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.0574 - val_loss: 15.8145\n",
      "Epoch 52/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 15.8860\n",
      "Epoch 00052: val_loss did not improve from 15.81454\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.8899 - val_loss: 18.8989\n",
      "Epoch 53/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 15.9458\n",
      "Epoch 00053: val_loss did not improve from 15.81454\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.9458 - val_loss: 17.1156\n",
      "Epoch 54/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 15.7478\n",
      "Epoch 00054: val_loss did not improve from 15.81454\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.7471 - val_loss: 16.0701\n",
      "Epoch 55/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 15.6827\n",
      "Epoch 00055: val_loss did not improve from 15.81454\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.6844 - val_loss: 15.8978\n",
      "Epoch 56/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 15.6860\n",
      "Epoch 00056: val_loss did not improve from 15.81454\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.6869 - val_loss: 16.5091\n",
      "Epoch 57/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 15.6447\n",
      "Epoch 00057: val_loss improved from 15.81454 to 15.36725, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.6428 - val_loss: 15.3672\n",
      "Epoch 58/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 15.4974\n",
      "Epoch 00058: val_loss did not improve from 15.36725\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.4976 - val_loss: 15.9345\n",
      "Epoch 59/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 15.5230\n",
      "Epoch 00059: val_loss did not improve from 15.36725\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.5233 - val_loss: 16.4132\n",
      "Epoch 60/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 15.3524\n",
      "Epoch 00060: val_loss improved from 15.36725 to 14.72712, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.3509 - val_loss: 14.7271\n",
      "Epoch 61/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 15.2907\n",
      "Epoch 00061: val_loss did not improve from 14.72712\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 15.2891 - val_loss: 14.9124\n",
      "Epoch 62/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 15.2141\n",
      "Epoch 00062: val_loss did not improve from 14.72712\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 15.2097 - val_loss: 15.9973\n",
      "Epoch 63/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 15.0413\n",
      "Epoch 00063: val_loss did not improve from 14.72712\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 15.0399 - val_loss: 14.8465\n",
      "Epoch 64/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 15.1596\n",
      "Epoch 00064: val_loss did not improve from 14.72712\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 15.1577 - val_loss: 16.0585\n",
      "Epoch 65/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 15.0022\n",
      "Epoch 00065: val_loss did not improve from 14.72712\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 15.0028 - val_loss: 16.3427\n",
      "Epoch 66/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 15.0053\n",
      "Epoch 00066: val_loss did not improve from 14.72712\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 15.0044 - val_loss: 15.7322\n",
      "Epoch 67/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 15.0074\n",
      "Epoch 00067: val_loss did not improve from 14.72712\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 15.0056 - val_loss: 16.3452\n",
      "Epoch 68/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 14.8576\n",
      "Epoch 00068: val_loss did not improve from 14.72712\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 14.8626 - val_loss: 15.4948\n",
      "Epoch 69/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 14.8609\n",
      "Epoch 00069: val_loss did not improve from 14.72712\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 14.8614 - val_loss: 14.9843\n",
      "Epoch 70/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 14.7530\n",
      "Epoch 00070: val_loss did not improve from 14.72712\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.7518 - val_loss: 16.9411\n",
      "Epoch 00070: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\add_feature_experiments\\results_0\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 14.73\n",
      "RMSE: 3.84\n",
      "CMAPSS score: 1.29\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 256)               2304      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 232,577\n",
      "Trainable params: 232,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 166.6603\n",
      "Epoch 00001: val_loss improved from inf to 58.90212, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 166.4027 - val_loss: 58.9021\n",
      "Epoch 2/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 48.7257\n",
      "Epoch 00002: val_loss improved from 58.90212 to 46.33983, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 48.7195 - val_loss: 46.3398\n",
      "Epoch 3/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 43.3913\n",
      "Epoch 00003: val_loss improved from 46.33983 to 42.66809, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 43.3912 - val_loss: 42.6681\n",
      "Epoch 4/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 39.8101\n",
      "Epoch 00004: val_loss improved from 42.66809 to 40.22159, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 39.8101 - val_loss: 40.2216\n",
      "Epoch 5/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 37.3261\n",
      "Epoch 00005: val_loss improved from 40.22159 to 37.16772, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 37.3218 - val_loss: 37.1677\n",
      "Epoch 6/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 35.3810\n",
      "Epoch 00006: val_loss improved from 37.16772 to 35.42869, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 35.3809 - val_loss: 35.4287\n",
      "Epoch 7/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 33.9628\n",
      "Epoch 00007: val_loss did not improve from 35.42869\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 33.9627 - val_loss: 38.2871\n",
      "Epoch 8/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 32.7506\n",
      "Epoch 00008: val_loss improved from 35.42869 to 30.53984, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 32.7479 - val_loss: 30.5398\n",
      "Epoch 9/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 31.8214\n",
      "Epoch 00009: val_loss did not improve from 30.53984\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 31.8200 - val_loss: 33.1686\n",
      "Epoch 10/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 30.5934\n",
      "Epoch 00010: val_loss improved from 30.53984 to 29.44520, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 30.5927 - val_loss: 29.4452\n",
      "Epoch 11/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 29.9312\n",
      "Epoch 00011: val_loss did not improve from 29.44520\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 29.9313 - val_loss: 29.6333\n",
      "Epoch 12/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 29.2001\n",
      "Epoch 00012: val_loss did not improve from 29.44520\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 29.1957 - val_loss: 34.9623\n",
      "Epoch 13/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 28.6296\n",
      "Epoch 00013: val_loss improved from 29.44520 to 27.72664, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 28.6300 - val_loss: 27.7266\n",
      "Epoch 14/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5034/5038 [============================>.] - ETA: 0s - loss: 28.1805\n",
      "Epoch 00014: val_loss did not improve from 27.72664\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 28.1821 - val_loss: 33.6144\n",
      "Epoch 15/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 27.4628\n",
      "Epoch 00015: val_loss did not improve from 27.72664\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 27.4628 - val_loss: 27.7901\n",
      "Epoch 16/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 27.0966\n",
      "Epoch 00016: val_loss improved from 27.72664 to 27.05089, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 27.0919 - val_loss: 27.0509\n",
      "Epoch 17/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 26.7852\n",
      "Epoch 00017: val_loss did not improve from 27.05089\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 26.7852 - val_loss: 27.2351\n",
      "Epoch 18/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 26.5093\n",
      "Epoch 00018: val_loss improved from 27.05089 to 25.59809, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 26.5083 - val_loss: 25.5981\n",
      "Epoch 19/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 25.9275\n",
      "Epoch 00019: val_loss did not improve from 25.59809\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 25.9275 - val_loss: 26.9823\n",
      "Epoch 20/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 25.5294\n",
      "Epoch 00020: val_loss improved from 25.59809 to 25.47121, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 25.5301 - val_loss: 25.4712\n",
      "Epoch 21/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 25.4881\n",
      "Epoch 00021: val_loss improved from 25.47121 to 25.04372, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 25.4867 - val_loss: 25.0437\n",
      "Epoch 22/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 25.1164\n",
      "Epoch 00022: val_loss did not improve from 25.04372\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 25.1193 - val_loss: 27.0049\n",
      "Epoch 23/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 24.9180\n",
      "Epoch 00023: val_loss improved from 25.04372 to 24.01012, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 24.9182 - val_loss: 24.0101\n",
      "Epoch 24/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 24.5573\n",
      "Epoch 00024: val_loss did not improve from 24.01012\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 24.5602 - val_loss: 25.9744\n",
      "Epoch 25/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 24.4981\n",
      "Epoch 00025: val_loss did not improve from 24.01012\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 24.4971 - val_loss: 24.9611\n",
      "Epoch 26/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 24.2759\n",
      "Epoch 00026: val_loss improved from 24.01012 to 23.94557, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 24.2783 - val_loss: 23.9456\n",
      "Epoch 27/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 23.9429\n",
      "Epoch 00027: val_loss did not improve from 23.94557\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 23.9446 - val_loss: 24.0331\n",
      "Epoch 28/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 23.6841\n",
      "Epoch 00028: val_loss did not improve from 23.94557\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 23.6812 - val_loss: 24.0039\n",
      "Epoch 29/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 23.5864 E\n",
      "Epoch 00029: val_loss did not improve from 23.94557\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 23.5903 - val_loss: 33.7179\n",
      "Epoch 30/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 23.4123\n",
      "Epoch 00030: val_loss improved from 23.94557 to 22.00016, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 23.4122 - val_loss: 22.0002\n",
      "Epoch 31/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 23.1833\n",
      "Epoch 00031: val_loss did not improve from 22.00016\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 23.1838 - val_loss: 24.0552\n",
      "Epoch 32/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 23.0083\n",
      "Epoch 00032: val_loss did not improve from 22.00016\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 23.0075 - val_loss: 23.8418\n",
      "Epoch 33/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 22.8909\n",
      "Epoch 00033: val_loss did not improve from 22.00016\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 22.8909 - val_loss: 23.2920\n",
      "Epoch 34/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 22.7458\n",
      "Epoch 00034: val_loss improved from 22.00016 to 21.62850, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 22.7438 - val_loss: 21.6285\n",
      "Epoch 35/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 22.5486\n",
      "Epoch 00035: val_loss did not improve from 21.62850\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 22.5475 - val_loss: 22.4388\n",
      "Epoch 36/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 22.4149 ETA:  - ETA: 0s - loss: 2\n",
      "Epoch 00036: val_loss did not improve from 21.62850\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 22.4135 - val_loss: 24.0776\n",
      "Epoch 37/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 22.1960\n",
      "Epoch 00037: val_loss improved from 21.62850 to 21.27273, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 22.1956 - val_loss: 21.2727\n",
      "Epoch 38/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 22.1443\n",
      "Epoch 00038: val_loss did not improve from 21.27273\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 22.1431 - val_loss: 22.4903\n",
      "Epoch 39/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 21.9716\n",
      "Epoch 00039: val_loss did not improve from 21.27273\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 21.9694 - val_loss: 22.2615\n",
      "Epoch 40/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 21.8022\n",
      "Epoch 00040: val_loss did not improve from 21.27273\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 21.8057 - val_loss: 24.5320\n",
      "Epoch 41/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 21.8242\n",
      "Epoch 00041: val_loss improved from 21.27273 to 21.06528, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.8246 - val_loss: 21.0653\n",
      "Epoch 42/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 21.7420\n",
      "Epoch 00042: val_loss did not improve from 21.06528\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 21.7420 - val_loss: 21.8903\n",
      "Epoch 43/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 21.5657\n",
      "Epoch 00043: val_loss did not improve from 21.06528\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 21.5668 - val_loss: 22.4479\n",
      "Epoch 44/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 21.4060\n",
      "Epoch 00044: val_loss did not improve from 21.06528\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.4062 - val_loss: 23.0194\n",
      "Epoch 45/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 21.2983\n",
      "Epoch 00045: val_loss improved from 21.06528 to 20.83461, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 21.2959 - val_loss: 20.8346\n",
      "Epoch 46/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 21.2594\n",
      "Epoch 00046: val_loss did not improve from 20.83461\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.2592 - val_loss: 21.8505\n",
      "Epoch 47/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 21.0724\n",
      "Epoch 00047: val_loss did not improve from 20.83461\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.0722 - val_loss: 22.6779\n",
      "Epoch 48/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 21.0700\n",
      "Epoch 00048: val_loss did not improve from 20.83461\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.0796 - val_loss: 27.4659\n",
      "Epoch 49/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 20.8775\n",
      "Epoch 00049: val_loss did not improve from 20.83461\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 20.8775 - val_loss: 21.5084\n",
      "Epoch 50/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 20.8593\n",
      "Epoch 00050: val_loss did not improve from 20.83461\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.8585 - val_loss: 22.3447\n",
      "Epoch 51/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 20.70 - ETA: 0s - loss: 20.7054\n",
      "Epoch 00051: val_loss did not improve from 20.83461\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.7054 - val_loss: 21.6136\n",
      "Epoch 52/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 20.7140\n",
      "Epoch 00052: val_loss did not improve from 20.83461\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 20.7163 - val_loss: 21.9550\n",
      "Epoch 53/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 20.5347\n",
      "Epoch 00053: val_loss did not improve from 20.83461\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.5345 - val_loss: 21.7863\n",
      "Epoch 54/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 20.4884\n",
      "Epoch 00054: val_loss improved from 20.83461 to 20.59659, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.4869 - val_loss: 20.5966\n",
      "Epoch 55/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 20.4307\n",
      "Epoch 00055: val_loss did not improve from 20.59659\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.4316 - val_loss: 22.6477\n",
      "Epoch 56/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 20.2784\n",
      "Epoch 00056: val_loss did not improve from 20.59659\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 20.2797 - val_loss: 21.1014\n",
      "Epoch 57/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 20.3711\n",
      "Epoch 00057: val_loss did not improve from 20.59659\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.3712 - val_loss: 23.3084\n",
      "Epoch 58/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 20.2510\n",
      "Epoch 00058: val_loss improved from 20.59659 to 20.57562, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 20.2514 - val_loss: 20.5756\n",
      "Epoch 59/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 20.1087\n",
      "Epoch 00059: val_loss did not improve from 20.57562\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.1107 - val_loss: 21.8756\n",
      "Epoch 60/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 20.0469\n",
      "Epoch 00060: val_loss improved from 20.57562 to 20.47621, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 20.0447 - val_loss: 20.4762\n",
      "Epoch 61/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 19.9731\n",
      "Epoch 00061: val_loss did not improve from 20.47621\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 19.9731 - val_loss: 21.4187\n",
      "Epoch 62/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 19.8825\n",
      "Epoch 00062: val_loss improved from 20.47621 to 20.04261, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.8797 - val_loss: 20.0426\n",
      "Epoch 63/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 19.8717\n",
      "Epoch 00063: val_loss did not improve from 20.04261\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.8715 - val_loss: 20.5038\n",
      "Epoch 64/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 19.7738\n",
      "Epoch 00064: val_loss did not improve from 20.04261\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 19.7735 - val_loss: 20.7938\n",
      "Epoch 65/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 19.7210\n",
      "Epoch 00065: val_loss did not improve from 20.04261\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 19.7207 - val_loss: 20.4770\n",
      "Epoch 66/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 19.6959\n",
      "Epoch 00066: val_loss did not improve from 20.04261\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 19.6959 - val_loss: 20.1625\n",
      "Epoch 67/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 19.5583\n",
      "Epoch 00067: val_loss did not improve from 20.04261\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 19.5589 - val_loss: 20.7248\n",
      "Epoch 68/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 19.5181\n",
      "Epoch 00068: val_loss did not improve from 20.04261\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 19.5186 - val_loss: 20.8301\n",
      "Epoch 69/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 19.4706\n",
      "Epoch 00069: val_loss improved from 20.04261 to 19.76663, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.4706 - val_loss: 19.7666\n",
      "Epoch 70/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 19.5105\n",
      "Epoch 00070: val_loss improved from 19.76663 to 19.68895, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.5079 - val_loss: 19.6889\n",
      "Epoch 71/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 19.4438\n",
      "Epoch 00071: val_loss did not improve from 19.68895\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 19.4430 - val_loss: 21.3009\n",
      "Epoch 72/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 19.2596 ETA: 0s - los\n",
      "Epoch 00072: val_loss did not improve from 19.68895\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 19.2604 - val_loss: 20.0540\n",
      "Epoch 73/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 19.3369\n",
      "Epoch 00073: val_loss did not improve from 19.68895\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.3391 - val_loss: 21.0323\n",
      "Epoch 74/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 19.2582 ETA: 0s -\n",
      "Epoch 00074: val_loss did not improve from 19.68895\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.2570 - val_loss: 20.2949\n",
      "Epoch 75/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 19.1846\n",
      "Epoch 00075: val_loss did not improve from 19.68895\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 19.1805 - val_loss: 22.1767\n",
      "Epoch 76/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 19.1609\n",
      "Epoch 00076: val_loss did not improve from 19.68895\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.1609 - val_loss: 19.7753\n",
      "Epoch 77/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 19.1318\n",
      "Epoch 00077: val_loss improved from 19.68895 to 19.65237, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.1315 - val_loss: 19.6524\n",
      "Epoch 78/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 19.0284\n",
      "Epoch 00078: val_loss did not improve from 19.65237\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 19.0285 - val_loss: 20.9319\n",
      "Epoch 79/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 19.0026\n",
      "Epoch 00079: val_loss did not improve from 19.65237\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 19.0032 - val_loss: 19.7215\n",
      "Epoch 80/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 18.9919\n",
      "Epoch 00080: val_loss did not improve from 19.65237\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.9920 - val_loss: 20.3439\n",
      "Epoch 81/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 18.8864\n",
      "Epoch 00081: val_loss did not improve from 19.65237\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.8860 - val_loss: 20.5133\n",
      "Epoch 82/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 18.8842\n",
      "Epoch 00082: val_loss did not improve from 19.65237\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.8845 - val_loss: 23.8236\n",
      "Epoch 83/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 18.7929\n",
      "Epoch 00083: val_loss did not improve from 19.65237\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.7957 - val_loss: 20.5979\n",
      "Epoch 84/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 18.7979\n",
      "Epoch 00084: val_loss did not improve from 19.65237\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.7985 - val_loss: 19.7177\n",
      "Epoch 85/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 18.7520\n",
      "Epoch 00085: val_loss did not improve from 19.65237\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.7520 - val_loss: 21.1578\n",
      "Epoch 86/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 18.7164\n",
      "Epoch 00086: val_loss improved from 19.65237 to 19.41318, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.7142 - val_loss: 19.4132\n",
      "Epoch 87/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 18.6707\n",
      "Epoch 00087: val_loss did not improve from 19.41318\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.6718 - val_loss: 20.5206\n",
      "Epoch 88/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 18.6469\n",
      "Epoch 00088: val_loss did not improve from 19.41318\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.6477 - val_loss: 20.2838\n",
      "Epoch 89/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 18.6497\n",
      "Epoch 00089: val_loss improved from 19.41318 to 19.03203, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.6496 - val_loss: 19.0320\n",
      "Epoch 90/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 18.4453\n",
      "Epoch 00090: val_loss did not improve from 19.03203\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.4468 - val_loss: 20.5771\n",
      "Epoch 91/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 18.4792\n",
      "Epoch 00091: val_loss did not improve from 19.03203\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.4792 - val_loss: 19.1791\n",
      "Epoch 92/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 18.3974\n",
      "Epoch 00092: val_loss did not improve from 19.03203\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.3934 - val_loss: 19.4414\n",
      "Epoch 93/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 18.4153\n",
      "Epoch 00093: val_loss did not improve from 19.03203\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.4157 - val_loss: 20.8300\n",
      "Epoch 94/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 18.3417\n",
      "Epoch 00094: val_loss did not improve from 19.03203\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.3457 - val_loss: 29.1178\n",
      "Epoch 95/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 18.3410\n",
      "Epoch 00095: val_loss did not improve from 19.03203\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.3405 - val_loss: 19.9278\n",
      "Epoch 96/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 18.3728\n",
      "Epoch 00096: val_loss did not improve from 19.03203\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.3746 - val_loss: 19.9332\n",
      "Epoch 97/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 18.2751\n",
      "Epoch 00097: val_loss did not improve from 19.03203\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.2740 - val_loss: 19.6200\n",
      "Epoch 98/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 18.2487 ETA: 0s - l\n",
      "Epoch 00098: val_loss did not improve from 19.03203\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.2497 - val_loss: 19.3911\n",
      "Epoch 99/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 18.2197\n",
      "Epoch 00099: val_loss improved from 19.03203 to 18.94028, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.2202 - val_loss: 18.9403\n",
      "Epoch 100/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 18.1699\n",
      "Epoch 00100: val_loss did not improve from 18.94028\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 18.1693 - val_loss: 19.5494\n",
      "Epoch 101/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 18.0964\n",
      "Epoch 00101: val_loss did not improve from 18.94028\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 18.1004 - val_loss: 32.3032\n",
      "Epoch 102/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 18.1250\n",
      "Epoch 00102: val_loss did not improve from 18.94028\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.1244 - val_loss: 20.4250\n",
      "Epoch 103/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 18.0393\n",
      "Epoch 00103: val_loss did not improve from 18.94028\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.0391 - val_loss: 19.6530\n",
      "Epoch 104/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 18.0792\n",
      "Epoch 00104: val_loss did not improve from 18.94028\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.0787 - val_loss: 19.8005\n",
      "Epoch 105/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 17.9711\n",
      "Epoch 00105: val_loss did not improve from 18.94028\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.9731 - val_loss: 19.8921\n",
      "Epoch 106/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 17.9660\n",
      "Epoch 00106: val_loss did not improve from 18.94028\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.9702 - val_loss: 19.5944\n",
      "Epoch 107/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 17.9620\n",
      "Epoch 00107: val_loss did not improve from 18.94028\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.9624 - val_loss: 19.2348\n",
      "Epoch 108/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 17.9071\n",
      "Epoch 00108: val_loss did not improve from 18.94028\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.9068 - val_loss: 20.4657\n",
      "Epoch 109/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 17.8568\n",
      "Epoch 00109: val_loss improved from 18.94028 to 18.68373, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.8569 - val_loss: 18.6837\n",
      "Epoch 110/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 17.8140\n",
      "Epoch 00110: val_loss did not improve from 18.68373\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.8149 - val_loss: 19.0573\n",
      "Epoch 111/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 17.86 - ETA: 0s - loss: 17.8680\n",
      "Epoch 00111: val_loss did not improve from 18.68373\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.8682 - val_loss: 19.2971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 17.7859\n",
      "Epoch 00112: val_loss improved from 18.68373 to 18.23484, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.7867 - val_loss: 18.2348\n",
      "Epoch 113/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 17.7860 ETA: 0s - loss: 1\n",
      "Epoch 00113: val_loss did not improve from 18.23484\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 17.7859 - val_loss: 18.4696\n",
      "Epoch 114/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 17.7302\n",
      "Epoch 00114: val_loss did not improve from 18.23484\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 17.7305 - val_loss: 19.6948\n",
      "Epoch 115/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 17.70 - ETA: 0s - loss: 17.7080\n",
      "Epoch 00115: val_loss did not improve from 18.23484\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.7068 - val_loss: 19.5166\n",
      "Epoch 116/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 17.6476\n",
      "Epoch 00116: val_loss did not improve from 18.23484\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.6476 - val_loss: 19.5686\n",
      "Epoch 117/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 17.6661\n",
      "Epoch 00117: val_loss did not improve from 18.23484\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.6675 - val_loss: 18.4958\n",
      "Epoch 118/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 17.6383\n",
      "Epoch 00118: val_loss did not improve from 18.23484\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.6394 - val_loss: 18.5240\n",
      "Epoch 119/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 17.6301\n",
      "Epoch 00119: val_loss did not improve from 18.23484\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.6290 - val_loss: 18.8798\n",
      "Epoch 120/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 17.5741\n",
      "Epoch 00120: val_loss did not improve from 18.23484\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.5766 - val_loss: 20.7984\n",
      "Epoch 121/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 17.6091\n",
      "Epoch 00121: val_loss did not improve from 18.23484\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.6077 - val_loss: 19.4302\n",
      "Epoch 122/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 17.5152\n",
      "Epoch 00122: val_loss did not improve from 18.23484\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.5152 - val_loss: 19.6521\n",
      "Epoch 00122: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\add_feature_experiments\\results_1\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 18.30\n",
      "RMSE: 4.28\n",
      "CMAPSS score: 1.35\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 256)               2304      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 232,577\n",
      "Trainable params: 232,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 163.1078\n",
      "Epoch 00001: val_loss improved from inf to 46.14086, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 162.8930 - val_loss: 46.1409\n",
      "Epoch 2/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 43.9774\n",
      "Epoch 00002: val_loss improved from 46.14086 to 39.03284, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 43.9676 - val_loss: 39.0328\n",
      "Epoch 3/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 38.4310\n",
      "Epoch 00003: val_loss improved from 39.03284 to 34.45499, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.4237 - val_loss: 34.4550\n",
      "Epoch 4/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 34.7961\n",
      "Epoch 00004: val_loss improved from 34.45499 to 33.42291, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 34.7971 - val_loss: 33.4229\n",
      "Epoch 5/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 32.2967\n",
      "Epoch 00005: val_loss improved from 33.42291 to 31.67146, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 32.2959 - val_loss: 31.6715\n",
      "Epoch 6/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 30.4003\n",
      "Epoch 00006: val_loss improved from 31.67146 to 29.17585, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 30.3999 - val_loss: 29.1758\n",
      "Epoch 7/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 29.1226\n",
      "Epoch 00007: val_loss did not improve from 29.17585\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 29.1221 - val_loss: 37.1996\n",
      "Epoch 8/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 27.8799\n",
      "Epoch 00008: val_loss improved from 29.17585 to 27.66833, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 27.8752 - val_loss: 27.6683\n",
      "Epoch 9/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 27.0299\n",
      "Epoch 00009: val_loss improved from 27.66833 to 25.88799, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 27.0294 - val_loss: 25.8880\n",
      "Epoch 10/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 26.1913\n",
      "Epoch 00010: val_loss did not improve from 25.88799\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 26.1889 - val_loss: 26.9512\n",
      "Epoch 11/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 25.5480\n",
      "Epoch 00011: val_loss improved from 25.88799 to 24.28825, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 28s 5ms/step - loss: 25.5483 - val_loss: 24.2883\n",
      "Epoch 12/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 24.9834\n",
      "Epoch 00012: val_loss improved from 24.28825 to 23.61787, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 24.9805 - val_loss: 23.6179\n",
      "Epoch 13/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 24.4297\n",
      "Epoch 00013: val_loss did not improve from 23.61787\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 24.4309 - val_loss: 24.9360\n",
      "Epoch 14/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 23.9346\n",
      "Epoch 00014: val_loss did not improve from 23.61787\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 23.9336 - val_loss: 24.5862\n",
      "Epoch 15/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 23.5043\n",
      "Epoch 00015: val_loss did not improve from 23.61787\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 23.5028 - val_loss: 23.7080\n",
      "Epoch 16/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 23.1334\n",
      "Epoch 00016: val_loss improved from 23.61787 to 23.24574, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 23.1286 - val_loss: 23.2457\n",
      "Epoch 17/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 22.8065\n",
      "Epoch 00017: val_loss did not improve from 23.24574\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 22.8059 - val_loss: 23.4571\n",
      "Epoch 18/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 22.4836\n",
      "Epoch 00018: val_loss improved from 23.24574 to 22.58422, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 22.4836 - val_loss: 22.5842\n",
      "Epoch 19/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 22.2023 ETA: 0s - los\n",
      "Epoch 00019: val_loss did not improve from 22.58422\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 22.2064 - val_loss: 28.7611\n",
      "Epoch 20/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 21.8866\n",
      "Epoch 00020: val_loss improved from 22.58422 to 21.61678, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 21.8865 - val_loss: 21.6168\n",
      "Epoch 21/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 21.7034\n",
      "Epoch 00021: val_loss improved from 21.61678 to 21.61470, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.7014 - val_loss: 21.6147\n",
      "Epoch 22/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 21.4357\n",
      "Epoch 00022: val_loss improved from 21.61470 to 20.94234, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.4367 - val_loss: 20.9423\n",
      "Epoch 23/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 21.2952 ETA: 0s\n",
      "Epoch 00023: val_loss did not improve from 20.94234\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 21.2952 - val_loss: 21.7179\n",
      "Epoch 24/200\n",
      "5021/5038 [============================>.] - ETA: 0s - loss: 20.9778\n",
      "Epoch 00024: val_loss did not improve from 20.94234\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.9811 - val_loss: 21.6675\n",
      "Epoch 25/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 20.8064 ETA\n",
      "Epoch 00025: val_loss did not improve from 20.94234\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 20.8075 - val_loss: 22.3358\n",
      "Epoch 26/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 20.7269\n",
      "Epoch 00026: val_loss improved from 20.94234 to 19.72363, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.7265 - val_loss: 19.7236\n",
      "Epoch 27/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 20.4599\n",
      "Epoch 00027: val_loss did not improve from 19.72363\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 20.4600 - val_loss: 21.0362\n",
      "Epoch 28/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 20.4395\n",
      "Epoch 00028: val_loss did not improve from 19.72363\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 20.4377 - val_loss: 20.0390\n",
      "Epoch 29/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 20.2112 ETA: 0s - loss: 2\n",
      "Epoch 00029: val_loss improved from 19.72363 to 19.50041, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.2112 - val_loss: 19.5004\n",
      "Epoch 30/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 19.9305\n",
      "Epoch 00030: val_loss did not improve from 19.50041\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.9296 - val_loss: 20.1706\n",
      "Epoch 31/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 19.9550\n",
      "Epoch 00031: val_loss did not improve from 19.50041\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.9580 - val_loss: 20.2531\n",
      "Epoch 32/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 19.7065\n",
      "Epoch 00032: val_loss did not improve from 19.50041\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 19.7062 - val_loss: 20.4527\n",
      "Epoch 33/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 19.6170\n",
      "Epoch 00033: val_loss did not improve from 19.50041\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.6179 - val_loss: 21.2016\n",
      "Epoch 34/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 19.4436\n",
      "Epoch 00034: val_loss did not improve from 19.50041\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.4434 - val_loss: 19.9769\n",
      "Epoch 35/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 19.4309\n",
      "Epoch 00035: val_loss did not improve from 19.50041\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.4326 - val_loss: 19.8731\n",
      "Epoch 36/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 19.2219\n",
      "Epoch 00036: val_loss did not improve from 19.50041\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 19.2209 - val_loss: 20.0004\n",
      "Epoch 37/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 19.0737\n",
      "Epoch 00037: val_loss improved from 19.50041 to 18.64626, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.0723 - val_loss: 18.6463\n",
      "Epoch 38/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 19.0528\n",
      "Epoch 00038: val_loss did not improve from 18.64626\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.0529 - val_loss: 19.9081\n",
      "Epoch 39/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 18.9426\n",
      "Epoch 00039: val_loss improved from 18.64626 to 18.34156, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.9407 - val_loss: 18.3416\n",
      "Epoch 40/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 18.7872 ETA: 0s - l\n",
      "Epoch 00040: val_loss improved from 18.34156 to 18.13834, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.7870 - val_loss: 18.1383\n",
      "Epoch 41/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 18.7107\n",
      "Epoch 00041: val_loss did not improve from 18.13834\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.7132 - val_loss: 19.3504\n",
      "Epoch 42/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 18.6351\n",
      "Epoch 00042: val_loss did not improve from 18.13834\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.6342 - val_loss: 20.3599\n",
      "Epoch 43/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 18.4051\n",
      "Epoch 00043: val_loss did not improve from 18.13834\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.4044 - val_loss: 20.5092\n",
      "Epoch 44/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 18.4728\n",
      "Epoch 00044: val_loss did not improve from 18.13834\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.4721 - val_loss: 19.1599\n",
      "Epoch 45/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 18.3625\n",
      "Epoch 00045: val_loss did not improve from 18.13834\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.3620 - val_loss: 20.3329\n",
      "Epoch 46/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5031/5038 [============================>.] - ETA: 0s - loss: 18.3283\n",
      "Epoch 00046: val_loss did not improve from 18.13834\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.3271 - val_loss: 21.2908\n",
      "Epoch 47/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 18.1349\n",
      "Epoch 00047: val_loss did not improve from 18.13834\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.1353 - val_loss: 19.2250\n",
      "Epoch 48/200\n",
      "5021/5038 [============================>.] - ETA: 0s - loss: 18.1570\n",
      "Epoch 00048: val_loss did not improve from 18.13834\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.1531 - val_loss: 18.6704\n",
      "Epoch 49/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 18.0652\n",
      "Epoch 00049: val_loss did not improve from 18.13834\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.0636 - val_loss: 19.6955\n",
      "Epoch 50/200\n",
      "5022/5038 [============================>.] - ETA: 0s - loss: 17.9946\n",
      "Epoch 00050: val_loss improved from 18.13834 to 17.78550, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.9931 - val_loss: 17.7855\n",
      "Epoch 51/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 17.8973\n",
      "Epoch 00051: val_loss did not improve from 17.78550\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.8973 - val_loss: 17.9449\n",
      "Epoch 52/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 17.8249\n",
      "Epoch 00052: val_loss did not improve from 17.78550\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.8238 - val_loss: 19.6620\n",
      "Epoch 53/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 17.7393\n",
      "Epoch 00053: val_loss did not improve from 17.78550\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.7397 - val_loss: 18.2730\n",
      "Epoch 54/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 17.7327\n",
      "Epoch 00054: val_loss did not improve from 17.78550\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.7321 - val_loss: 18.4286\n",
      "Epoch 55/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 17.6172\n",
      "Epoch 00055: val_loss did not improve from 17.78550\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.6187 - val_loss: 20.2888\n",
      "Epoch 56/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 17.4758\n",
      "Epoch 00056: val_loss did not improve from 17.78550\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.4767 - val_loss: 19.8842\n",
      "Epoch 57/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 17.3854\n",
      "Epoch 00057: val_loss did not improve from 17.78550\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.3853 - val_loss: 18.4793\n",
      "Epoch 58/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 17.4694\n",
      "Epoch 00058: val_loss did not improve from 17.78550\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.4683 - val_loss: 20.2348\n",
      "Epoch 59/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 17.3378\n",
      "Epoch 00059: val_loss improved from 17.78550 to 17.73390, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.3366 - val_loss: 17.7339\n",
      "Epoch 60/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 17.3209\n",
      "Epoch 00060: val_loss did not improve from 17.73390\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 17.3189 - val_loss: 18.8797\n",
      "Epoch 61/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 17.3058\n",
      "Epoch 00061: val_loss did not improve from 17.73390\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.3036 - val_loss: 18.4855\n",
      "Epoch 62/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 17.1547\n",
      "Epoch 00062: val_loss did not improve from 17.73390\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.1524 - val_loss: 19.8636\n",
      "Epoch 63/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 17.1044\n",
      "Epoch 00063: val_loss did not improve from 17.73390\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.1077 - val_loss: 18.7797\n",
      "Epoch 64/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 17.0674\n",
      "Epoch 00064: val_loss improved from 17.73390 to 17.54596, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.0675 - val_loss: 17.5460\n",
      "Epoch 65/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 16.9908\n",
      "Epoch 00065: val_loss did not improve from 17.54596\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.9904 - val_loss: 17.9529\n",
      "Epoch 66/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 16.9102\n",
      "Epoch 00066: val_loss did not improve from 17.54596\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.9098 - val_loss: 17.8243\n",
      "Epoch 67/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 16.9251\n",
      "Epoch 00067: val_loss did not improve from 17.54596\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 16.9284 - val_loss: 18.5191\n",
      "Epoch 68/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 16.8398\n",
      "Epoch 00068: val_loss did not improve from 17.54596\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.8395 - val_loss: 18.8424\n",
      "Epoch 69/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 16.8250\n",
      "Epoch 00069: val_loss did not improve from 17.54596\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.8283 - val_loss: 19.0560\n",
      "Epoch 70/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 16.7836\n",
      "Epoch 00070: val_loss did not improve from 17.54596\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.7845 - val_loss: 19.0331\n",
      "Epoch 71/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 16.6994\n",
      "Epoch 00071: val_loss did not improve from 17.54596\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 16.7009 - val_loss: 18.5225\n",
      "Epoch 72/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 16.6648\n",
      "Epoch 00072: val_loss improved from 17.54596 to 16.79252, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 25s 5ms/step - loss: 16.6637 - val_loss: 16.7925\n",
      "Epoch 73/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 16.6884\n",
      "Epoch 00073: val_loss did not improve from 16.79252\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.6893 - val_loss: 18.4828\n",
      "Epoch 74/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 16.5860\n",
      "Epoch 00074: val_loss did not improve from 16.79252\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.5865 - val_loss: 17.8574\n",
      "Epoch 75/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 16.5741\n",
      "Epoch 00075: val_loss did not improve from 16.79252\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.5753 - val_loss: 17.1766\n",
      "Epoch 76/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 16.4223\n",
      "Epoch 00076: val_loss did not improve from 16.79252\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 16.4244 - val_loss: 23.0722\n",
      "Epoch 77/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 16.4861\n",
      "Epoch 00077: val_loss improved from 16.79252 to 16.78344, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.4851 - val_loss: 16.7834\n",
      "Epoch 78/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 16.3374\n",
      "Epoch 00078: val_loss did not improve from 16.78344\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 16.3374 - val_loss: 18.1718\n",
      "Epoch 79/200\n",
      "5022/5038 [============================>.] - ETA: 0s - loss: 16.2823\n",
      "Epoch 00079: val_loss did not improve from 16.78344\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 16.2831 - val_loss: 17.1248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 16.2402\n",
      "Epoch 00080: val_loss did not improve from 16.78344\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.2432 - val_loss: 18.6551\n",
      "Epoch 81/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 16.2033\n",
      "Epoch 00081: val_loss did not improve from 16.78344\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 16.2031 - val_loss: 17.8454\n",
      "Epoch 82/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 16.2642\n",
      "Epoch 00082: val_loss did not improve from 16.78344\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.2642 - val_loss: 19.0495\n",
      "Epoch 83/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 16.2182\n",
      "Epoch 00083: val_loss did not improve from 16.78344\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 16.2178 - val_loss: 17.6252\n",
      "Epoch 84/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 16.1578\n",
      "Epoch 00084: val_loss did not improve from 16.78344\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 16.1575 - val_loss: 18.5998\n",
      "Epoch 85/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 16.1154\n",
      "Epoch 00085: val_loss did not improve from 16.78344\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 16.1175 - val_loss: 17.4062\n",
      "Epoch 86/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 16.1094\n",
      "Epoch 00086: val_loss did not improve from 16.78344\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 16.1097 - val_loss: 17.0033\n",
      "Epoch 87/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 16.0861\n",
      "Epoch 00087: val_loss did not improve from 16.78344\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 16.0844 - val_loss: 17.3657\n",
      "Epoch 00087: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\add_feature_experiments\\results_2\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 16.83\n",
      "RMSE: 4.10\n",
      "CMAPSS score: 1.32\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 256)               2304      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 232,577\n",
      "Trainable params: 232,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5024/5038 [============================>.] - ETA: 0s - loss: 165.4922- ETA: 0s - loss: 16\n",
      "Epoch 00001: val_loss improved from inf to 46.02284, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 165.1793 - val_loss: 46.0228\n",
      "Epoch 2/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 44.4643\n",
      "Epoch 00002: val_loss improved from 46.02284 to 41.40693, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 44.4643 - val_loss: 41.4069\n",
      "Epoch 3/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 38.5861\n",
      "Epoch 00003: val_loss improved from 41.40693 to 34.54729, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 38.5779 - val_loss: 34.5473\n",
      "Epoch 4/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 34.7147\n",
      "Epoch 00004: val_loss improved from 34.54729 to 32.63125, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 34.7126 - val_loss: 32.6312\n",
      "Epoch 5/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 32.1708\n",
      "Epoch 00005: val_loss did not improve from 32.63125\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 32.1679 - val_loss: 32.7551\n",
      "Epoch 6/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 30.3829\n",
      "Epoch 00006: val_loss improved from 32.63125 to 29.91228, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 30.3834 - val_loss: 29.9123\n",
      "Epoch 7/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 29.10 - ETA: 0s - loss: 29.1134\n",
      "Epoch 00007: val_loss improved from 29.91228 to 29.65935, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 29.1149 - val_loss: 29.6594\n",
      "Epoch 8/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 28.0553\n",
      "Epoch 00008: val_loss did not improve from 29.65935\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 28.0534 - val_loss: 30.1244\n",
      "Epoch 9/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 27.2070\n",
      "Epoch 00009: val_loss improved from 29.65935 to 26.67982, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 27.2054 - val_loss: 26.6798\n",
      "Epoch 10/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 26.4764\n",
      "Epoch 00010: val_loss did not improve from 26.67982\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 26.4781 - val_loss: 29.6943\n",
      "Epoch 11/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 25.7448\n",
      "Epoch 00011: val_loss improved from 26.67982 to 25.03261, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 25.7445 - val_loss: 25.0326\n",
      "Epoch 12/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 25.2728\n",
      "Epoch 00012: val_loss did not improve from 25.03261\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 25.2695 - val_loss: 26.1942\n",
      "Epoch 13/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 24.7486\n",
      "Epoch 00013: val_loss did not improve from 25.03261\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 24.7486 - val_loss: 25.2121\n",
      "Epoch 14/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 24.3764\n",
      "Epoch 00014: val_loss improved from 25.03261 to 24.52034, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 24.3748 - val_loss: 24.5203\n",
      "Epoch 15/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 23.9070\n",
      "Epoch 00015: val_loss improved from 24.52034 to 23.14807, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 23.9033 - val_loss: 23.1481\n",
      "Epoch 16/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 23.5291\n",
      "Epoch 00016: val_loss did not improve from 23.14807\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 23.5288 - val_loss: 23.5553\n",
      "Epoch 17/200\n",
      "5022/5038 [============================>.] - ETA: 0s - loss: 23.1777\n",
      "Epoch 00017: val_loss did not improve from 23.14807\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 23.1743 - val_loss: 23.4584\n",
      "Epoch 18/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 22.8083\n",
      "Epoch 00018: val_loss improved from 23.14807 to 22.57311, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5038/5038 [==============================] - 24s 5ms/step - loss: 22.8092 - val_loss: 22.5731\n",
      "Epoch 19/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 22.4702\n",
      "Epoch 00019: val_loss did not improve from 22.57311\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 22.4809 - val_loss: 29.1695\n",
      "Epoch 20/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 22.2030\n",
      "Epoch 00020: val_loss improved from 22.57311 to 22.16662, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 22.2060 - val_loss: 22.1666\n",
      "Epoch 21/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 22.0278\n",
      "Epoch 00021: val_loss improved from 22.16662 to 21.86615, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 22.0243 - val_loss: 21.8662\n",
      "Epoch 22/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 21.7486\n",
      "Epoch 00022: val_loss did not improve from 21.86615\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.7490 - val_loss: 22.6469\n",
      "Epoch 23/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 21.4646\n",
      "Epoch 00023: val_loss improved from 21.86615 to 21.47985, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.4652 - val_loss: 21.4798\n",
      "Epoch 24/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 21.1793\n",
      "Epoch 00024: val_loss improved from 21.47985 to 21.17666, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.1805 - val_loss: 21.1767\n",
      "Epoch 25/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 21.0519\n",
      "Epoch 00025: val_loss did not improve from 21.17666\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 21.0522 - val_loss: 21.6789\n",
      "Epoch 26/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 20.8404\n",
      "Epoch 00026: val_loss improved from 21.17666 to 20.33232, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.8401 - val_loss: 20.3323\n",
      "Epoch 27/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 20.6854\n",
      "Epoch 00027: val_loss did not improve from 20.33232\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.6863 - val_loss: 20.6785\n",
      "Epoch 28/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 20.6143\n",
      "Epoch 00028: val_loss did not improve from 20.33232\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.6145 - val_loss: 20.5303\n",
      "Epoch 29/200\n",
      "5037/5038 [============================>.] - ETA: 0s - loss: 20.3762 ETA: 0s - loss:\n",
      "Epoch 00029: val_loss did not improve from 20.33232\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.3762 - val_loss: 21.2336\n",
      "Epoch 30/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 20.2019\n",
      "Epoch 00030: val_loss improved from 20.33232 to 19.82676, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.2036 - val_loss: 19.8268\n",
      "Epoch 31/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 20.0155\n",
      "Epoch 00031: val_loss did not improve from 19.82676\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 20.0202 - val_loss: 19.8599\n",
      "Epoch 32/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 19.8797\n",
      "Epoch 00032: val_loss did not improve from 19.82676\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.8800 - val_loss: 20.9016\n",
      "Epoch 33/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 19.6935\n",
      "Epoch 00033: val_loss did not improve from 19.82676\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.6964 - val_loss: 21.2703\n",
      "Epoch 34/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 19.6554\n",
      "Epoch 00034: val_loss did not improve from 19.82676\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.6568 - val_loss: 20.9929\n",
      "Epoch 35/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 19.3246\n",
      "Epoch 00035: val_loss did not improve from 19.82676\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.3261 - val_loss: 25.7157\n",
      "Epoch 36/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 19.3630\n",
      "Epoch 00036: val_loss improved from 19.82676 to 19.58566, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.3624 - val_loss: 19.5857\n",
      "Epoch 37/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 19.1745\n",
      "Epoch 00037: val_loss did not improve from 19.58566\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 19.1731 - val_loss: 20.8926\n",
      "Epoch 38/200\n",
      "5022/5038 [============================>.] - ETA: 0s - loss: 18.9854\n",
      "Epoch 00038: val_loss did not improve from 19.58566\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.9845 - val_loss: 20.9064\n",
      "Epoch 39/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 18.8808 ETA: 0s - loss: 18. - ETA: 0s - l\n",
      "Epoch 00039: val_loss did not improve from 19.58566\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.8808 - val_loss: 21.3308\n",
      "Epoch 40/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 18.6006\n",
      "Epoch 00040: val_loss did not improve from 19.58566\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.6006 - val_loss: 22.1447\n",
      "Epoch 41/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 18.40 - ETA: 0s - loss: 18.4059\n",
      "Epoch 00041: val_loss improved from 19.58566 to 18.43277, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 18.4068 - val_loss: 18.4328\n",
      "Epoch 42/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 18.2381\n",
      "Epoch 00042: val_loss improved from 18.43277 to 18.25394, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 18.2394 - val_loss: 18.2539\n",
      "Epoch 43/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 17.9678\n",
      "Epoch 00043: val_loss did not improve from 18.25394\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.9685 - val_loss: 19.7647\n",
      "Epoch 44/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 17.7721\n",
      "Epoch 00044: val_loss did not improve from 18.25394\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.7738 - val_loss: 20.1511\n",
      "Epoch 45/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 17.6061 ETA:  -\n",
      "Epoch 00045: val_loss improved from 18.25394 to 17.72305, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 17.6042 - val_loss: 17.7230\n",
      "Epoch 46/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 17.5686\n",
      "Epoch 00046: val_loss did not improve from 17.72305\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.5654 - val_loss: 19.0638\n",
      "Epoch 47/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 17.3685\n",
      "Epoch 00047: val_loss did not improve from 17.72305\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.3688 - val_loss: 18.4945\n",
      "Epoch 48/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 17.2705\n",
      "Epoch 00048: val_loss did not improve from 17.72305\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.2705 - val_loss: 18.6915\n",
      "Epoch 49/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 17.0584 ETA: 0s - loss:\n",
      "Epoch 00049: val_loss did not improve from 17.72305\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 17.0567 - val_loss: 17.7906\n",
      "Epoch 50/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 16.9515 ETA: 0s - loss: 16.\n",
      "Epoch 00050: val_loss improved from 17.72305 to 17.39329, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.9523 - val_loss: 17.3933\n",
      "Epoch 51/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 16.7743\n",
      "Epoch 00051: val_loss improved from 17.39329 to 16.85486, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.7745 - val_loss: 16.8549\n",
      "Epoch 52/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 16.8157\n",
      "Epoch 00052: val_loss improved from 16.85486 to 16.72479, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.8160 - val_loss: 16.7248\n",
      "Epoch 53/200\n",
      "5031/5038 [============================>.] - ETA: 0s - loss: 16.5577\n",
      "Epoch 00053: val_loss did not improve from 16.72479\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.5579 - val_loss: 17.7964\n",
      "Epoch 54/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 16.4333\n",
      "Epoch 00054: val_loss did not improve from 16.72479\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.4327 - val_loss: 20.0636\n",
      "Epoch 55/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 16.5170 ETA: 0s\n",
      "Epoch 00055: val_loss did not improve from 16.72479\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.5170 - val_loss: 16.8149\n",
      "Epoch 56/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 16.2875\n",
      "Epoch 00056: val_loss did not improve from 16.72479\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.2924 - val_loss: 17.5024\n",
      "Epoch 57/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 16.2113\n",
      "Epoch 00057: val_loss did not improve from 16.72479\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.2114 - val_loss: 18.4457\n",
      "Epoch 58/200\n",
      "5023/5038 [============================>.] - ETA: 0s - loss: 16.1680\n",
      "Epoch 00058: val_loss did not improve from 16.72479\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.1640 - val_loss: 16.9563\n",
      "Epoch 59/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 16.0111 ETA: \n",
      "Epoch 00059: val_loss did not improve from 16.72479\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 16.0119 - val_loss: 16.9304\n",
      "Epoch 60/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 15.9469\n",
      "Epoch 00060: val_loss improved from 16.72479 to 16.26010, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.9443 - val_loss: 16.2601\n",
      "Epoch 61/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 15.8869\n",
      "Epoch 00061: val_loss did not improve from 16.26010\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.8863 - val_loss: 18.0054\n",
      "Epoch 62/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 15.8503\n",
      "Epoch 00062: val_loss improved from 16.26010 to 15.87667, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.8478 - val_loss: 15.8767\n",
      "Epoch 63/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 15.6592\n",
      "Epoch 00063: val_loss did not improve from 15.87667\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.6641 - val_loss: 17.5089\n",
      "Epoch 64/200\n",
      "5021/5038 [============================>.] - ETA: 0s - loss: 15.6185\n",
      "Epoch 00064: val_loss improved from 15.87667 to 15.34548, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.6158 - val_loss: 15.3455\n",
      "Epoch 65/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 15.5226\n",
      "Epoch 00065: val_loss did not improve from 15.34548\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.5217 - val_loss: 19.4745\n",
      "Epoch 66/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 15.6058\n",
      "Epoch 00066: val_loss did not improve from 15.34548\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.6053 - val_loss: 15.8941\n",
      "Epoch 67/200\n",
      "5038/5038 [==============================] - ETA: 0s - loss: 15.4866\n",
      "Epoch 00067: val_loss did not improve from 15.34548\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.4866 - val_loss: 17.3638\n",
      "Epoch 68/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 15.2703\n",
      "Epoch 00068: val_loss did not improve from 15.34548\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.2710 - val_loss: 16.4356\n",
      "Epoch 69/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 15.2699 ETA: \n",
      "Epoch 00069: val_loss did not improve from 15.34548\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.2769 - val_loss: 16.2125\n",
      "Epoch 70/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 15.1671\n",
      "Epoch 00070: val_loss did not improve from 15.34548\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.1658 - val_loss: 15.5659\n",
      "Epoch 71/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 15.2472\n",
      "Epoch 00071: val_loss did not improve from 15.34548\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 15.2473 - val_loss: 16.0171\n",
      "Epoch 72/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 15.0877\n",
      "Epoch 00072: val_loss did not improve from 15.34548\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.0885 - val_loss: 16.5409\n",
      "Epoch 73/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 15.0046\n",
      "Epoch 00073: val_loss did not improve from 15.34548\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.0048 - val_loss: 16.4265\n",
      "Epoch 74/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 15.0042\n",
      "Epoch 00074: val_loss improved from 15.34548 to 15.19125, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 15.0043 - val_loss: 15.1912\n",
      "Epoch 75/200\n",
      "5029/5038 [============================>.] - ETA: 0s - loss: 14.9675\n",
      "Epoch 00075: val_loss did not improve from 15.19125\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.9645 - val_loss: 16.5608\n",
      "Epoch 76/200\n",
      "5028/5038 [============================>.] - ETA: 0s - loss: 14.8654\n",
      "Epoch 00076: val_loss did not improve from 15.19125\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.8691 - val_loss: 17.5404\n",
      "Epoch 77/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 14.8377\n",
      "Epoch 00077: val_loss did not improve from 15.19125\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 14.8374 - val_loss: 16.4757\n",
      "Epoch 78/200\n",
      "5036/5038 [============================>.] - ETA: 0s - loss: 14.7677\n",
      "Epoch 00078: val_loss did not improve from 15.19125\n",
      "5038/5038 [==============================] - 22s 4ms/step - loss: 14.7677 - val_loss: 17.3323\n",
      "Epoch 79/200\n",
      "5025/5038 [============================>.] - ETA: 0s - loss: 14.7469\n",
      "Epoch 00079: val_loss did not improve from 15.19125\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.7469 - val_loss: 17.6296\n",
      "Epoch 80/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 14.5795\n",
      "Epoch 00080: val_loss did not improve from 15.19125\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.5871 - val_loss: 21.6954\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5037/5038 [============================>.] - ETA: 0s - loss: 14.5608\n",
      "Epoch 00081: val_loss improved from 15.19125 to 14.75358, saving model to DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\mlp_model_trained.h5\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.5606 - val_loss: 14.7536\n",
      "Epoch 82/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 14.4760\n",
      "Epoch 00082: val_loss did not improve from 14.75358\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.4773 - val_loss: 16.0268\n",
      "Epoch 83/200\n",
      "5033/5038 [============================>.] - ETA: 0s - loss: 14.4978\n",
      "Epoch 00083: val_loss did not improve from 14.75358\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.4965 - val_loss: 15.5978\n",
      "Epoch 84/200\n",
      "5026/5038 [============================>.] - ETA: 0s - loss: 14.4018\n",
      "Epoch 00084: val_loss did not improve from 14.75358\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.4036 - val_loss: 15.9980\n",
      "Epoch 85/200\n",
      "5034/5038 [============================>.] - ETA: 0s - loss: 14.3750\n",
      "Epoch 00085: val_loss did not improve from 14.75358\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.3742 - val_loss: 16.2697\n",
      "Epoch 86/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 14.3439\n",
      "Epoch 00086: val_loss did not improve from 14.75358\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.3440 - val_loss: 14.8927\n",
      "Epoch 87/200\n",
      "5030/5038 [============================>.] - ETA: 0s - loss: 14.3225\n",
      "Epoch 00087: val_loss did not improve from 14.75358\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.3230 - val_loss: 16.7911\n",
      "Epoch 88/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 14.2449\n",
      "Epoch 00088: val_loss did not improve from 14.75358\n",
      "5038/5038 [==============================] - 24s 5ms/step - loss: 14.2500 - val_loss: 15.9826\n",
      "Epoch 89/200\n",
      "5027/5038 [============================>.] - ETA: 0s - loss: 14.1760\n",
      "Epoch 00089: val_loss did not improve from 14.75358\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.1759 - val_loss: 16.0413\n",
      "Epoch 90/200\n",
      "5035/5038 [============================>.] - ETA: 0s - loss: 14.2005\n",
      "Epoch 00090: val_loss did not improve from 14.75358\n",
      "5038/5038 [==============================] - 23s 5ms/step - loss: 14.2018 - val_loss: 15.0578\n",
      "Epoch 91/200\n",
      "5032/5038 [============================>.] - ETA: 0s - loss: 14.2455\n",
      "Epoch 00091: val_loss did not improve from 14.75358\n",
      "5038/5038 [==============================] - 23s 4ms/step - loss: 14.2452 - val_loss: 16.4542\n",
      "Epoch 00091: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\add_feature_experiments\\results_3\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 14.71\n",
      "RMSE: 3.84\n",
      "CMAPSS score: 1.29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# Test effect of adding/removing certain features\n",
    "#################################################\n",
    "NUM_TRIALS = 1\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "layer_sizes = [256, 256, 512, 64]\n",
    "\n",
    "results_file = os.path.join(output_path, \"results.csv\")\n",
    "with open(results_file, \"w\") as file:\n",
    "    file.write(\"experiment_num,selected_features,num_features,mse(mean),mse(std),rmse(mean),rmse(std),cmapss(mean),cmapss(std)\\n\")\n",
    "\n",
    "for idx in range(len(selected_columns_list)):\n",
    "    selected_columns = selected_columns_list[idx]\n",
    "    x_train_feature_selection = x_train[selected_columns]\n",
    "    \n",
    "    mse_vals = []\n",
    "    rmse_vals = []\n",
    "    cmapss_vals = []\n",
    "    \n",
    "    for random_seed in range(NUM_TRIALS):\n",
    "        # Train-validation split for early stopping\n",
    "        x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train_feature_selection, \n",
    "                                                                                  y_train, \n",
    "                                                                                  test_size=0.3, \n",
    "                                                                                  random_state=random_seed)\n",
    "        # Create output path\n",
    "        results_path_crr_th = os.path.join(output_path, \"add_feature_experiments\", \n",
    "                                           \"results_{}\".format(idx))\n",
    "        results_path_crr_split = os.path.join(results_path_crr_th, \"split_{}\".format(random_seed))\n",
    "        if not os.path.exists(results_path_crr_split):\n",
    "            os.makedirs(results_path_crr_split)\n",
    "\n",
    "        # Standardization\n",
    "        scaler = StandardScaler()\n",
    "        x_train_scaled = scaler.fit_transform(x_train_split)\n",
    "        x_val_scaled = scaler.transform(x_val_split)\n",
    "        input_dim = x_train_scaled.shape[1]\n",
    "\n",
    "        # Create model\n",
    "        weights_file = os.path.join(results_path_crr_th, 'mlp_initial_weights.h5')\n",
    "        model_path = os.path.join(results_path_crr_split, 'mlp_model_trained.h5')\n",
    "        \n",
    "        if random_seed == 0:\n",
    "            model = create_mlp_model(input_dim, layer_sizes, activation='tanh', \n",
    "                                     output_weights_file=weights_file)\n",
    "        else:\n",
    "            model = create_mlp_model(input_dim, layer_sizes, activation='tanh')\n",
    "        model.summary()\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint(model_path, monitor='val_loss', mode='min', verbose=2, \n",
    "                             save_best_only=True)\n",
    "\n",
    "        # Train model\n",
    "        history = train_model_existing_weights(model, weights_file, \n",
    "                                               x_train_scaled, y_train_split, \n",
    "                                               x_val_scaled, y_val_split, \n",
    "                                               batch_size=batch_size, \n",
    "                                               epochs=epochs, \n",
    "                                               callbacks=[es, mc])\n",
    "\n",
    "        history_file = os.path.join(results_path_crr_split, \"history.pkl\")\n",
    "        save_history(history, history_file)\n",
    "\n",
    "        # Performance evaluation\n",
    "        x_test_feature_selection = x_test[selected_columns]\n",
    "        x_test_scaled = scaler.transform(x_test_feature_selection)\n",
    "\n",
    "        loaded_model = load_model(model_path)\n",
    "        predictions_test = loaded_model.predict(x_test_scaled).flatten()\n",
    "        mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)\n",
    "        \n",
    "        mse_vals.append(mse)\n",
    "        rmse_vals.append(rmse)\n",
    "        cmapss_vals.append(cmapss_score)\n",
    "    \n",
    "    mse_mean = np.mean(mse_vals)\n",
    "    mse_std = np.std(mse_vals)\n",
    "    rmse_mean = np.mean(rmse_vals)\n",
    "    rmse_std = np.std(rmse_vals)\n",
    "    cmapss_mean = np.mean(cmapss_vals)\n",
    "    cmapss_std = np.std(cmapss_vals)\n",
    "    \n",
    "    with open(results_file, \"a\") as file:\n",
    "        file.write(f\"{idx}, {feature_list_to_string(selected_columns)}, {len(selected_columns)}, {mse_mean}, {mse_std}, {rmse_mean}, {rmse_std}, {cmapss_mean}, {cmapss_std}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "MSE: 13.92\n",
      "RMSE: 3.73\n",
      "CMAPSS score: 1.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = r'D:\\Licenta\\notebooks\\DS02\\experiment_set_8\\corr_th_experiments\\results_0.99\\split_0\\mlp_model_trained.h5'\n",
    "selected_columns = ['T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P24', 'Ps30', 'Nf', 'Wf', \n",
    "                    'SmFan', 'SmLPC', 'SmHPC', 'Mach', 'TRA']\n",
    "\n",
    "x_train_feature_selection = x_train[selected_columns]\n",
    "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train_feature_selection, \n",
    "                                                                              y_train, \n",
    "                                                                              test_size=0.3, \n",
    "                                                                              random_state=seed)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_split)\n",
    "x_val_scaled = scaler.transform(x_val_split)\n",
    "    \n",
    "\n",
    "x_test_feature_selection = x_test[selected_columns]\n",
    "x_test_scaled = scaler.transform(x_test_feature_selection)\n",
    "\n",
    "loaded_model = load_model(model_path)\n",
    "predictions_test = loaded_model.predict(x_test_scaled).flatten()\n",
    "mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set['RUL']\n",
    "x_train = train_set.drop(['RUL'], axis=1)\n",
    "\n",
    "y_test = test_set['RUL']\n",
    "x_test = test_set.drop(['RUL'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "MSE: 70.72\n",
      "RMSE: 8.41\n",
      "CMAPSS score: 1.87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = r'D:\\Licenta\\notebooks\\DS02\\experiment_set_8\\corr_th_experiments\\results_0.99\\split_0\\mlp_model_trained.h5'\n",
    "selected_columns = ['T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P24', 'Ps30', 'Nf', 'Wf', \n",
    "                    'SmFan', 'SmLPC', 'SmHPC', 'Mach', 'TRA']\n",
    "\n",
    "x_train_feature_selection = x_train[selected_columns]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_feature_selection)\n",
    "\n",
    "x_test_feature_selection = x_test[selected_columns]\n",
    "x_test_scaled = scaler.transform(x_test_feature_selection)\n",
    "    \n",
    "loaded_model = load_model(model_path)\n",
    "predictions_test = loaded_model.predict(x_test_scaled).flatten()\n",
    "mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# Train with best features on entire training set\n",
    "#################################################\n",
    "selected_columns = ['T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P24', 'Ps30', 'Nf', 'Wf', \n",
    "                    'SmFan', 'SmLPC', 'SmHPC', 'Mach', 'TRA']\n",
    "\n",
    "y_train = train_set['RUL']\n",
    "x_train = train_set[selected_columns]\n",
    "\n",
    "y_test = test_set['RUL']\n",
    "x_test = test_set[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               4096      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 234,369\n",
      "Trainable params: 234,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "7187/7197 [============================>.] - ETA: 0s - loss: 129.3073\n",
      "Epoch 00001: val_loss improved from inf to 56.30443, saving model to DS02/experiment_set_8\\results_whole_training_set\\mlp_model_trained.h5\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 129.2036 - val_loss: 56.3044\n",
      "Epoch 2/200\n",
      "7184/7197 [============================>.] - ETA: 0s - loss: 40.1691\n",
      "Epoch 00002: val_loss improved from 56.30443 to 44.73309, saving model to DS02/experiment_set_8\\results_whole_training_set\\mlp_model_trained.h5\n",
      "7197/7197 [==============================] - 35s 5ms/step - loss: 40.1615 - val_loss: 44.7331\n",
      "Epoch 3/200\n",
      "7187/7197 [============================>.] - ETA: 0s - loss: 34.0714\n",
      "Epoch 00003: val_loss improved from 44.73309 to 29.99361, saving model to DS02/experiment_set_8\\results_whole_training_set\\mlp_model_trained.h5\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 34.0683 - val_loss: 29.9936\n",
      "Epoch 4/200\n",
      "7191/7197 [============================>.] - ETA: 0s - loss: 30.5607\n",
      "Epoch 00004: val_loss improved from 29.99361 to 29.00677, saving model to DS02/experiment_set_8\\results_whole_training_set\\mlp_model_trained.h5\n",
      "7197/7197 [==============================] - 35s 5ms/step - loss: 30.5595 - val_loss: 29.0068\n",
      "Epoch 5/200\n",
      "7187/7197 [============================>.] - ETA: 0s - loss: 27.9161\n",
      "Epoch 00005: val_loss did not improve from 29.00677\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 27.9164 - val_loss: 29.5964\n",
      "Epoch 6/200\n",
      "7195/7197 [============================>.] - ETA: 0s - loss: 25.8850\n",
      "Epoch 00006: val_loss improved from 29.00677 to 26.44315, saving model to DS02/experiment_set_8\\results_whole_training_set\\mlp_model_trained.h5\n",
      "7197/7197 [==============================] - 35s 5ms/step - loss: 25.8847 - val_loss: 26.4432\n",
      "Epoch 7/200\n",
      "7183/7197 [============================>.] - ETA: 0s - loss: 24.2438\n",
      "Epoch 00007: val_loss improved from 26.44315 to 24.26231, saving model to DS02/experiment_set_8\\results_whole_training_set\\mlp_model_trained.h5\n",
      "7197/7197 [==============================] - 35s 5ms/step - loss: 24.2436 - val_loss: 24.2623\n",
      "Epoch 8/200\n",
      "7191/7197 [============================>.] - ETA: 0s - loss: 22.8778\n",
      "Epoch 00008: val_loss improved from 24.26231 to 20.01340, saving model to DS02/experiment_set_8\\results_whole_training_set\\mlp_model_trained.h5\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 22.8771 - val_loss: 20.0134\n",
      "Epoch 9/200\n",
      "7194/7197 [============================>.] - ETA: 0s - loss: 21.8792\n",
      "Epoch 00009: val_loss did not improve from 20.01340\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 21.8783 - val_loss: 23.0546\n",
      "Epoch 10/200\n",
      "7189/7197 [============================>.] - ETA: 0s - loss: 21.2211\n",
      "Epoch 00010: val_loss did not improve from 20.01340\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 21.2215 - val_loss: 31.2151\n",
      "Epoch 11/200\n",
      "7190/7197 [============================>.] - ETA: 0s - loss: 20.4337\n",
      "Epoch 00011: val_loss did not improve from 20.01340\n",
      "7197/7197 [==============================] - 35s 5ms/step - loss: 20.4341 - val_loss: 21.5218\n",
      "Epoch 12/200\n",
      "7195/7197 [============================>.] - ETA: 0s - loss: 19.7260\n",
      "Epoch 00012: val_loss improved from 20.01340 to 18.24324, saving model to DS02/experiment_set_8\\results_whole_training_set\\mlp_model_trained.h5\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 19.7265 - val_loss: 18.2432\n",
      "Epoch 13/200\n",
      "7184/7197 [============================>.] - ETA: 0s - loss: 19.1748\n",
      "Epoch 00013: val_loss did not improve from 18.24324\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 19.1740 - val_loss: 18.3690\n",
      "Epoch 14/200\n",
      "7191/7197 [============================>.] - ETA: 0s - loss: 18.7619\n",
      "Epoch 00014: val_loss did not improve from 18.24324\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 18.7624 - val_loss: 18.4175\n",
      "Epoch 15/200\n",
      "7194/7197 [============================>.] - ETA: 0s - loss: 18.3263\n",
      "Epoch 00015: val_loss did not improve from 18.24324\n",
      "7197/7197 [==============================] - 35s 5ms/step - loss: 18.3259 - val_loss: 19.3290\n",
      "Epoch 16/200\n",
      "7190/7197 [============================>.] - ETA: 0s - loss: 18.2066\n",
      "Epoch 00016: val_loss did not improve from 18.24324\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 18.2060 - val_loss: 21.2750\n",
      "Epoch 17/200\n",
      "7186/7197 [============================>.] - ETA: 0s - loss: 17.8201\n",
      "Epoch 00017: val_loss improved from 18.24324 to 17.24383, saving model to DS02/experiment_set_8\\results_whole_training_set\\mlp_model_trained.h5\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 17.8164 - val_loss: 17.2438\n",
      "Epoch 18/200\n",
      "7184/7197 [============================>.] - ETA: 0s - loss: 17.3967\n",
      "Epoch 00018: val_loss did not improve from 17.24383\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 17.4003 - val_loss: 39.1597\n",
      "Epoch 19/200\n",
      "7189/7197 [============================>.] - ETA: 0s - loss: 17.2368\n",
      "Epoch 00019: val_loss improved from 17.24383 to 16.08634, saving model to DS02/experiment_set_8\\results_whole_training_set\\mlp_model_trained.h5\n",
      "7197/7197 [==============================] - 35s 5ms/step - loss: 17.2356 - val_loss: 16.0863\n",
      "Epoch 20/200\n",
      "7189/7197 [============================>.] - ETA: 0s - loss: 16.8708\n",
      "Epoch 00020: val_loss did not improve from 16.08634\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 16.8689 - val_loss: 16.3155\n",
      "Epoch 21/200\n",
      "7189/7197 [============================>.] - ETA: 0s - loss: 16.5604\n",
      "Epoch 00021: val_loss did not improve from 16.08634\n",
      "7197/7197 [==============================] - 35s 5ms/step - loss: 16.5596 - val_loss: 18.1954\n",
      "Epoch 22/200\n",
      "7189/7197 [============================>.] - ETA: 0s - loss: 16.4486\n",
      "Epoch 00022: val_loss did not improve from 16.08634\n",
      "7197/7197 [==============================] - 35s 5ms/step - loss: 16.4468 - val_loss: 17.6557\n",
      "Epoch 23/200\n",
      "7192/7197 [============================>.] - ETA: 0s - loss: 16.2131\n",
      "Epoch 00023: val_loss did not improve from 16.08634\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 16.2149 - val_loss: 20.3996\n",
      "Epoch 24/200\n",
      "7186/7197 [============================>.] - ETA: 0s - loss: 16.0121\n",
      "Epoch 00024: val_loss did not improve from 16.08634\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 16.0150 - val_loss: 19.6414\n",
      "Epoch 25/200\n",
      "7192/7197 [============================>.] - ETA: 0s - loss: 15.8021\n",
      "Epoch 00025: val_loss did not improve from 16.08634\n",
      "7197/7197 [==============================] - 33s 5ms/step - loss: 15.8016 - val_loss: 18.5342\n",
      "Epoch 26/200\n",
      "7195/7197 [============================>.] - ETA: 0s - loss: 15.6618\n",
      "Epoch 00026: val_loss did not improve from 16.08634\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 15.6617 - val_loss: 19.7128\n",
      "Epoch 27/200\n",
      "7187/7197 [============================>.] - ETA: 0s - loss: 15.5268\n",
      "Epoch 00027: val_loss improved from 16.08634 to 14.08241, saving model to DS02/experiment_set_8\\results_whole_training_set\\mlp_model_trained.h5\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 15.5244 - val_loss: 14.0824\n",
      "Epoch 28/200\n",
      "7188/7197 [============================>.] - ETA: 0s - loss: 15.2865\n",
      "Epoch 00028: val_loss did not improve from 14.08241\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 15.2865 - val_loss: 14.5539\n",
      "Epoch 29/200\n",
      "7183/7197 [============================>.] - ETA: 0s - loss: 15.2836\n",
      "Epoch 00029: val_loss did not improve from 14.08241\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 15.2815 - val_loss: 15.3919\n",
      "Epoch 30/200\n",
      "7183/7197 [============================>.] - ETA: 0s - loss: 15.0631\n",
      "Epoch 00030: val_loss did not improve from 14.08241\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 15.0611 - val_loss: 14.6314\n",
      "Epoch 31/200\n",
      "7190/7197 [============================>.] - ETA: 0s - loss: 14.8212\n",
      "Epoch 00031: val_loss did not improve from 14.08241\n",
      "7197/7197 [==============================] - 33s 5ms/step - loss: 14.8234 - val_loss: 16.5138\n",
      "Epoch 32/200\n",
      "7196/7197 [============================>.] - ETA: 0s - loss: 14.7844\n",
      "Epoch 00032: val_loss did not improve from 14.08241\n",
      "7197/7197 [==============================] - 35s 5ms/step - loss: 14.7845 - val_loss: 15.3868\n",
      "Epoch 33/200\n",
      "7189/7197 [============================>.] - ETA: 0s - loss: 14.6188\n",
      "Epoch 00033: val_loss did not improve from 14.08241\n",
      "7197/7197 [==============================] - 35s 5ms/step - loss: 14.6195 - val_loss: 17.0778\n",
      "Epoch 34/200\n",
      "7194/7197 [============================>.] - ETA: 0s - loss: 14.4819\n",
      "Epoch 00034: val_loss did not improve from 14.08241\n",
      "7197/7197 [==============================] - 33s 5ms/step - loss: 14.4813 - val_loss: 49.9044\n",
      "Epoch 35/200\n",
      "7182/7197 [============================>.] - ETA: 0s - loss: 14.4568\n",
      "Epoch 00035: val_loss did not improve from 14.08241\n",
      "7197/7197 [==============================] - 34s 5ms/step - loss: 14.4555 - val_loss: 14.4631\n",
      "Epoch 36/200\n",
      "7186/7197 [============================>.] - ETA: 0s - loss: 14.1798\n",
      "Epoch 00036: val_loss did not improve from 14.08241\n",
      "7197/7197 [==============================] - 33s 5ms/step - loss: 14.1801 - val_loss: 14.8137\n",
      "Epoch 37/200\n",
      "7191/7197 [============================>.] - ETA: 0s - loss: 14.2359\n",
      "Epoch 00037: val_loss did not improve from 14.08241\n",
      "7197/7197 [==============================] - 33s 5ms/step - loss: 14.2370 - val_loss: 15.3516\n",
      "Epoch 00037: early stopping\n",
      "Saved training history to file: DS02/experiment_set_8\\results_whole_training_set\\history.pkl\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "MSE: 70.02\n",
      "RMSE: 8.37\n",
      "CMAPSS score: 1.86\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c5c560250e84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[0mcmapss_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmapss_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcmapss_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{idx}, {feature_list_to_string(selected_columns)}, {len(selected_columns)}, {mse_mean}, {mse_std}, {rmse_mean}, {rmse_std}, {cmapss_mean}, {cmapss_std}\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'idx' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 200\n",
    "layer_sizes = [256, 256, 512, 64]\n",
    "\n",
    "results_path = os.path.join(output_path, \"results_whole_training_set\")\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "\n",
    "results_file = os.path.join(results_path, \"results.csv\")\n",
    "with open(results_file, \"w\") as file:\n",
    "    file.write(\"experiment_num,selected_features,num_features,mse(mean),mse(std),rmse(mean),rmse(std),cmapss(mean),cmapss(std)\\n\")\n",
    "\n",
    "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train, \n",
    "                                                                          y_train, \n",
    "                                                                          test_size=0.3, \n",
    "                                                                          random_state=seed)\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_split)\n",
    "x_val_scaled = scaler.transform(x_val_split)\n",
    "input_dim = x_train_scaled.shape[1]\n",
    "\n",
    "scaler_file = os.path.join(results_path, 'scaler.pkl')\n",
    "with open(scaler_file, 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "# Create model\n",
    "weights_file = os.path.join(results_path, 'mlp_initial_weights.h5')\n",
    "model_path = os.path.join(results_path, 'mlp_model_trained.h5')\n",
    "model = create_mlp_model(input_dim, layer_sizes, activation='tanh', \n",
    "                         output_weights_file=weights_file)\n",
    "model.summary()\n",
    "    \n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint(model_path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "# Train model\n",
    "history = train_model_existing_weights(model, weights_file, \n",
    "                                       x_train_scaled, y_train_split, \n",
    "                                       x_val_scaled, y_val_split, \n",
    "                                       batch_size=batch_size, \n",
    "                                       epochs=epochs, \n",
    "                                       callbacks=[es, mc])\n",
    "    \n",
    "history_file = os.path.join(results_path, \"history.pkl\")\n",
    "save_history(history, history_file)\n",
    "plot_loss_curves(history.history, output_path=results_path)\n",
    "    \n",
    "# Performance evaluation\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "    \n",
    "loaded_model = load_model(model_path)\n",
    "predictions_test = loaded_model.predict(x_test_scaled).flatten()\n",
    "mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)\n",
    "\n",
    "mse_mean, mse_std = mse, 0\n",
    "rmse_mean, rmse_std = rmse, 0\n",
    "cmapss_mean, cmapss_std = cmapss_score, 0\n",
    "with open(results_file, \"a\") as file:\n",
    "    file.write(f\"{idx}, {feature_list_to_string(selected_columns)}, {len(selected_columns)}, {mse_mean}, {mse_std}, {rmse_mean}, {rmse_std}, {cmapss_mean}, {cmapss_std}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "MSE: 13.60\n",
      "RMSE: 3.69\n",
      "CMAPSS score: 1.27\n",
      "\n",
      "Test set:\n",
      "MSE: 70.02\n",
      "RMSE: 8.37\n",
      "CMAPSS score: 1.86\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_evaluation(loaded_model, x_test_scaled, y_test, x_train_scaled, y_train_split, plot_range=[0, 10**3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation time (sec):  3.875\n",
      "\n",
      "Train set shape: (5263447, 47)\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# Evaluation\n",
    "###########################\n",
    "\n",
    "start_time = time.process_time()  \n",
    "train_set, test_set, columns = load_dataset(filename)\n",
    "print(\"Operation time (sec): \" , (time.process_time() - start_time))\n",
    "print()\n",
    "print(\"Train set shape: \" + str(train_set.shape))\n",
    "\n",
    "columns_aux = columns[0] \n",
    "columns_health_params = columns[1] \n",
    "columns_sensor_measurements = columns[2] \n",
    "columns_virtual_sensors = columns[3]\n",
    "columns_operating_conditions = columns[4] \n",
    "target_col = columns[5]\n",
    "\n",
    "y_train = train_set['RUL']\n",
    "x_train = train_set.drop(['RUL'], axis=1)\n",
    "\n",
    "y_test = test_set['RUL']\n",
    "x_test = test_set.drop(['RUL'], axis=1)\n",
    "\n",
    "initial_columns = x_train.columns\n",
    "selected_columns_1 = columns_sensor_measurements + columns_virtual_sensors + columns_operating_conditions\n",
    "\n",
    "x_train.drop(labels=[x for x in initial_columns if x not in selected_columns_1], axis=1, inplace=True)\n",
    "\n",
    "x_train, x_holdout, y_train, y_holdout = train_test_split(x_train, y_train, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results CORR_TH =  None\n",
      "Holdout set: \n",
      "Test set:\n",
      "MSE: 15.20\n",
      "RMSE: 3.90\n",
      "CMAPSS score: 1.30\n",
      "\n",
      "Test set: \n",
      "Test set:\n",
      "MSE: 45.96\n",
      "RMSE: 6.78\n",
      "CMAPSS score: 1.62\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Results CORR_TH =  0.99\n",
      "Holdout set: \n",
      "Test set:\n",
      "MSE: 13.92\n",
      "RMSE: 3.73\n",
      "CMAPSS score: 1.28\n",
      "\n",
      "Test set: \n",
      "Test set:\n",
      "MSE: 70.63\n",
      "RMSE: 8.40\n",
      "CMAPSS score: 1.87\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Results CORR_TH =  0.95\n",
      "Holdout set: \n",
      "Test set:\n",
      "MSE: 31.93\n",
      "RMSE: 5.65\n",
      "CMAPSS score: 1.58\n",
      "\n",
      "Test set: \n",
      "Test set:\n",
      "MSE: 73.66\n",
      "RMSE: 8.58\n",
      "CMAPSS score: 1.90\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Results CORR_TH =  0.9\n",
      "Holdout set: \n",
      "Test set:\n",
      "MSE: 30.81\n",
      "RMSE: 5.55\n",
      "CMAPSS score: 1.56\n",
      "\n",
      "Test set: \n",
      "Test set:\n",
      "MSE: 64.30\n",
      "RMSE: 8.02\n",
      "CMAPSS score: 1.83\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_columns = x_train.columns\n",
    "corr_th_list = [None, 0.99, 0.95, 0.9]\n",
    "\n",
    "for corr_th in corr_th_list:\n",
    "    print(\"Results CORR_TH = \", corr_th)\n",
    "    results_folder = \"results_all\" if corr_th is None else \"results_{}\".format(corr_th)\n",
    "    results_path = os.path.join(output_path, \"corr_th_experiments\", results_folder)\n",
    "    model_path = os.path.join(results_path, \"split_0\", 'mlp_model_trained.h5')\n",
    "    \n",
    "    # Select features based on training set\n",
    "    if corr_th is not None:\n",
    "        selected_columns = get_non_correlated_features(x_train, corr_th=corr_th, debug=False)\n",
    "    else:\n",
    "        selected_columns = x_train.columns\n",
    "    \n",
    "    x_train_feature_selection = x_train[selected_columns]\n",
    "    \n",
    "    # Train-validation split for early stopping\n",
    "    x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train_feature_selection, \n",
    "                                                                              y_train, \n",
    "                                                                              test_size=0.3, \n",
    "                                                                              random_state=seed)\n",
    "\n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train_split)\n",
    "    x_val_scaled = scaler.transform(x_val_split)\n",
    "    \n",
    "    # Performance evaluation\n",
    "    x_holdout_feature_selection = x_holdout[selected_columns]\n",
    "    x_holdout_scaled = scaler.transform(x_holdout_feature_selection)\n",
    "\n",
    "    x_test_feature_selection = x_test[selected_columns]\n",
    "    x_test_scaled = scaler.transform(x_test_feature_selection)\n",
    "    \n",
    "    loaded_model = load_model(model_path)\n",
    "    print(\"Holdout set: \")\n",
    "    predictions_holdout = loaded_model.predict(x_holdout_scaled).flatten()\n",
    "    mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_holdout, y_holdout)\n",
    "    \n",
    "    print(\"Test set: \")\n",
    "    predictions_test = loaded_model.predict(x_test_scaled).flatten()\n",
    "    mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)\n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results CORR_TH =  None\n",
      "Test set:\n",
      "MSE: 45.96\n",
      "RMSE: 6.78\n",
      "CMAPSS score: 1.62\n",
      "\n",
      "Test set:\n",
      "MSE: 50.61\n",
      "RMSE: 7.11\n",
      "CMAPSS score: 1.67\n",
      "\n",
      "Test set:\n",
      "MSE: 49.85\n",
      "RMSE: 7.06\n",
      "CMAPSS score: 1.65\n",
      "\n",
      "\n",
      "MSE mean = 48.8061641708166\n",
      "MSE std = 2.03701923144007\n",
      "\n",
      "\n",
      "RMSE mean = 6.98459820093371\n",
      "RMSE std = 0.14680647918325718\n",
      "\n",
      "\n",
      "CMAPSS mean = 1.646654278030259\n",
      "CMAPSS std = 0.022963143047180595\n",
      "\n",
      "Results CORR_TH =  0.99\n",
      "Test set:\n",
      "MSE: 70.63\n",
      "RMSE: 8.40\n",
      "CMAPSS score: 1.87\n",
      "\n",
      "Test set:\n",
      "MSE: 75.85\n",
      "RMSE: 8.71\n",
      "CMAPSS score: 1.92\n",
      "\n",
      "Test set:\n",
      "MSE: 65.48\n",
      "RMSE: 8.09\n",
      "CMAPSS score: 1.83\n",
      "\n",
      "\n",
      "MSE mean = 70.65527531693327\n",
      "MSE std = 4.235095464131678\n",
      "\n",
      "\n",
      "RMSE mean = 8.401888716344407\n",
      "RMSE std = 0.25207402701944215\n",
      "\n",
      "\n",
      "CMAPSS mean = 1.8723792630508644\n",
      "CMAPSS std = 0.03621536967804698\n",
      "\n",
      "Results CORR_TH =  0.95\n",
      "Test set:\n",
      "MSE: 73.66\n",
      "RMSE: 8.58\n",
      "CMAPSS score: 1.90\n",
      "\n",
      "Test set:\n",
      "MSE: 66.68\n",
      "RMSE: 8.17\n",
      "CMAPSS score: 1.85\n",
      "\n",
      "Test set:\n",
      "MSE: 57.22\n",
      "RMSE: 7.56\n",
      "CMAPSS score: 1.76\n",
      "\n",
      "\n",
      "MSE mean = 65.85140225080023\n",
      "MSE std = 6.735034371109586\n",
      "\n",
      "\n",
      "RMSE mean = 8.104124435169187\n",
      "RMSE std = 0.4178150189904059\n",
      "\n",
      "\n",
      "CMAPSS mean = 1.8367392905067945\n",
      "CMAPSS std = 0.05780696467337864\n",
      "\n",
      "Results CORR_TH =  0.9\n",
      "Test set:\n",
      "MSE: 64.30\n",
      "RMSE: 8.02\n",
      "CMAPSS score: 1.83\n",
      "\n",
      "Test set:\n",
      "MSE: 66.53\n",
      "RMSE: 8.16\n",
      "CMAPSS score: 1.85\n",
      "\n",
      "Test set:\n",
      "MSE: 64.90\n",
      "RMSE: 8.06\n",
      "CMAPSS score: 1.83\n",
      "\n",
      "\n",
      "MSE mean = 65.24468793830509\n",
      "MSE std = 0.9437548713614302\n",
      "\n",
      "\n",
      "RMSE mean = 8.077207886928228\n",
      "RMSE std = 0.05831543234431301\n",
      "\n",
      "\n",
      "CMAPSS mean = 1.83532569273517\n",
      "CMAPSS std = 0.007061031847467973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_columns = x_train.columns\n",
    "corr_th_list = [None, 0.99, 0.95, 0.9]\n",
    "\n",
    "for corr_th in corr_th_list:\n",
    "    print(\"Results CORR_TH = \", corr_th)\n",
    "    results_folder = \"results_all\" if corr_th is None else \"results_{}\".format(corr_th)\n",
    "    results_path = os.path.join(output_path, \"corr_th_experiments\", results_folder)\n",
    "    \n",
    "    # Select features based on training set\n",
    "    if corr_th is not None:\n",
    "        selected_columns = get_non_correlated_features(x_train, corr_th=corr_th, debug=False)\n",
    "    else:\n",
    "        selected_columns = x_train.columns\n",
    "    \n",
    "    x_train_feature_selection = x_train[selected_columns]\n",
    "    \n",
    "    # Train-validation split for early stopping\n",
    "    x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train_feature_selection, \n",
    "                                                                              y_train, \n",
    "                                                                              test_size=0.3, \n",
    "                                                                              random_state=seed)\n",
    "\n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train_split)\n",
    "\n",
    "    x_test_feature_selection = x_test[selected_columns]\n",
    "    x_test_scaled = scaler.transform(x_test_feature_selection)\n",
    "    \n",
    "    mse_vals = []\n",
    "    rmse_vals = []\n",
    "    cmapss_vals = []\n",
    "    \n",
    "    for split_no in range(3):\n",
    "        model_path = os.path.join(results_path, f\"split_{split_no}\", 'mlp_model_trained.h5')\n",
    "        loaded_model = load_model(model_path)\n",
    "\n",
    "        predictions_test = loaded_model.predict(x_test_scaled).flatten()\n",
    "        mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)\n",
    "\n",
    "        mse_vals.append(mse)\n",
    "        rmse_vals.append(rmse)\n",
    "        cmapss_vals.append(cmapss_score)\n",
    "    \n",
    "    mse_mean = np.mean(mse_vals)\n",
    "    mse_std = np.std(mse_vals)\n",
    "    rmse_mean = np.mean(rmse_vals)\n",
    "    rmse_std = np.std(rmse_vals)\n",
    "    cmapss_mean = np.mean(cmapss_vals)\n",
    "    cmapss_std = np.std(cmapss_vals)\n",
    "    \n",
    "    print(\"\\nMSE mean = {}\\nMSE std = {}\\n\".format(mse_mean, mse_std))      \n",
    "    print(\"\\nRMSE mean = {}\\nRMSE std = {}\\n\".format(rmse_mean, rmse_std))\n",
    "    print(\"\\nCMAPSS mean = {}\\nCMAPSS std = {}\\n\".format(cmapss_mean, cmapss_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-keras-gpu",
   "language": "python",
   "name": "tf-keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
