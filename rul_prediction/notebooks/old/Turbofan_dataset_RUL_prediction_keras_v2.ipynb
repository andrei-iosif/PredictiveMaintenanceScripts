{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "seed = 0\n",
    "os.environ['PYTHONHASSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/turbofan_dataset/N-CMAPSS_DS02-006.h5'\n",
    "output_path = 'DS02/experiment_set_8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename, load_test_set=True):\n",
    "    \"\"\" Reads a dataset from a given .h5 file and compose (in memory) the train and test data. \n",
    "    Args:\n",
    "        filename(str): path to the .h5 file\n",
    "    Returns:\n",
    "        train_set(pd.DataFrame), test_set(pd.DataFrame)\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'r') as hdf:\n",
    "        # Development set\n",
    "        W_dev = np.array(hdf.get('W_dev'))             # W\n",
    "        X_s_dev = np.array(hdf.get('X_s_dev'))         # X_s\n",
    "        X_v_dev = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "        T_dev = np.array(hdf.get('T_dev'))             # T\n",
    "        Y_dev = np.array(hdf.get('Y_dev'))             # RUL  \n",
    "        A_dev = np.array(hdf.get('A_dev'))             # Auxiliary\n",
    "\n",
    "        # Test set\n",
    "        if load_test_set:\n",
    "            W_test = np.array(hdf.get('W_test'))           # W\n",
    "            X_s_test = np.array(hdf.get('X_s_test'))       # X_s\n",
    "            X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "            T_test = np.array(hdf.get('T_test'))           # T\n",
    "            Y_test = np.array(hdf.get('Y_test'))           # RUL  \n",
    "            A_test = np.array(hdf.get('A_test'))           # Auxiliary\n",
    "        \n",
    "        # Column names\n",
    "        W_var = np.array(hdf.get('W_var'))\n",
    "        X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "        X_v_var = np.array(hdf.get('X_v_var')) \n",
    "        T_var = np.array(hdf.get('T_var'))\n",
    "        A_var = np.array(hdf.get('A_var'))\n",
    "        \n",
    "        columns = []\n",
    "        columns.append(list(np.array(A_var, dtype='U20')))\n",
    "        columns.append(list(np.array(T_var, dtype='U20')))\n",
    "        columns.append(list(np.array(X_s_var, dtype='U20')))\n",
    "        columns.append(list(np.array(X_v_var, dtype='U20')))\n",
    "        columns.append(list(np.array(W_var, dtype='U20')))\n",
    "        columns.append(['RUL'])\n",
    "        \n",
    "        columns_list = []\n",
    "        for columns_per_category in columns:\n",
    "            columns_list += columns_per_category\n",
    "        \n",
    "    train_set = np.concatenate((A_dev, T_dev, X_s_dev, X_v_dev, W_dev, Y_dev), axis=1)\n",
    "    if load_test_set:\n",
    "        test_set = np.concatenate((A_test, T_test, X_s_test, X_v_test, W_test, Y_test), axis=1)\n",
    "        return pd.DataFrame(data=train_set, columns=columns_list), pd.DataFrame(data=test_set, columns=columns_list), columns\n",
    "    else:\n",
    "        return pd.DataFrame(data=train_set, columns=columns_list), None, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation time (sec):  4.640625\n",
      "Train set shape: (5263447, 47)\n",
      "Test set shape: (1253743, 47)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.process_time()  \n",
    "train_set, test_set, columns = load_dataset(filename, load_test_set=True)\n",
    "print(\"Operation time (sec): \" , (time.process_time() - start_time))\n",
    "print(\"Train set shape: \" + str(train_set.shape))\n",
    "print(\"Test set shape: \" + str(test_set.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_aux = columns[0] \n",
    "columns_health_params = columns[1] \n",
    "columns_sensor_measurements = columns[2] \n",
    "\n",
    "columns_virtual_sensors = columns[3]\n",
    "columns_operating_conditions = columns[4] \n",
    "target_col = columns[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_cycle_info(df, compute_cycle_len=False):\n",
    "    unit_ids = np.unique(df['unit'])\n",
    "    print('Engine units in df: ', unit_ids)\n",
    "    for i in unit_ids:\n",
    "        num_cycles = len(np.unique(df.loc[df['unit'] == i, 'cycle']))\n",
    "        num_entries = len(df.loc[df['unit'] == i])\n",
    "        print('Unit: ', i, ' - Num flight cycles: ', num_cycles, ' - Num entries: ', num_entries)\n",
    "        \n",
    "    if compute_cycle_len:\n",
    "        cycle_ids = np.unique(df['cycle'])\n",
    "        print('Total number of cycles: ', len(cycle_ids))\n",
    "        min_len = np.inf\n",
    "        max_len = 0\n",
    "        for i in cycle_ids:\n",
    "            cycle_len = len(df.loc[df['cycle'] == i, 'cycle'])\n",
    "            if cycle_len < min_len:\n",
    "                min_len = cycle_len\n",
    "            elif cycle_len > max_len:\n",
    "                max_len = cycle_len\n",
    "        print('Min cycle length: ', min_len)\n",
    "        print('Max cycle length: ', max_len)\n",
    "    \n",
    "    return unit_ids\n",
    "\n",
    "def failure_modes_info(df):\n",
    "    unit_ids = np.unique(df['unit'])\n",
    "    print('Engine units in df: ', unit_ids)\n",
    "    \n",
    "    for i in unit_ids:\n",
    "        df_unit = df.loc[df['unit'] == i]\n",
    "        \n",
    "        constant_filter = VarianceThreshold(threshold=variance_th)\n",
    "        constant_filter.fit(dataset)\n",
    "        constant_features = [col for col in dataset.columns \n",
    "                           if col not in dataset.columns[constant_filter.get_support()]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter constant and quasi-constant features\n",
    "def get_quasi_constant_features(dataset, variance_th=0.01, debug=True):\n",
    "    constant_filter = VarianceThreshold(threshold=variance_th)\n",
    "    constant_filter.fit(dataset)\n",
    "    constant_features = [col for col in dataset.columns \n",
    "                         if col not in dataset.columns[constant_filter.get_support()]]\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Number of non-constant features: \", len(dataset.columns[constant_filter.get_support()]))\n",
    "        \n",
    "        print(\"Number of quasi-constant features: \", len(constant_features))\n",
    "        print(\"Quasi-constant features: \")\n",
    "        for col in constant_features:\n",
    "            print(col)\n",
    "    return constant_features\n",
    "\n",
    "def get_non_correlated_features(dataset, corr_th=0.9, debug=True):\n",
    "    corr_mat = dataset.corr()\n",
    "    corr_mat = np.abs(corr_mat)\n",
    "    \n",
    "    num_cols = corr_mat.shape[0]\n",
    "    columns = np.full((num_cols,), True, dtype=bool)\n",
    "    for i in range(num_cols):\n",
    "        for j in range(i+1, num_cols):\n",
    "            if columns[i]:\n",
    "                val = corr_mat.iloc[i, j]\n",
    "                if val >= corr_th and columns[j]:\n",
    "                    columns[j] = False\n",
    "                    if debug:\n",
    "                        print(dataset.columns[i], \"|\", dataset.columns[j], \"|\", round(val, 2))\n",
    "    if debug:        \n",
    "        correlated_features = dataset.columns[~columns]\n",
    "        print(\"Number of correlated features: \", len(correlated_features))\n",
    "        print(\"Correlated features: \", list(correlated_features))\n",
    "    \n",
    "    selected_columns = dataset.columns[columns]\n",
    "    if debug:\n",
    "        print(\"Number of selected features: \", len(selected_columns))\n",
    "        print(\"Selected features: \", list(selected_columns))\n",
    "    return selected_columns\n",
    "\n",
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold. Removing collinear features can help a model \n",
    "        to generalize and improves the interpretability of the model.\n",
    "\n",
    "    Inputs: \n",
    "        x: features dataframe\n",
    "        threshold: features with correlations greater than this value are removed\n",
    "\n",
    "    Output: \n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    corr_matrix = np.abs(corr_matrix)\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i+1):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "\n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                # Print the correlated features and the correlation value\n",
    "                print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns=drops)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set['RUL']\n",
    "x_train = train_set.drop(['RUL'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.9, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_columns = ['cycle', 'hs', 'Fc', 'unit']\n",
    "x_train.drop(labels=[x for x in auxiliary_columns if x in x_train.columns], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation time (sec):  859.984375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "start_time = time.process_time()  \n",
    "mi = mutual_info_regression(x_train, y_train, random_state=seed)\n",
    "print(\"Operation time (sec): \" , (time.process_time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_series = pd.Series(mi, index=x_train.columns)\n",
    "mi_series = mi_series.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_mi = (mi_series - mi_series.min()) / (mi_series.max() - mi_series.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "normalized_mi[:15].plot.bar(color='steelblue', figsize=(12, 6))\n",
    "plt.savefig(\"mutual_info.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HPT_eff_mod     0.751772\n",
       "LPT_flow_mod    0.489825\n",
       "LPT_eff_mod     0.473342\n",
       "SmLPC           0.215121\n",
       "SmHPC           0.105208\n",
       "Nc              0.026882\n",
       "W25             0.013903\n",
       "W32             0.013903\n",
       "W31             0.013903\n",
       "W22             0.013901\n",
       "W50             0.013282\n",
       "W48             0.013246\n",
       "Mach            0.013179\n",
       "T30             0.012871\n",
       "SmFan           0.012770\n",
       "TRA             0.008674\n",
       "Nf              0.008351\n",
       "alt             0.007691\n",
       "P40             0.005571\n",
       "P30             0.005571\n",
       "Ps30            0.004417\n",
       "W21            -0.001102\n",
       "P45            -0.003166\n",
       "P21            -0.003623\n",
       "P15            -0.003623\n",
       "T2             -0.004511\n",
       "P2             -0.004756\n",
       "P24            -0.005246\n",
       "P50            -0.005671\n",
       "T24            -0.007947\n",
       "Wf             -0.017543\n",
       "T40            -0.050204\n",
       "T48            -0.072477\n",
       "phi            -0.091470\n",
       "T50            -0.123531\n",
       "fan_eff_mod          NaN\n",
       "fan_flow_mod         NaN\n",
       "LPC_eff_mod          NaN\n",
       "LPC_flow_mod         NaN\n",
       "HPC_eff_mod          NaN\n",
       "HPC_flow_mod         NaN\n",
       "HPT_flow_mod         NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.corrwith(y_train).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_columns = x_train.columns\n",
    "selected_columns_1 = columns_sensor_measurements + columns_virtual_sensors + columns_operating_conditions\n",
    "\n",
    "x_train.drop(labels=[x for x in initial_columns if x not in selected_columns_1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3684412, 35)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T30 | Nc | 1.0\n",
      "T48 | T40 | 1.0\n",
      "P15 | P21 | 1.0\n",
      "P15 | P50 | 0.99\n",
      "P2 | alt | 0.99\n",
      "P2 | T2 | 0.99\n",
      "P24 | W21 | 1.0\n",
      "Ps30 | P40 | 1.0\n",
      "Ps30 | P30 | 1.0\n",
      "Ps30 | P45 | 1.0\n",
      "Ps30 | W22 | 0.99\n",
      "Ps30 | W25 | 0.99\n",
      "Ps30 | W31 | 0.99\n",
      "Ps30 | W32 | 0.99\n",
      "Ps30 | W48 | 0.99\n",
      "Ps30 | W50 | 0.99\n",
      "SmHPC | phi | 0.99\n",
      "Number of correlated features:  17\n",
      "Correlated features:  ['P21', 'P40', 'P50', 'Nc', 'T40', 'P30', 'P45', 'W21', 'W22', 'W25', 'W31', 'W32', 'W48', 'W50', 'phi', 'alt', 'T2']\n",
      "Number of selected features:  18\n",
      "Selected features:  ['HPT_eff_mod', 'LPT_eff_mod', 'LPT_flow_mod', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P24', 'Ps30', 'Nf', 'Wf', 'SmFan', 'SmLPC', 'SmHPC', 'Mach', 'TRA']\n"
     ]
    }
   ],
   "source": [
    "selected_columns = get_non_correlated_features(x_train, corr_th=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T24 | Ps30 | 0.99\n",
      "T24 | P40 | 0.99\n",
      "T24 | Wf | 0.97\n",
      "T24 | P30 | 0.99\n",
      "T24 | P45 | 0.98\n",
      "T24 | W22 | 0.98\n",
      "T24 | W25 | 0.98\n",
      "T24 | W31 | 0.98\n",
      "T24 | W32 | 0.98\n",
      "T24 | W48 | 0.98\n",
      "T24 | W50 | 0.98\n",
      "T30 | T48 | 0.97\n",
      "T30 | T50 | 0.95\n",
      "T30 | Nc | 1.0\n",
      "T30 | T40 | 0.97\n",
      "P15 | P2 | 0.97\n",
      "P15 | P21 | 1.0\n",
      "P15 | P24 | 0.99\n",
      "P15 | P50 | 0.99\n",
      "P15 | W21 | 0.99\n",
      "P15 | alt | 0.96\n",
      "P15 | T2 | 0.97\n",
      "Nf | SmHPC | 0.98\n",
      "Nf | phi | 0.97\n",
      "Nf | TRA | 0.97\n",
      "Number of correlated features:  25\n",
      "Correlated features:  ['T48', 'T50', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nc', 'Wf', 'T40', 'P30', 'P45', 'W21', 'W22', 'W25', 'W31', 'W32', 'W48', 'W50', 'SmHPC', 'phi', 'alt', 'TRA', 'T2']\n",
      "Number of selected features:  10\n",
      "Selected features:  ['HPT_eff_mod', 'LPT_eff_mod', 'LPT_flow_mod', 'T24', 'T30', 'P15', 'Nf', 'SmFan', 'SmLPC', 'Mach']\n"
     ]
    }
   ],
   "source": [
    "selected_columns = get_non_correlated_features(x_train, corr_th=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-constant features:  35\n",
      "Number of quasi-constant features:  7\n",
      "Quasi-constant features: \n",
      "fan_eff_mod\n",
      "fan_flow_mod\n",
      "LPC_eff_mod\n",
      "LPC_flow_mod\n",
      "HPC_eff_mod\n",
      "HPC_flow_mod\n",
      "HPT_flow_mod\n",
      "Train shape:  (3684412, 35)\n"
     ]
    }
   ],
   "source": [
    "constant_features = get_quasi_constant_features(x_train, variance_th=0.0)\n",
    "x_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "print(\"Train shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features\n",
    "selected_columns = get_non_correlated_features(x_train, corr_th=0.99)\n",
    "# x_train = x_train[selected_columns]\n",
    "# print(\"Train shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop auxiliary data columns\n",
    "auxiliary_columns = ['cycle', 'hs', 'Fc', 'unit']\n",
    "x_train.drop(labels=[x for x in auxiliary_columns if x in x_train.columns], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_set['RUL']\n",
    "x_test = test_set.drop(['RUL'], axis=1)\n",
    "x_test = x_test[x_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.3, random_state=seed)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmapss_score_function(actual, predictions, normalize=True):\n",
    "    # diff < 0 -> over-estimation\n",
    "    # diff > 0 -> under-estimation\n",
    "    diff = actual - predictions\n",
    "    alpha = np.full_like(diff, 1/13)\n",
    "    negative_diff_mask = diff < 0\n",
    "    alpha[negative_diff_mask] = 1/10\n",
    "    score = np.sum(np.exp(alpha * np.abs(diff)))\n",
    "    \n",
    "    if normalize:\n",
    "        N = len(predictions)\n",
    "        score /= N\n",
    "    return score\n",
    "\n",
    "def compute_evaluation_metrics(actual, predictions, label='Test'):\n",
    "    mse = mean_squared_error(actual, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    cmapss_score = cmapss_score_function(actual, predictions)\n",
    "    print('{} set:\\nMSE: {:.2f}\\nRMSE: {:.2f}\\nCMAPSS score: {:.2f}\\n'.format(label, mse, rmse, \n",
    "                                                                     Decimal(cmapss_score)))\n",
    "    return mse, rmse, cmapss_score\n",
    "    \n",
    "def plot_loss_curves(history, output_path=None, y_lim=[0, 150]):\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim(y_lim)\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    \n",
    "    if output_path is not None:\n",
    "        plt.savefig(os.path.join(output_path, 'loss_curves.png'), format='png', dpi=300) \n",
    "    plt.show()\n",
    "    \n",
    "def plot_rul(expected, predicted):\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(expected)), expected, label='Expected')\n",
    "    plt.plot(range(len(predicted)), predicted, label='Predicted')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def create_mlp_model(input_dim, hidden_layer_sizes, activation='relu', output_weights_file=None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_sizes[0], \n",
    "                    input_dim=input_dim, \n",
    "                    kernel_initializer='random_normal', \n",
    "                    activation=activation))\n",
    "\n",
    "    for layer_size in hidden_layer_sizes[1:]:\n",
    "        model.add(Dense(layer_size, \n",
    "                        kernel_initializer='random_normal', \n",
    "                        activation=activation))\n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer='random_normal'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    if output_weights_file is not None:\n",
    "        model.save_weights(output_weights_file)\n",
    "    return model\n",
    "\n",
    "def train_model_existing_weights(model, weights_file, x_train, y_train, x_val, y_val, epochs=200, batch_size=512, callbacks=[]):\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.load_weights(weights_file)\n",
    "    return model.fit(x_train, y_train,\n",
    "                     validation_data=(x_val, y_val),\n",
    "                     epochs=epochs,\n",
    "                     batch_size=batch_size,\n",
    "                     verbose=1,\n",
    "                     callbacks=callbacks)\n",
    "\n",
    "def save_history(history, output_file=os.path.join(output_path, \"history.pkl\")):\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(history.history, file)\n",
    "    print(\"Saved training history to file: {}\".format(output_file))\n",
    "\n",
    "def load_history(file):\n",
    "    return pickle.load(open(file, \"rb\"))\n",
    "\n",
    "def model_evaluation(model, x_test, y_test, x_train=None, y_train=None, plot_range=[0, 10**3]):\n",
    "    if x_train is not None and y_train is not None:\n",
    "        predictions_train = model.predict(x_train).flatten()\n",
    "        compute_evaluation_metrics(predictions_train, y_train, 'Train')\n",
    "        \n",
    "        expected = y_train[plot_range[0]:plot_range[1]]\n",
    "        predicted = predictions_train[plot_range[0]:plot_range[1]]\n",
    "        plot_rul(expected, predicted)\n",
    "        \n",
    "    predictions_test = model.predict(x_test).flatten()\n",
    "    compute_evaluation_metrics(predictions_test, y_test)\n",
    "    \n",
    "    expected = y_test[plot_range[0]:plot_range[1]]\n",
    "    predicted = predictions_test[plot_range[0]:plot_range[1]]\n",
    "    plot_rul(expected, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Random search (experiment set 3)\n",
    "# Number of nodes in one hidden layer is twice \n",
    "# the number of nodes in the previous layer\n",
    "##############################################\n",
    "ITERATIONS = 60 \n",
    "EPOCHS = 30\n",
    "\n",
    "hidden_layer_sizes = [64, 128, 256, 512]\n",
    "num_hidden_layers = [1, 2, 3, 4]\n",
    "activation_functions = ['relu', 'tanh']\n",
    "batch_sizes = [512, 1024, 2048]\n",
    "\n",
    "results = pd.DataFrame(columns=['layers', 'activation', 'batch_size', 'training_time', 'loss_train', 'loss_val'])  \n",
    "\n",
    "input_dim = x_train_scaled.shape[1]\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    num_layers = random.sample(num_hidden_layers, 1)[0]\n",
    "    max_idx = len(hidden_layer_sizes) - num_layers\n",
    "    first_layer_size = random.sample(hidden_layer_sizes[:(max_idx+1)], 1)[0]\n",
    "    \n",
    "    layer_sizes = [first_layer_size * 2 ** i for i in range(num_layers)]    \n",
    "    \n",
    "    activation = random.sample(activation_functions, 1)[0]\n",
    "    batch_size = random.sample(batch_sizes, 1)[0]\n",
    "\n",
    "    print('\\n\\nIteration ', i+1)\n",
    "    print('Layer sizes = {}, activation = {}, batch size = {}'.format(layer_sizes,\n",
    "                                                                      activation,\n",
    "                                                                      batch_size))\n",
    "    \n",
    "    weights_file = os.path.join(output_path, 'mlp_weights_{}.h5'.format(i + 1))\n",
    "    model = create_mlp_model(input_dim, layer_sizes, weights_file)\n",
    "    model.summary()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train_scaled, y_train,\n",
    "                        validation_data=(x_val_scaled, y_val),\n",
    "                        epochs=EPOCHS,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=2)\n",
    "    training_time = time.time() - start_time\n",
    "    print(\"Training time for iteration {}: {}\".format(i + 1, training_time))\n",
    "    \n",
    "    mse_train = history.history['loss'][-1]\n",
    "    mse_val = history.history['val_loss'][-1]\n",
    "    \n",
    "    iter_results = {'loss_train': mse_train, \n",
    "         'loss_val': mse_val,\n",
    "         'layers': str(layer_sizes),\n",
    "         'activation': activation, \n",
    "         'batch_size': batch_size,\n",
    "         'training_time': training_time}\n",
    "        \n",
    "    results = results.append(pd.DataFrame(iter_results, index=[0]), ignore_index=True)\n",
    "    results.to_csv(os.path.join(output_path, 'parameter_search.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Random search (experiment set 4)\n",
    "##################################\n",
    "ITERATIONS = 60 \n",
    "EPOCHS = 30\n",
    "\n",
    "hidden_layer_sizes = [64, 128, 256, 512]\n",
    "num_hidden_layers = [1, 2, 3, 4]\n",
    "activation_functions = ['relu', 'tanh']\n",
    "batch_sizes = [512, 1024, 2048]\n",
    "\n",
    "results = pd.DataFrame(columns=['layers', 'activation', 'batch_size', 'training_time', 'loss_train', 'loss_val'])  \n",
    "\n",
    "input_dim = x_train_scaled.shape[1]\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    num_layers = random.sample(num_hidden_layers, 1)[0]\n",
    "    layer_sizes = [random.sample(hidden_layer_sizes, 1)[0] for _ in range(num_layers)]  \n",
    "    activation = random.sample(activation_functions, 1)[0]\n",
    "    batch_size = random.sample(batch_sizes, 1)[0]\n",
    "\n",
    "    print('\\n\\nIteration ', i+1)\n",
    "    print('Layer sizes = {}, activation = {}, batch size = {}'.format(layer_sizes,\n",
    "                                                                      activation,\n",
    "                                                                      batch_size))\n",
    "    \n",
    "    weights_file = os.path.join(output_path, 'mlp_weights_{}.h5'.format(i + 1))\n",
    "    model = create_mlp_model(input_dim, layer_sizes, weights_file)\n",
    "    model.summary()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train_scaled, y_train,\n",
    "                        validation_data=(x_val_scaled, y_val),\n",
    "                        epochs=EPOCHS,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=2)\n",
    "    training_time = time.time() - start_time\n",
    "    print(\"Training time for iteration {}: {}\".format(i + 1, training_time))\n",
    "    \n",
    "    mse_train = history.history['loss'][-1]\n",
    "    mse_val = history.history['val_loss'][-1]\n",
    "    \n",
    "    iter_results = {'loss_train': mse_train, \n",
    "         'loss_val': mse_val,\n",
    "         'layers': str(layer_sizes),\n",
    "         'activation': activation, \n",
    "         'batch_size': batch_size,\n",
    "         'training_time': training_time}\n",
    "        \n",
    "    results = results.append(pd.DataFrame(iter_results, index=[0]), ignore_index=True)\n",
    "    results.to_csv(os.path.join(output_path, 'parameter_search.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(r'D:\\Licenta\\notebooks\\DS02\\experiment_set_4\\random_search\\parameter_search.csv')\n",
    "results = results.sort_values('loss_val')\n",
    "results.to_csv(os.path.join(output_path, 'parameter_search.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain best model in experiment set 4\n",
    "weights_file = r\"D:\\Licenta\\notebooks\\DS02\\experiment_set_4\\mlp_weights_41.h5\"\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "\n",
    "layer_sizes = [256, 256, 512, 64]\n",
    "input_dim = x_train_scaled.shape[1]\n",
    "\n",
    "model = create_mlp_model(input_dim, layer_sizes, weights_file, activation='tanh')\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint(os.path.join(output_path, 'mlp_model_baseline.h5'), monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "history = train_existing_model(model, weights_file, x_train_scaled, y_train, x_val_scaled, y_val, \n",
    "                     batch_size=batch_size, epochs=epochs, callbacks=[es, mc])\n",
    "\n",
    "save_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Test multiple splits\n",
    "######################\n",
    "weights_file = r\"D:\\Licenta\\notebooks\\DS02\\experiment_set_4\\mlp_weights.h5\"\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "\n",
    "layer_sizes = [256, 256, 512, 64]\n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "results = []\n",
    "\n",
    "for random_seed in range(5):\n",
    "    model_path = os.path.join(output_path, \"split_seed_{}\".format(random_seed))\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    \n",
    "    x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train, y_train, \n",
    "                                                                              test_size=0.3, \n",
    "                                                                              random_state=random_seed)\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train_split)\n",
    "    x_val_scaled = scaler.transform(x_val_split)\n",
    "\n",
    "    model = create_mlp_model(input_dim, layer_sizes, activation='tanh')\n",
    "    model.summary()\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint(os.path.join(model_path, 'mlp_model_baseline.h5'),\n",
    "                         monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "    history = train_model_existing_weights(model, weights_file, \n",
    "                                           x_train_scaled, y_train_split, \n",
    "                                           x_val_scaled, y_val_split, \n",
    "                                           batch_size=batch_size, \n",
    "                                           epochs=epochs, \n",
    "                                           callbacks=[es, mc])\n",
    "    \n",
    "    history_path = os.path.join(model_path, \"history.pkl\")\n",
    "    save_history(history, history_path)\n",
    "    results.append((model, history, scaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_vals = []\n",
    "rmse_vals = []\n",
    "cmapss_score_vals = []\n",
    "\n",
    "for i in range(5):\n",
    "    model_path = os.path.join(output_path, \"split_seed_{}\".format(i))\n",
    "    model = load_model(os.path.join(model_path, \"mlp_model_baseline.h5\"))\n",
    "    scaler = results[i][2]\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    predictions_test = model.predict(x_test_scaled).flatten()\n",
    "    mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)\n",
    "    mse_vals.append(mse)\n",
    "    rmse_vals.append(rmse)\n",
    "    cmapss_score_vals.append(cmapss_score)\n",
    "    \n",
    "print(mse_vals)\n",
    "print(rmse_vals)\n",
    "print(cmapss_score_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_statistics(arr):\n",
    "    print(\"Mean: \", np.mean(arr))\n",
    "    print(\"Stddev: \", np.std(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE: \")\n",
    "print_summary_statistics(mse_vals)\n",
    "\n",
    "print(\"\\nRMSE: \")\n",
    "print_summary_statistics(rmse_vals)\n",
    "\n",
    "print(\"\\nCMAPSS score: \")\n",
    "print_summary_statistics(cmapss_score_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "scaler = results[random_seed][2]\n",
    "x_train_split, _, y_train_split, _ = train_test_split(x_train, y_train, test_size=0.3, \n",
    "                                                      random_state=random_seed)\n",
    "\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "x_train_scaled = scaler.transform(x_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = r'D:\\Licenta\\notebooks\\DS02\\experiment_set_4\\split_seed_0\\history.pkl'\n",
    "model_path = r'D:\\Licenta\\notebooks\\DS02\\experiment_set_4\\split_seed_0\\mlp_model_baseline.h5'\n",
    "\n",
    "history = load_history(history_path)\n",
    "plot_loss_curves(history, output_path, y_lim=[0, 120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model_path)\n",
    "model_evaluation(loaded_model, x_test=x_test_scaled, y_test=y_test, \n",
    "                 x_train=x_train_scaled, y_train=y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-keras-gpu",
   "language": "python",
   "name": "tf-keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
