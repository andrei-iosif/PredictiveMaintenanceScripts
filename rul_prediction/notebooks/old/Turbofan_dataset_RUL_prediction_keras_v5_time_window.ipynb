{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "seed = 0\n",
    "os.environ['PYTHONHASSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/turbofan_dataset/N-CMAPSS_DS02-006.h5'\n",
    "output_path = 'DS02/experiment_set_11/corr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename, load_test_set=True):\n",
    "    \"\"\" Reads a dataset from a given .h5 file and compose (in memory) the train and test data. \n",
    "    Args:\n",
    "        filename(str): path to the .h5 file\n",
    "    Returns:\n",
    "        train_set(pd.DataFrame), test_set(pd.DataFrame)\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'r') as hdf:\n",
    "        # Development set\n",
    "        W_dev = np.array(hdf.get('W_dev'))             # W\n",
    "        X_s_dev = np.array(hdf.get('X_s_dev'))         # X_s\n",
    "        X_v_dev = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "        T_dev = np.array(hdf.get('T_dev'))             # T\n",
    "        Y_dev = np.array(hdf.get('Y_dev'))             # RUL  \n",
    "        A_dev = np.array(hdf.get('A_dev'))             # Auxiliary\n",
    "\n",
    "        # Test set\n",
    "        if load_test_set:\n",
    "            W_test = np.array(hdf.get('W_test'))           # W\n",
    "            X_s_test = np.array(hdf.get('X_s_test'))       # X_s\n",
    "            X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "            T_test = np.array(hdf.get('T_test'))           # T\n",
    "            Y_test = np.array(hdf.get('Y_test'))           # RUL  \n",
    "            A_test = np.array(hdf.get('A_test'))           # Auxiliary\n",
    "        \n",
    "        # Column names\n",
    "        W_var = np.array(hdf.get('W_var'))\n",
    "        X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "        X_v_var = np.array(hdf.get('X_v_var')) \n",
    "        T_var = np.array(hdf.get('T_var'))\n",
    "        A_var = np.array(hdf.get('A_var'))\n",
    "        \n",
    "        columns = []\n",
    "        columns.append(list(np.array(A_var, dtype='U20')))\n",
    "        columns.append(list(np.array(T_var, dtype='U20')))\n",
    "        columns.append(list(np.array(X_s_var, dtype='U20')))\n",
    "        columns.append(list(np.array(X_v_var, dtype='U20')))\n",
    "        columns.append(list(np.array(W_var, dtype='U20')))\n",
    "        columns.append(['RUL'])\n",
    "        \n",
    "        columns_list = []\n",
    "        for columns_per_category in columns:\n",
    "            columns_list += columns_per_category\n",
    "        \n",
    "    train_set = np.concatenate((A_dev, T_dev, X_s_dev, X_v_dev, W_dev, Y_dev), axis=1)\n",
    "    if load_test_set:\n",
    "        test_set = np.concatenate((A_test, T_test, X_s_test, X_v_test, W_test, Y_test), axis=1)\n",
    "        return pd.DataFrame(data=train_set, columns=columns_list), pd.DataFrame(data=test_set, columns=columns_list), columns\n",
    "    else:\n",
    "        return pd.DataFrame(data=train_set, columns=columns_list), None, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_cycle_info(df, compute_cycle_len=False):\n",
    "    unit_ids = np.unique(df['unit'])\n",
    "    print('Engine units in df: ', unit_ids)\n",
    "    for i in unit_ids:\n",
    "        num_cycles = len(np.unique(df.loc[df['unit'] == i, 'cycle']))\n",
    "        print('Unit: ', i, ' - Number of flight cycles: ', num_cycles)\n",
    "        \n",
    "    if compute_cycle_len:\n",
    "        cycle_ids = np.unique(df['cycle'])\n",
    "        print('Total number of cycles: ', len(cycle_ids))\n",
    "        min_len = np.inf\n",
    "        max_len = 0\n",
    "        for i in cycle_ids:\n",
    "            cycle_len = len(df.loc[df['cycle'] == i, 'cycle'])\n",
    "            if cycle_len < min_len:\n",
    "                min_len = cycle_len\n",
    "            elif cycle_len > max_len:\n",
    "                max_len = cycle_len\n",
    "        print('Min cycle length: ', min_len)\n",
    "        print('Max cycle length: ', max_len)\n",
    "    \n",
    "    return unit_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter constant and quasi-constant features\n",
    "def get_quasi_constant_features(dataset, variance_th=0.01, debug=True):\n",
    "    constant_filter = VarianceThreshold(threshold=variance_th)\n",
    "    constant_filter.fit(dataset)\n",
    "    constant_features = [col for col in dataset.columns \n",
    "                         if col not in dataset.columns[constant_filter.get_support()]]\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Number of non-constant features: \", len(dataset.columns[constant_filter.get_support()]))\n",
    "        \n",
    "        print(\"Number of quasi-constant features: \", len(constant_features))\n",
    "        print(\"Quasi-constant features: \")\n",
    "        for col in constant_features:\n",
    "            print(col)\n",
    "    return constant_features\n",
    "\n",
    "def get_non_correlated_features(dataset, corr_th=0.9, debug=True):\n",
    "    corr_mat = dataset.corr()\n",
    "    corr_mat = np.abs(corr_mat)\n",
    "    \n",
    "    num_cols = corr_mat.shape[0]\n",
    "    columns = np.full((num_cols,), True, dtype=bool)\n",
    "    for i in range(num_cols):\n",
    "        for j in range(i+1, num_cols):\n",
    "            val = corr_mat.iloc[i, j]\n",
    "            if val >= corr_th:\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "                    if debug:\n",
    "                        print(dataset.columns[i], \"|\", dataset.columns[j], \"|\", round(val, 2))\n",
    "    if debug:        \n",
    "        correlated_features = dataset.columns[~columns]\n",
    "        print(\"Number of correlated features: \", len(correlated_features))\n",
    "        print(\"Correlated features: \", list(correlated_features))\n",
    "    \n",
    "    selected_columns = dataset.columns[columns]\n",
    "    if debug:\n",
    "        print(\"Number of selected features: \", len(selected_columns))\n",
    "        print(\"Selected features: \", list(selected_columns))\n",
    "    return selected_columns\n",
    "\n",
    "def cmapss_score_function(actual, predictions, normalize=True):\n",
    "    # diff < 0 -> over-estimation\n",
    "    # diff > 0 -> under-estimation\n",
    "    diff = actual - predictions\n",
    "    alpha = np.full_like(diff, 1/13)\n",
    "    negative_diff_mask = diff < 0\n",
    "    alpha[negative_diff_mask] = 1/10\n",
    "    score = np.sum(np.exp(alpha * np.abs(diff)))\n",
    "    \n",
    "    if normalize:\n",
    "        N = len(predictions)\n",
    "        score /= N\n",
    "    return score\n",
    "\n",
    "def compute_evaluation_metrics(actual, predictions, label='Test'):\n",
    "    mse = mean_squared_error(actual, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    cmapss_score = cmapss_score_function(actual, predictions)\n",
    "    print('{} set:\\nMSE: {:.2f}\\nRMSE: {:.2f}\\nCMAPSS score: {:.2f}\\n'.format(label, mse, rmse, \n",
    "                                                                     cmapss_score))\n",
    "    return mse, rmse, cmapss_score\n",
    "    \n",
    "def plot_loss_curves(history, output_path=None, y_lim=[0, 150]):\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim(y_lim)\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    \n",
    "    if output_path is not None:\n",
    "        plt.savefig(os.path.join(output_path, 'loss_curves.png'), format='png', dpi=300) \n",
    "    plt.show()\n",
    "    \n",
    "def plot_rul(expected, predicted):\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(expected)), expected, label='Expected')\n",
    "    plt.plot(range(len(predicted)), predicted, label='Predicted')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def create_mlp_model(input_dim, hidden_layer_sizes, activation='relu', output_weights_file=None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_sizes[0], \n",
    "                    input_dim=input_dim, \n",
    "                    kernel_initializer='random_normal', \n",
    "                    activation=activation))\n",
    "\n",
    "    for layer_size in hidden_layer_sizes[1:]:\n",
    "        model.add(Dense(layer_size, \n",
    "                        kernel_initializer='random_normal', \n",
    "                        activation=activation))\n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer='random_normal'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    if output_weights_file is not None:\n",
    "        model.save_weights(output_weights_file)\n",
    "    return model\n",
    "\n",
    "def train_model_existing_weights(model, weights_file, x_train, y_train, x_val, y_val, epochs=200, batch_size=512, callbacks=[]):\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.load_weights(weights_file)\n",
    "    return model.fit(x_train, y_train,\n",
    "                     validation_data=(x_val, y_val),\n",
    "                     epochs=epochs,\n",
    "                     batch_size=batch_size,\n",
    "                     verbose=1,\n",
    "                     callbacks=callbacks)\n",
    "\n",
    "def save_history(history, output_file=os.path.join(output_path, \"history.pkl\")):\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(history.history, file)\n",
    "    print(\"Saved training history to file: {}\".format(output_file))\n",
    "\n",
    "def load_history(file):\n",
    "    return pickle.load(open(file, \"rb\"))\n",
    "\n",
    "def save_object(obj, output_file):\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "    print(\"Saved object to file: {}\".format(output_file))\n",
    "    \n",
    "def load_object(file):\n",
    "    return pickle.load(open(file, \"rb\"))\n",
    "\n",
    "def model_evaluation(model, x_test, y_test, x_train=None, y_train=None, plot_range=[0, 10**3]):\n",
    "    if x_train is not None and y_train is not None:\n",
    "        predictions_train = model.predict(x_train).flatten()\n",
    "        compute_evaluation_metrics(predictions_train, y_train, 'Train')\n",
    "        \n",
    "        expected = y_train[plot_range[0]:plot_range[1]]\n",
    "        predicted = predictions_train[plot_range[0]:plot_range[1]]\n",
    "        plot_rul(expected, predicted)\n",
    "        \n",
    "    predictions_test = model.predict(x_test).flatten()\n",
    "    compute_evaluation_metrics(predictions_test, y_test)\n",
    "    \n",
    "    expected = y_test[plot_range[0]:plot_range[1]]\n",
    "    predicted = predictions_test[plot_range[0]:plot_range[1]]\n",
    "    plot_rul(expected, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list(string_list, output_file):\n",
    "    output_file.write(\"[\")\n",
    "    n = len(string_list)\n",
    "    for i in range(n - 1):\n",
    "        output_file.write(\"{}, \".format(string_list[i]))\n",
    "    output_file.write(\"{}]\\n\".format(string_list[-1]))\n",
    "    \n",
    "def feature_list_to_string(feature_list):\n",
    "    return \"__\".join(feature_list)\n",
    "\n",
    "def numbers_list_to_string(num_list):\n",
    "    return \" \".join([str(x) for x in num_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mi_ranked_features(mi, n):\n",
    "    mi_sorted = mi.sort_values(by=\"MI\", ascending=False)\n",
    "    return mi[:n][\"Col\"].tolist()\n",
    "\n",
    "def time_window_processing_old(X, y, window_size, stride=1, drop_nan=True):\n",
    "    df = X\n",
    "    lag_min = stride\n",
    "    lag_max = stride + window_size\n",
    "    \n",
    "    shifted_cols = {}\n",
    "    for lag in range(lag_min, lag_max):\n",
    "        cols = df.groupby(\"unit\").shift(lag)\n",
    "        shifted_cols[lag] = cols\n",
    "    \n",
    "    for lag, cols in shifted_cols.items():\n",
    "        df = df.join(cols.rename(columns=lambda x: x + \"_(t-{})\".format(lag)))\n",
    "    \n",
    "    df = df.join(y)\n",
    "    \n",
    "    if drop_nan:\n",
    "        df.dropna(inplace=True)\n",
    "    df.drop(labels=['unit'], axis=1, inplace=True)\n",
    "    \n",
    "    feature_columns = [col for col in df.columns.values if col != y.name]    \n",
    "    return df[feature_columns], df[y.name]\n",
    "\n",
    "def time_window_processing(X, y, window_size, stride=1):\n",
    "    df = X\n",
    "    \n",
    "    # Create lagged version for each column\n",
    "    shifted_cols = {}\n",
    "    for lag in range(window_size):\n",
    "        cols = df.groupby(\"unit\").shift(lag)\n",
    "        shifted_cols[lag] = cols\n",
    "    \n",
    "    # Add lag columns to initial dataframe\n",
    "    for lag, cols in shifted_cols.items():\n",
    "        df = df.join(cols.rename(columns=lambda x: x + \"_(t-{})\".format(lag)))\n",
    "    \n",
    "    # Add output column\n",
    "    df = df.join(y)\n",
    "    \n",
    "    # Drop rows with NaN and the unit column\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop(labels=['unit'], axis=1, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Remove overlapping windows with overlap less than stride\n",
    "    if stride > 1:\n",
    "        df = df[df.index % (stride + 1) == 0]\n",
    "    \n",
    "    feature_columns = [col for col in df.columns.values if col != y.name]    \n",
    "    return df[feature_columns], df[y.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation time (sec):  3.25\n",
      "Train set shape: (5263447, 47)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.process_time()  \n",
    "train_set, _, columns = load_dataset(filename, load_test_set=False)\n",
    "print(\"Operation time (sec): \" , (time.process_time() - start_time))\n",
    "print(\"Train set shape: \" + str(train_set.shape))\n",
    "\n",
    "columns_aux = columns[0] \n",
    "columns_health_params = columns[1] \n",
    "columns_sensor_measurements = columns[2] \n",
    "columns_virtual_sensors = columns[3]\n",
    "columns_operating_conditions = columns[4] \n",
    "target_col = columns[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set['RUL']\n",
    "x_train = train_set.drop(['RUL'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mutual_info_series = pd.read_csv(os.path.join(output_path, \"mutual_info.csv\"))\n",
    "#selected_columns = get_mi_ranked_features(mutual_info_series, 10)\n",
    "selected_columns = ['T24', 'T30', 'P15', 'SmFan', 'SmLPC']\n",
    "selected_columns += ['unit']\n",
    "x_train = x_train[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5263447 entries, 0 to 5263446\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   T24     float32\n",
      " 1   T30     float32\n",
      " 2   P15     float32\n",
      " 3   SmFan   float32\n",
      " 4   SmLPC   float32\n",
      " 5   unit    float32\n",
      "dtypes: float32(6)\n",
      "memory usage: 120.5 MB\n"
     ]
    }
   ],
   "source": [
    "x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tw, y_train_tw = time_window_processing(x_train, y_train, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>P15</th>\n",
       "      <th>SmFan</th>\n",
       "      <th>SmLPC</th>\n",
       "      <th>T24_(t-0)</th>\n",
       "      <th>T30_(t-0)</th>\n",
       "      <th>P15_(t-0)</th>\n",
       "      <th>SmFan_(t-0)</th>\n",
       "      <th>SmLPC_(t-0)</th>\n",
       "      <th>...</th>\n",
       "      <th>T24_(t-8)</th>\n",
       "      <th>T30_(t-8)</th>\n",
       "      <th>P15_(t-8)</th>\n",
       "      <th>SmFan_(t-8)</th>\n",
       "      <th>SmLPC_(t-8)</th>\n",
       "      <th>T24_(t-9)</th>\n",
       "      <th>T30_(t-9)</th>\n",
       "      <th>P15_(t-9)</th>\n",
       "      <th>SmFan_(t-9)</th>\n",
       "      <th>SmLPC_(t-9)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600.111267</td>\n",
       "      <td>1438.689209</td>\n",
       "      <td>15.785221</td>\n",
       "      <td>16.642193</td>\n",
       "      <td>9.903013</td>\n",
       "      <td>600.111267</td>\n",
       "      <td>1438.689209</td>\n",
       "      <td>15.785221</td>\n",
       "      <td>16.642193</td>\n",
       "      <td>9.903013</td>\n",
       "      <td>...</td>\n",
       "      <td>600.055908</td>\n",
       "      <td>1438.350220</td>\n",
       "      <td>15.795477</td>\n",
       "      <td>16.639221</td>\n",
       "      <td>9.904926</td>\n",
       "      <td>600.148010</td>\n",
       "      <td>1438.498169</td>\n",
       "      <td>15.806267</td>\n",
       "      <td>16.648832</td>\n",
       "      <td>9.898130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>600.142395</td>\n",
       "      <td>1438.692017</td>\n",
       "      <td>15.783556</td>\n",
       "      <td>16.658705</td>\n",
       "      <td>9.895153</td>\n",
       "      <td>600.142395</td>\n",
       "      <td>1438.692017</td>\n",
       "      <td>15.783556</td>\n",
       "      <td>16.658705</td>\n",
       "      <td>9.895153</td>\n",
       "      <td>...</td>\n",
       "      <td>600.298218</td>\n",
       "      <td>1439.063965</td>\n",
       "      <td>15.807512</td>\n",
       "      <td>16.649031</td>\n",
       "      <td>9.897465</td>\n",
       "      <td>600.369690</td>\n",
       "      <td>1439.240234</td>\n",
       "      <td>15.816360</td>\n",
       "      <td>16.653812</td>\n",
       "      <td>9.905518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>600.082153</td>\n",
       "      <td>1438.551758</td>\n",
       "      <td>15.774821</td>\n",
       "      <td>16.660770</td>\n",
       "      <td>9.895080</td>\n",
       "      <td>600.082153</td>\n",
       "      <td>1438.551758</td>\n",
       "      <td>15.774821</td>\n",
       "      <td>16.660770</td>\n",
       "      <td>9.895080</td>\n",
       "      <td>...</td>\n",
       "      <td>600.241394</td>\n",
       "      <td>1438.947021</td>\n",
       "      <td>15.800091</td>\n",
       "      <td>16.654341</td>\n",
       "      <td>9.901053</td>\n",
       "      <td>600.153015</td>\n",
       "      <td>1438.802368</td>\n",
       "      <td>15.792666</td>\n",
       "      <td>16.639463</td>\n",
       "      <td>9.905753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>599.990112</td>\n",
       "      <td>1438.344604</td>\n",
       "      <td>15.761504</td>\n",
       "      <td>16.660002</td>\n",
       "      <td>9.894007</td>\n",
       "      <td>599.990112</td>\n",
       "      <td>1438.344604</td>\n",
       "      <td>15.761504</td>\n",
       "      <td>16.660002</td>\n",
       "      <td>9.894007</td>\n",
       "      <td>...</td>\n",
       "      <td>600.205200</td>\n",
       "      <td>1438.843628</td>\n",
       "      <td>15.793442</td>\n",
       "      <td>16.660341</td>\n",
       "      <td>9.898022</td>\n",
       "      <td>600.111267</td>\n",
       "      <td>1438.689209</td>\n",
       "      <td>15.785221</td>\n",
       "      <td>16.642193</td>\n",
       "      <td>9.903013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>600.022766</td>\n",
       "      <td>1438.354736</td>\n",
       "      <td>15.762060</td>\n",
       "      <td>16.676348</td>\n",
       "      <td>9.888162</td>\n",
       "      <td>600.022766</td>\n",
       "      <td>1438.354736</td>\n",
       "      <td>15.762060</td>\n",
       "      <td>16.676348</td>\n",
       "      <td>9.888162</td>\n",
       "      <td>...</td>\n",
       "      <td>600.119446</td>\n",
       "      <td>1438.638794</td>\n",
       "      <td>15.780783</td>\n",
       "      <td>16.658916</td>\n",
       "      <td>9.895327</td>\n",
       "      <td>600.142395</td>\n",
       "      <td>1438.692017</td>\n",
       "      <td>15.783556</td>\n",
       "      <td>16.658705</td>\n",
       "      <td>9.895153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263380</th>\n",
       "      <td>567.638062</td>\n",
       "      <td>1285.592407</td>\n",
       "      <td>14.283421</td>\n",
       "      <td>20.427475</td>\n",
       "      <td>6.839921</td>\n",
       "      <td>567.638062</td>\n",
       "      <td>1285.592407</td>\n",
       "      <td>14.283421</td>\n",
       "      <td>20.427475</td>\n",
       "      <td>6.839921</td>\n",
       "      <td>...</td>\n",
       "      <td>567.418640</td>\n",
       "      <td>1285.076538</td>\n",
       "      <td>14.251586</td>\n",
       "      <td>20.444849</td>\n",
       "      <td>6.835948</td>\n",
       "      <td>567.386108</td>\n",
       "      <td>1285.004272</td>\n",
       "      <td>14.247150</td>\n",
       "      <td>20.445320</td>\n",
       "      <td>6.836019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263383</th>\n",
       "      <td>567.729065</td>\n",
       "      <td>1285.798218</td>\n",
       "      <td>14.296276</td>\n",
       "      <td>20.422405</td>\n",
       "      <td>6.840520</td>\n",
       "      <td>567.729065</td>\n",
       "      <td>1285.798218</td>\n",
       "      <td>14.296276</td>\n",
       "      <td>20.422405</td>\n",
       "      <td>6.840520</td>\n",
       "      <td>...</td>\n",
       "      <td>567.494568</td>\n",
       "      <td>1285.264526</td>\n",
       "      <td>14.263375</td>\n",
       "      <td>20.434511</td>\n",
       "      <td>6.838491</td>\n",
       "      <td>567.462219</td>\n",
       "      <td>1285.195557</td>\n",
       "      <td>14.258837</td>\n",
       "      <td>20.436394</td>\n",
       "      <td>6.838428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263386</th>\n",
       "      <td>567.778870</td>\n",
       "      <td>1285.943604</td>\n",
       "      <td>14.305333</td>\n",
       "      <td>20.408386</td>\n",
       "      <td>6.844467</td>\n",
       "      <td>567.778870</td>\n",
       "      <td>1285.943604</td>\n",
       "      <td>14.305333</td>\n",
       "      <td>20.408386</td>\n",
       "      <td>6.844467</td>\n",
       "      <td>...</td>\n",
       "      <td>567.592468</td>\n",
       "      <td>1285.474731</td>\n",
       "      <td>14.276195</td>\n",
       "      <td>20.434193</td>\n",
       "      <td>6.838289</td>\n",
       "      <td>567.560303</td>\n",
       "      <td>1285.404419</td>\n",
       "      <td>14.272107</td>\n",
       "      <td>20.433979</td>\n",
       "      <td>6.838126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263389</th>\n",
       "      <td>567.942627</td>\n",
       "      <td>1287.089600</td>\n",
       "      <td>14.322270</td>\n",
       "      <td>20.402950</td>\n",
       "      <td>6.878830</td>\n",
       "      <td>567.942627</td>\n",
       "      <td>1287.089600</td>\n",
       "      <td>14.322270</td>\n",
       "      <td>20.402950</td>\n",
       "      <td>6.878830</td>\n",
       "      <td>...</td>\n",
       "      <td>567.689453</td>\n",
       "      <td>1285.684814</td>\n",
       "      <td>14.289452</td>\n",
       "      <td>20.431725</td>\n",
       "      <td>6.838095</td>\n",
       "      <td>567.638062</td>\n",
       "      <td>1285.592407</td>\n",
       "      <td>14.283421</td>\n",
       "      <td>20.427475</td>\n",
       "      <td>6.839921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263392</th>\n",
       "      <td>568.615234</td>\n",
       "      <td>1289.865845</td>\n",
       "      <td>14.364547</td>\n",
       "      <td>20.379560</td>\n",
       "      <td>6.897261</td>\n",
       "      <td>568.615234</td>\n",
       "      <td>1289.865845</td>\n",
       "      <td>14.364547</td>\n",
       "      <td>20.379560</td>\n",
       "      <td>6.897261</td>\n",
       "      <td>...</td>\n",
       "      <td>567.752808</td>\n",
       "      <td>1285.851318</td>\n",
       "      <td>14.299707</td>\n",
       "      <td>20.420259</td>\n",
       "      <td>6.840891</td>\n",
       "      <td>567.729065</td>\n",
       "      <td>1285.798218</td>\n",
       "      <td>14.296276</td>\n",
       "      <td>20.422405</td>\n",
       "      <td>6.840520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1754465 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                T24          T30        P15      SmFan     SmLPC   T24_(t-0)  \\\n",
       "0        600.111267  1438.689209  15.785221  16.642193  9.903013  600.111267   \n",
       "3        600.142395  1438.692017  15.783556  16.658705  9.895153  600.142395   \n",
       "6        600.082153  1438.551758  15.774821  16.660770  9.895080  600.082153   \n",
       "9        599.990112  1438.344604  15.761504  16.660002  9.894007  599.990112   \n",
       "12       600.022766  1438.354736  15.762060  16.676348  9.888162  600.022766   \n",
       "...             ...          ...        ...        ...       ...         ...   \n",
       "5263380  567.638062  1285.592407  14.283421  20.427475  6.839921  567.638062   \n",
       "5263383  567.729065  1285.798218  14.296276  20.422405  6.840520  567.729065   \n",
       "5263386  567.778870  1285.943604  14.305333  20.408386  6.844467  567.778870   \n",
       "5263389  567.942627  1287.089600  14.322270  20.402950  6.878830  567.942627   \n",
       "5263392  568.615234  1289.865845  14.364547  20.379560  6.897261  568.615234   \n",
       "\n",
       "           T30_(t-0)  P15_(t-0)  SmFan_(t-0)  SmLPC_(t-0)  ...   T24_(t-8)  \\\n",
       "0        1438.689209  15.785221    16.642193     9.903013  ...  600.055908   \n",
       "3        1438.692017  15.783556    16.658705     9.895153  ...  600.298218   \n",
       "6        1438.551758  15.774821    16.660770     9.895080  ...  600.241394   \n",
       "9        1438.344604  15.761504    16.660002     9.894007  ...  600.205200   \n",
       "12       1438.354736  15.762060    16.676348     9.888162  ...  600.119446   \n",
       "...              ...        ...          ...          ...  ...         ...   \n",
       "5263380  1285.592407  14.283421    20.427475     6.839921  ...  567.418640   \n",
       "5263383  1285.798218  14.296276    20.422405     6.840520  ...  567.494568   \n",
       "5263386  1285.943604  14.305333    20.408386     6.844467  ...  567.592468   \n",
       "5263389  1287.089600  14.322270    20.402950     6.878830  ...  567.689453   \n",
       "5263392  1289.865845  14.364547    20.379560     6.897261  ...  567.752808   \n",
       "\n",
       "           T30_(t-8)  P15_(t-8)  SmFan_(t-8)  SmLPC_(t-8)   T24_(t-9)  \\\n",
       "0        1438.350220  15.795477    16.639221     9.904926  600.148010   \n",
       "3        1439.063965  15.807512    16.649031     9.897465  600.369690   \n",
       "6        1438.947021  15.800091    16.654341     9.901053  600.153015   \n",
       "9        1438.843628  15.793442    16.660341     9.898022  600.111267   \n",
       "12       1438.638794  15.780783    16.658916     9.895327  600.142395   \n",
       "...              ...        ...          ...          ...         ...   \n",
       "5263380  1285.076538  14.251586    20.444849     6.835948  567.386108   \n",
       "5263383  1285.264526  14.263375    20.434511     6.838491  567.462219   \n",
       "5263386  1285.474731  14.276195    20.434193     6.838289  567.560303   \n",
       "5263389  1285.684814  14.289452    20.431725     6.838095  567.638062   \n",
       "5263392  1285.851318  14.299707    20.420259     6.840891  567.729065   \n",
       "\n",
       "           T30_(t-9)  P15_(t-9)  SmFan_(t-9)  SmLPC_(t-9)  \n",
       "0        1438.498169  15.806267    16.648832     9.898130  \n",
       "3        1439.240234  15.816360    16.653812     9.905518  \n",
       "6        1438.802368  15.792666    16.639463     9.905753  \n",
       "9        1438.689209  15.785221    16.642193     9.903013  \n",
       "12       1438.692017  15.783556    16.658705     9.895153  \n",
       "...              ...        ...          ...          ...  \n",
       "5263380  1285.004272  14.247150    20.445320     6.836019  \n",
       "5263383  1285.195557  14.258837    20.436394     6.838428  \n",
       "5263386  1285.404419  14.272107    20.433979     6.838126  \n",
       "5263389  1285.592407  14.283421    20.427475     6.839921  \n",
       "5263392  1285.798218  14.296276    20.422405     6.840520  \n",
       "\n",
       "[1754465 rows x 55 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1754465 entries, 0 to 5263392\n",
      "Data columns (total 55 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   T24          float32\n",
      " 1   T30          float32\n",
      " 2   P15          float32\n",
      " 3   SmFan        float32\n",
      " 4   SmLPC        float32\n",
      " 5   T24_(t-0)    float32\n",
      " 6   T30_(t-0)    float32\n",
      " 7   P15_(t-0)    float32\n",
      " 8   SmFan_(t-0)  float32\n",
      " 9   SmLPC_(t-0)  float32\n",
      " 10  T24_(t-1)    float32\n",
      " 11  T30_(t-1)    float32\n",
      " 12  P15_(t-1)    float32\n",
      " 13  SmFan_(t-1)  float32\n",
      " 14  SmLPC_(t-1)  float32\n",
      " 15  T24_(t-2)    float32\n",
      " 16  T30_(t-2)    float32\n",
      " 17  P15_(t-2)    float32\n",
      " 18  SmFan_(t-2)  float32\n",
      " 19  SmLPC_(t-2)  float32\n",
      " 20  T24_(t-3)    float32\n",
      " 21  T30_(t-3)    float32\n",
      " 22  P15_(t-3)    float32\n",
      " 23  SmFan_(t-3)  float32\n",
      " 24  SmLPC_(t-3)  float32\n",
      " 25  T24_(t-4)    float32\n",
      " 26  T30_(t-4)    float32\n",
      " 27  P15_(t-4)    float32\n",
      " 28  SmFan_(t-4)  float32\n",
      " 29  SmLPC_(t-4)  float32\n",
      " 30  T24_(t-5)    float32\n",
      " 31  T30_(t-5)    float32\n",
      " 32  P15_(t-5)    float32\n",
      " 33  SmFan_(t-5)  float32\n",
      " 34  SmLPC_(t-5)  float32\n",
      " 35  T24_(t-6)    float32\n",
      " 36  T30_(t-6)    float32\n",
      " 37  P15_(t-6)    float32\n",
      " 38  SmFan_(t-6)  float32\n",
      " 39  SmLPC_(t-6)  float32\n",
      " 40  T24_(t-7)    float32\n",
      " 41  T30_(t-7)    float32\n",
      " 42  P15_(t-7)    float32\n",
      " 43  SmFan_(t-7)  float32\n",
      " 44  SmLPC_(t-7)  float32\n",
      " 45  T24_(t-8)    float32\n",
      " 46  T30_(t-8)    float32\n",
      " 47  P15_(t-8)    float32\n",
      " 48  SmFan_(t-8)  float32\n",
      " 49  SmLPC_(t-8)  float32\n",
      " 50  T24_(t-9)    float32\n",
      " 51  T30_(t-9)    float32\n",
      " 52  P15_(t-9)    float32\n",
      " 53  SmFan_(t-9)  float32\n",
      " 54  SmLPC_(t-9)  float32\n",
      "dtypes: float32(55)\n",
      "memory usage: 381.5 MB\n"
     ]
    }
   ],
   "source": [
    "x_train_tw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved object to file: DS02/experiment_set_11/corr\\results_10\\split_0\\scaler.pkl\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               14336     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 244,609\n",
      "Trainable params: 244,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "1076/1080 [============================>.] - ETA: 0s - loss: 603.1248\n",
      "Epoch 00001: val_loss improved from inf to 222.05882, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 601.8584 - val_loss: 222.0588\n",
      "Epoch 2/200\n",
      "1064/1080 [============================>.] - ETA: 0s - loss: 152.7130\n",
      "Epoch 00002: val_loss improved from 222.05882 to 100.70123, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 152.0914 - val_loss: 100.7012\n",
      "Epoch 3/200\n",
      "1067/1080 [============================>.] - ETA: 0s - loss: 94.7967\n",
      "Epoch 00003: val_loss improved from 100.70123 to 97.54143, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 94.6322 - val_loss: 97.5414\n",
      "Epoch 4/200\n",
      "1074/1080 [============================>.] - ETA: 0s - loss: 81.5597\n",
      "Epoch 00004: val_loss improved from 97.54143 to 78.37993, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 81.5941 - val_loss: 78.3799\n",
      "Epoch 5/200\n",
      "1080/1080 [==============================] - ETA: 0s - loss: 75.0347\n",
      "Epoch 00005: val_loss did not improve from 78.37993\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 75.0347 - val_loss: 103.9867\n",
      "Epoch 6/200\n",
      "1064/1080 [============================>.] - ETA: 0s - loss: 73.9100\n",
      "Epoch 00006: val_loss improved from 78.37993 to 63.03237, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 73.8435 - val_loss: 63.0324\n",
      "Epoch 7/200\n",
      "1078/1080 [============================>.] - ETA: 0s - loss: 68.4841\n",
      "Epoch 00007: val_loss did not improve from 63.03237\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 68.4656 - val_loss: 63.1339\n",
      "Epoch 8/200\n",
      "1070/1080 [============================>.] - ETA: 0s - loss: 67.1636\n",
      "Epoch 00008: val_loss improved from 63.03237 to 60.74570, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 67.1064 - val_loss: 60.7457\n",
      "Epoch 9/200\n",
      "1073/1080 [============================>.] - ETA: 0s - loss: 64.6402\n",
      "Epoch 00009: val_loss improved from 60.74570 to 59.08253, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 64.6957 - val_loss: 59.0825\n",
      "Epoch 10/200\n",
      "1076/1080 [============================>.] - ETA: 0s - loss: 63.5988\n",
      "Epoch 00010: val_loss did not improve from 59.08253\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 63.6292 - val_loss: 96.4697\n",
      "Epoch 11/200\n",
      "1079/1080 [============================>.] - ETA: 0s - loss: 62.1663\n",
      "Epoch 00011: val_loss did not improve from 59.08253\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 62.1693 - val_loss: 68.4613\n",
      "Epoch 12/200\n",
      "1079/1080 [============================>.] - ETA: 0s - loss: 59.1103\n",
      "Epoch 00012: val_loss improved from 59.08253 to 53.41174, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 59.1066 - val_loss: 53.4117\n",
      "Epoch 13/200\n",
      "1073/1080 [============================>.] - ETA: 0s - loss: 59.2420\n",
      "Epoch 00013: val_loss improved from 53.41174 to 53.17389, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 59.2117 - val_loss: 53.1739\n",
      "Epoch 14/200\n",
      "1074/1080 [============================>.] - ETA: 0s - loss: 57.8836\n",
      "Epoch 00014: val_loss did not improve from 53.17389\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 57.8693 - val_loss: 76.7999\n",
      "Epoch 15/200\n",
      "1072/1080 [============================>.] - ETA: 0s - loss: 56.0533\n",
      "Epoch 00015: val_loss did not improve from 53.17389\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 56.1165 - val_loss: 71.6321\n",
      "Epoch 16/200\n",
      "1071/1080 [============================>.] - ETA: 0s - loss: 56.5265\n",
      "Epoch 00016: val_loss improved from 53.17389 to 52.53087, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 56.4457 - val_loss: 52.5309\n",
      "Epoch 17/200\n",
      "1072/1080 [============================>.] - ETA: 0s - loss: 54.3530\n",
      "Epoch 00017: val_loss did not improve from 52.53087\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 54.3738 - val_loss: 65.0471\n",
      "Epoch 18/200\n",
      "1071/1080 [============================>.] - ETA: 0s - loss: 53.4437\n",
      "Epoch 00018: val_loss improved from 52.53087 to 49.59419, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 53.4050 - val_loss: 49.5942\n",
      "Epoch 19/200\n",
      "1071/1080 [============================>.] - ETA: 0s - loss: 52.3388\n",
      "Epoch 00019: val_loss did not improve from 49.59419\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 52.3344 - val_loss: 57.3931\n",
      "Epoch 20/200\n",
      "1072/1080 [============================>.] - ETA: 0s - loss: 53.6194\n",
      "Epoch 00020: val_loss did not improve from 49.59419\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 53.6368 - val_loss: 55.2103\n",
      "Epoch 21/200\n",
      "1065/1080 [============================>.] - ETA: 0s - loss: 51.6974\n",
      "Epoch 00021: val_loss did not improve from 49.59419\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 51.7225 - val_loss: 53.3985\n",
      "Epoch 22/200\n",
      "1076/1080 [============================>.] - ETA: 0s - loss: 50.1021\n",
      "Epoch 00022: val_loss did not improve from 49.59419\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 50.0959 - val_loss: 50.5148\n",
      "Epoch 23/200\n",
      "1079/1080 [============================>.] - ETA: 0s - loss: 50.6059\n",
      "Epoch 00023: val_loss did not improve from 49.59419\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 50.6030 - val_loss: 56.7668\n",
      "Epoch 24/200\n",
      "1073/1080 [============================>.] - ETA: 0s - loss: 49.2270\n",
      "Epoch 00024: val_loss improved from 49.59419 to 48.87032, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 49.2365 - val_loss: 48.8703\n",
      "Epoch 25/200\n",
      "1070/1080 [============================>.] - ETA: 0s - loss: 48.9249\n",
      "Epoch 00025: val_loss did not improve from 48.87032\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 48.9097 - val_loss: 55.0658\n",
      "Epoch 26/200\n",
      "1072/1080 [============================>.] - ETA: 0s - loss: 48.6383\n",
      "Epoch 00026: val_loss improved from 48.87032 to 46.58361, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 48.6404 - val_loss: 46.5836\n",
      "Epoch 27/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1075/1080 [============================>.] - ETA: 0s - loss: 47.8906\n",
      "Epoch 00027: val_loss did not improve from 46.58361\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 47.8763 - val_loss: 47.1361\n",
      "Epoch 28/200\n",
      "1074/1080 [============================>.] - ETA: 0s - loss: 46.4570\n",
      "Epoch 00028: val_loss did not improve from 46.58361\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 46.4502 - val_loss: 55.9118\n",
      "Epoch 29/200\n",
      "1071/1080 [============================>.] - ETA: 0s - loss: 47.8839\n",
      "Epoch 00029: val_loss did not improve from 46.58361\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 47.8355 - val_loss: 51.1052\n",
      "Epoch 30/200\n",
      "1069/1080 [============================>.] - ETA: 0s - loss: 46.1623\n",
      "Epoch 00030: val_loss did not improve from 46.58361\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 46.3394 - val_loss: 53.2621\n",
      "Epoch 31/200\n",
      "1075/1080 [============================>.] - ETA: 0s - loss: 45.5283\n",
      "Epoch 00031: val_loss improved from 46.58361 to 44.39869, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 45.5236 - val_loss: 44.3987\n",
      "Epoch 32/200\n",
      "1075/1080 [============================>.] - ETA: 0s - loss: 45.1402\n",
      "Epoch 00032: val_loss improved from 44.39869 to 41.76421, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 45.1345 - val_loss: 41.7642\n",
      "Epoch 33/200\n",
      "1065/1080 [============================>.] - ETA: 0s - loss: 44.4918\n",
      "Epoch 00033: val_loss did not improve from 41.76421\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 44.4673 - val_loss: 42.8193\n",
      "Epoch 34/200\n",
      "1068/1080 [============================>.] - ETA: 0s - loss: 45.0549\n",
      "Epoch 00034: val_loss did not improve from 41.76421\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 45.0293 - val_loss: 54.8629\n",
      "Epoch 35/200\n",
      "1075/1080 [============================>.] - ETA: 0s - loss: 43.9332\n",
      "Epoch 00035: val_loss did not improve from 41.76421\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 43.9830 - val_loss: 50.7332\n",
      "Epoch 36/200\n",
      "1070/1080 [============================>.] - ETA: 0s - loss: 43.0638\n",
      "Epoch 00036: val_loss did not improve from 41.76421\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 43.1118 - val_loss: 42.5291\n",
      "Epoch 37/200\n",
      "1075/1080 [============================>.] - ETA: 0s - loss: 43.1928\n",
      "Epoch 00037: val_loss did not improve from 41.76421\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 43.1751 - val_loss: 41.9391\n",
      "Epoch 38/200\n",
      "1072/1080 [============================>.] - ETA: 0s - loss: 42.7760\n",
      "Epoch 00038: val_loss did not improve from 41.76421\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 42.7660 - val_loss: 50.2110\n",
      "Epoch 39/200\n",
      "1068/1080 [============================>.] - ETA: 0s - loss: 41.8564\n",
      "Epoch 00039: val_loss did not improve from 41.76421\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 41.8993 - val_loss: 50.9068\n",
      "Epoch 40/200\n",
      "1067/1080 [============================>.] - ETA: 0s - loss: 42.3554\n",
      "Epoch 00040: val_loss improved from 41.76421 to 40.32309, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 42.3721 - val_loss: 40.3231\n",
      "Epoch 41/200\n",
      "1068/1080 [============================>.] - ETA: 0s - loss: 41.2686\n",
      "Epoch 00041: val_loss improved from 40.32309 to 37.63760, saving model to DS02/experiment_set_11/corr\\results_10\\split_0\\mlp_model_trained.h5\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 41.2391 - val_loss: 37.6376\n",
      "Epoch 42/200\n",
      "1068/1080 [============================>.] - ETA: 0s - loss: 42.3142\n",
      "Epoch 00042: val_loss did not improve from 37.63760\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 42.2588 - val_loss: 43.2364\n",
      "Epoch 43/200\n",
      "1068/1080 [============================>.] - ETA: 0s - loss: 42.0885\n",
      "Epoch 00043: val_loss did not improve from 37.63760\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 42.0839 - val_loss: 42.3595\n",
      "Epoch 44/200\n",
      "1079/1080 [============================>.] - ETA: 0s - loss: 40.7897\n",
      "Epoch 00044: val_loss did not improve from 37.63760\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 40.7849 - val_loss: 42.2796\n",
      "Epoch 45/200\n",
      "1070/1080 [============================>.] - ETA: 0s - loss: 40.5023\n",
      "Epoch 00045: val_loss did not improve from 37.63760\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 40.5333 - val_loss: 43.6110\n",
      "Epoch 46/200\n",
      "1073/1080 [============================>.] - ETA: 0s - loss: 40.6536\n",
      "Epoch 00046: val_loss did not improve from 37.63760\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 40.6597 - val_loss: 42.6750\n",
      "Epoch 47/200\n",
      "1068/1080 [============================>.] - ETA: 0s - loss: 39.3948\n",
      "Epoch 00047: val_loss did not improve from 37.63760\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 39.4037 - val_loss: 38.2286\n",
      "Epoch 48/200\n",
      "1068/1080 [============================>.] - ETA: 0s - loss: 39.1238\n",
      "Epoch 00048: val_loss did not improve from 37.63760\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 39.1596 - val_loss: 57.0586\n",
      "Epoch 49/200\n",
      "1080/1080 [==============================] - ETA: 0s - loss: 39.0318 ETA: 0s - los\n",
      "Epoch 00049: val_loss did not improve from 37.63760\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 39.0318 - val_loss: 39.9102\n",
      "Epoch 50/200\n",
      "1072/1080 [============================>.] - ETA: 0s - loss: 39.5478\n",
      "Epoch 00050: val_loss did not improve from 37.63760\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 39.5336 - val_loss: 37.7540\n",
      "Epoch 51/200\n",
      "1072/1080 [============================>.] - ETA: 0s - loss: 38.7399\n",
      "Epoch 00051: val_loss did not improve from 37.63760\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 38.7485 - val_loss: 38.6797\n",
      "Epoch 00051: early stopping\n",
      "Saved training history to file: DS02/experiment_set_11/corr\\results_10\\split_0\\history.pkl\n",
      "Test set:\n",
      "MSE: 37.56\n",
      "RMSE: 6.13\n",
      "CMAPSS score: 1.62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# Test effect of window size\n",
    "###########################################\n",
    "NUM_TRIALS = 1\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "layer_sizes = [256, 256, 512, 64]\n",
    "\n",
    "window_size_list = [10]\n",
    "\n",
    "results_file = os.path.join(output_path, \"results_time_window.csv\")\n",
    "with open(results_file, \"w\") as file:\n",
    "    file.write(\"window_size,mse,rmse,cmapss,mse(mean),mse(std),rmse(mean),rmse(std),cmapss(mean),cmapss(std)\\n\")\n",
    "\n",
    "\n",
    "for i in range(len(window_size_list)):\n",
    "    # Time window processing\n",
    "    window_size = window_size_list[i]\n",
    "    stride = window_size // 2\n",
    "    # stride = 1\n",
    "    \n",
    "    x_train_tw, y_train_tw = time_window_processing(x_train, y_train, window_size, stride)\n",
    "    \n",
    "    # Train-validation split\n",
    "    x_train_tw, x_holdout, y_train_tw, y_holdout = train_test_split(x_train_tw, \n",
    "                                                                    y_train_tw, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    random_state=seed)\n",
    "    \n",
    "    mse_vals = []\n",
    "    rmse_vals = []\n",
    "    cmapss_vals = []\n",
    "    \n",
    "    for random_seed in range(NUM_TRIALS):\n",
    "        # Train-validation split for early stopping\n",
    "        x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train_tw, \n",
    "                                                                                  y_train_tw, \n",
    "                                                                                  test_size=0.1, \n",
    "                                                                                  random_state=random_seed)\n",
    "        # Create output path\n",
    "        results_folder =\"results_{}\".format(window_size)\n",
    "        results_path_crr = os.path.join(output_path, results_folder)\n",
    "        results_path_crr_split = os.path.join(results_path_crr, \"split_{}\".format(random_seed))\n",
    "        if not os.path.exists(results_path_crr_split):\n",
    "            os.makedirs(results_path_crr_split)\n",
    "\n",
    "        # Standardization\n",
    "        scaler_file = os.path.join(results_path_crr_split, 'scaler.pkl')\n",
    "        scaler = StandardScaler()\n",
    "        x_train_scaled = scaler.fit_transform(x_train_split)\n",
    "        x_val_scaled = scaler.transform(x_val_split)\n",
    "        input_dim = x_train_scaled.shape[1]\n",
    "        save_object(scaler, scaler_file)\n",
    "\n",
    "        # Create model\n",
    "        weights_file = os.path.join(results_path_crr, 'mlp_initial_weights.h5')\n",
    "        model_path = os.path.join(results_path_crr_split, 'mlp_model_trained.h5')\n",
    "        \n",
    "        # Save initial weights\n",
    "        if random_seed == 0:\n",
    "            model = create_mlp_model(input_dim, layer_sizes, activation='tanh',\n",
    "                                     output_weights_file=weights_file)\n",
    "        else:\n",
    "            model = create_mlp_model(input_dim, layer_sizes, activation='tanh')\n",
    "        model.summary()\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint(model_path, monitor='val_loss', mode='min', verbose=2, \n",
    "                             save_best_only=True)\n",
    "\n",
    "        # Train model\n",
    "        history = train_model_existing_weights(model, weights_file, \n",
    "                                               x_train_scaled, y_train_split, \n",
    "                                               x_val_scaled, y_val_split, \n",
    "                                               batch_size=batch_size, \n",
    "                                               epochs=epochs, \n",
    "                                               callbacks=[es, mc])\n",
    "\n",
    "        history_file = os.path.join(results_path_crr_split, \"history.pkl\")\n",
    "        save_history(history, history_file)\n",
    "\n",
    "        # Performance evaluation\n",
    "        x_holdout_scaled = scaler.transform(x_holdout)\n",
    "\n",
    "        loaded_model = load_model(model_path)\n",
    "        predictions_holdout = loaded_model.predict(x_holdout_scaled).flatten()\n",
    "        mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_holdout, y_holdout)\n",
    "        \n",
    "        mse_vals.append(mse)\n",
    "        rmse_vals.append(rmse)\n",
    "        cmapss_vals.append(cmapss_score)\n",
    "    \n",
    "    mse_mean = np.mean(mse_vals)\n",
    "    mse_std = np.std(mse_vals)\n",
    "    rmse_mean = np.mean(rmse_vals)\n",
    "    rmse_std = np.std(rmse_vals)\n",
    "    cmapss_mean = np.mean(cmapss_vals)\n",
    "    cmapss_std = np.std(cmapss_vals)\n",
    "    \n",
    "    with open(results_file, \"a\") as file:\n",
    "        file.write(f\"{window_size}, {numbers_list_to_string(mse_vals)}, {numbers_list_to_string(rmse_vals)}, {numbers_list_to_string(cmapss_vals)}, {mse_mean}, {mse_std}, {rmse_mean}, {rmse_std}, {cmapss_mean}, {cmapss_std}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set, columns = load_dataset(filename, load_test_set=True)\n",
    "y_test = test_set['RUL']\n",
    "x_test = test_set.drop(['RUL'], axis=1)\n",
    "\n",
    "# mutual_info_series = pd.read_csv(os.path.join(output_path, \"mutual_info.csv\"))\n",
    "# selected_columns = get_mi_ranked_features(mutual_info_series, 10) \n",
    "selected_columns = ['T24', 'T30', 'P15', 'SmFan', 'SmLPC']\n",
    "selected_columns += ['unit']\n",
    "x_test = x_test[selected_columns]\n",
    "\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "MSE: 56.06\n",
      "RMSE: 7.49\n",
      "CMAPSS score: 1.72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# Final evaluation on test set\n",
    "##############################\n",
    "NUM_TRIALS = 1\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "layer_sizes = [256, 256, 512, 64]\n",
    "\n",
    "window_size_list = [10]\n",
    "\n",
    "results_file = os.path.join(output_path, \"results_time_window_test_set.csv\")\n",
    "with open(results_file, \"w\") as file:\n",
    "    file.write(\"window_size,mse,rmse,cmapss,mse(mean),mse(std),rmse(mean),rmse(std),cmapss(mean),cmapss(std)\\n\")\n",
    "\n",
    "\n",
    "for i in range(len(window_size_list)):\n",
    "    window_size = window_size_list[i]\n",
    "    stride = window_size // 2\n",
    "    x_test_tw, y_test = time_window_processing(x_test, y_test, window_size, stride)\n",
    "    \n",
    "    mse_vals = []\n",
    "    rmse_vals = []\n",
    "    cmapss_vals = []\n",
    "    \n",
    "    for random_seed in range(NUM_TRIALS):\n",
    "        results_folder =\"results_{}\".format(window_size)\n",
    "        results_path_crr = os.path.join(output_path, results_folder)\n",
    "        results_path_crr_split = os.path.join(results_path_crr, \"split_{}\".format(random_seed))\n",
    "        \n",
    "        scaler_file = os.path.join(results_path_crr_split, 'scaler.pkl')\n",
    "        scaler = load_object(scaler_file)\n",
    "\n",
    "        model_path = os.path.join(results_path_crr_split, 'mlp_model_trained.h5')\n",
    "        \n",
    "        # Performance evaluation\n",
    "        x_test_scaled = scaler.transform(x_test_tw)\n",
    "\n",
    "        loaded_model = load_model(model_path)\n",
    "        predictions_test = loaded_model.predict(x_test_scaled).flatten()\n",
    "        mse, rmse, cmapss_score = compute_evaluation_metrics(predictions_test, y_test)\n",
    "        \n",
    "        mse_vals.append(mse)\n",
    "        rmse_vals.append(rmse)\n",
    "        cmapss_vals.append(cmapss_score)\n",
    "    \n",
    "    mse_mean = np.mean(mse_vals)\n",
    "    mse_std = np.std(mse_vals)\n",
    "    rmse_mean = np.mean(rmse_vals)\n",
    "    rmse_std = np.std(rmse_vals)\n",
    "    cmapss_mean = np.mean(cmapss_vals)\n",
    "    cmapss_std = np.std(cmapss_vals)\n",
    "    \n",
    "    with open(results_file, \"a\") as file:\n",
    "        file.write(f\"{window_size}, {numbers_list_to_string(mse_vals)}, {numbers_list_to_string(rmse_vals)}, {numbers_list_to_string(cmapss_vals)}, {mse_mean}, {mse_std}, {rmse_mean}, {rmse_std}, {cmapss_mean}, {cmapss_std}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-keras-gpu",
   "language": "python",
   "name": "tf-keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
